{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/kohei3/anaconda3/envs/tensorflow/lib/python3.6/site-packages/\")\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5106827182071126523\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7896396596\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16482771610563754234\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/input/train.csv\")\n",
    "test = pd.read_csv(\"../data/input/test.csv\")\n",
    "target = train[\"scalar_coupling_constant\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "        \"Atom\",\n",
    "        #\"AtomPosition\",\n",
    "        #\"AtomDistance\",\n",
    "        #\"CouplingType\",\n",
    "        #\"AtomEnvironment\",\n",
    "        #\"AtomNeighbors\",\n",
    "        #\"BruteForce\",\n",
    "        #\"DistanceFromClosest\",\n",
    "        #\"ElectroNegFromClosest\",\n",
    "        \"ACSF\",\n",
    "        #\"MullikenChargePred\",\n",
    "        #\"OpenBabelBasic\",\n",
    "        #\"CosineDistance\",\n",
    "        #\"PotentialPred\", \n",
    "        \"DisIsAllYouNeed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([feather.read_dataframe(\"../features/\" + feature + \"_train.feather\") for feature in features], axis=1)\n",
    "X_test = pd.concat([feather.read_dataframe(\"../features/\" + feature + \"_test.feather\") for feature in features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4658147, 290), (2505542, 290))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>acsf_0_a0</th>\n",
       "      <th>acsf_1_a0</th>\n",
       "      <th>acsf_2_a0</th>\n",
       "      <th>acsf_3_a0</th>\n",
       "      <th>acsf_4_a0</th>\n",
       "      <th>acsf_5_a0</th>\n",
       "      <th>acsf_6_a0</th>\n",
       "      <th>acsf_7_a0</th>\n",
       "      <th>...</th>\n",
       "      <th>d_7_2</th>\n",
       "      <th>d_7_3</th>\n",
       "      <th>d_8_0</th>\n",
       "      <th>d_8_1</th>\n",
       "      <th>d_8_2</th>\n",
       "      <th>d_8_3</th>\n",
       "      <th>d_9_0</th>\n",
       "      <th>d_9_1</th>\n",
       "      <th>d_9_2</th>\n",
       "      <th>d_9_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2.770731</td>\n",
       "      <td>2.643446</td>\n",
       "      <td>2.757731</td>\n",
       "      <td>2.769428</td>\n",
       "      <td>5.248240e-08</td>\n",
       "      <td>0.468098</td>\n",
       "      <td>2.319362</td>\n",
       "      <td>0.970867</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2.770731</td>\n",
       "      <td>2.643446</td>\n",
       "      <td>2.757731</td>\n",
       "      <td>2.769428</td>\n",
       "      <td>5.248240e-08</td>\n",
       "      <td>0.468098</td>\n",
       "      <td>2.319362</td>\n",
       "      <td>0.970867</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2.770731</td>\n",
       "      <td>2.643446</td>\n",
       "      <td>2.757731</td>\n",
       "      <td>2.769428</td>\n",
       "      <td>5.248240e-08</td>\n",
       "      <td>0.468098</td>\n",
       "      <td>2.319362</td>\n",
       "      <td>0.970867</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2.770731</td>\n",
       "      <td>2.643446</td>\n",
       "      <td>2.757731</td>\n",
       "      <td>2.769428</td>\n",
       "      <td>5.248240e-08</td>\n",
       "      <td>0.468098</td>\n",
       "      <td>2.319362</td>\n",
       "      <td>0.970867</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2.770731</td>\n",
       "      <td>2.643447</td>\n",
       "      <td>2.757731</td>\n",
       "      <td>2.769428</td>\n",
       "      <td>5.248280e-08</td>\n",
       "      <td>0.468098</td>\n",
       "      <td>2.319362</td>\n",
       "      <td>0.970867</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  atom_0 atom_1  acsf_0_a0  acsf_1_a0  acsf_2_a0  acsf_3_a0     acsf_4_a0  \\\n",
       "0      H      C   2.770731   2.643446   2.757731   2.769428  5.248240e-08   \n",
       "1      H      H   2.770731   2.643446   2.757731   2.769428  5.248240e-08   \n",
       "2      H      H   2.770731   2.643446   2.757731   2.769428  5.248240e-08   \n",
       "3      H      H   2.770731   2.643446   2.757731   2.769428  5.248240e-08   \n",
       "4      H      C   2.770731   2.643447   2.757731   2.769428  5.248280e-08   \n",
       "\n",
       "   acsf_5_a0  acsf_6_a0  acsf_7_a0  ...  d_7_2  d_7_3  d_8_0  d_8_1  d_8_2  \\\n",
       "0   0.468098   2.319362   0.970867  ...    NaN    NaN    NaN    NaN    NaN   \n",
       "1   0.468098   2.319362   0.970867  ...    NaN    NaN    NaN    NaN    NaN   \n",
       "2   0.468098   2.319362   0.970867  ...    NaN    NaN    NaN    NaN    NaN   \n",
       "3   0.468098   2.319362   0.970867  ...    NaN    NaN    NaN    NaN    NaN   \n",
       "4   0.468098   2.319362   0.970867  ...    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "   d_8_3  d_9_0  d_9_1  d_9_2  d_9_3  \n",
       "0    NaN    NaN    NaN    NaN    NaN  \n",
       "1    NaN    NaN    NaN    NaN    NaN  \n",
       "2    NaN    NaN    NaN    NaN    NaN  \n",
       "3    NaN    NaN    NaN    NaN    NaN  \n",
       "4    NaN    NaN    NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 290 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting atom_0\n",
      "Starting atom_1\n"
     ]
    }
   ],
   "source": [
    "# keep nan as is\n",
    "#coupling_types = X_train['type']\n",
    "categorical_cols = list(X_train.columns[X_train.dtypes == object])\n",
    "\n",
    "\n",
    "for col in categorical_cols:    \n",
    "    print(f'Starting {col}')\n",
    "    uniques = list(X_train[col].unique())\n",
    "    if None in uniques:\n",
    "        uniques.remove(None)\n",
    "    mapping = dict(zip(uniques, range(1, len(uniques)+1)))\n",
    "    X_train[col] = X_train[col].map(mapping)\n",
    "    X_test[col] = X_test[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 9524.41 MB\n",
      "Memory usage after optimization is: 5019.86 MB\n",
      "Decreased by 47.3%\n",
      "Memory usage of dataframe is 5123.03 MB\n",
      "Memory usage after optimization is: 2700.10 MB\n",
      "Decreased by 47.3%\n"
     ]
    }
   ],
   "source": [
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['atom_0', 'atom_1', 'acsf_0_a0', 'acsf_1_a0', 'acsf_2_a0',\n",
       "       'acsf_3_a0', 'acsf_4_a0', 'acsf_5_a0', 'acsf_6_a0', 'acsf_7_a0',\n",
       "       'acsf_8_a0', 'acsf_9_a0', 'acsf_10_a0', 'acsf_11_a0', 'acsf_12_a0',\n",
       "       'acsf_13_a0', 'acsf_14_a0', 'acsf_15_a0', 'acsf_16_a0',\n",
       "       'acsf_17_a0', 'acsf_18_a0', 'acsf_19_a0', 'acsf_20_a0',\n",
       "       'acsf_21_a0', 'acsf_22_a0', 'acsf_23_a0', 'acsf_24_a0',\n",
       "       'acsf_25_a0', 'acsf_26_a0', 'acsf_27_a0', 'acsf_28_a0',\n",
       "       'acsf_29_a0', 'acsf_30_a0', 'acsf_31_a0', 'acsf_32_a0',\n",
       "       'acsf_33_a0', 'acsf_34_a0', 'acsf_35_a0', 'acsf_36_a0',\n",
       "       'acsf_37_a0', 'acsf_38_a0', 'acsf_39_a0', 'acsf_40_a0',\n",
       "       'acsf_41_a0', 'acsf_42_a0', 'acsf_43_a0', 'acsf_44_a0',\n",
       "       'acsf_45_a0', 'acsf_46_a0', 'acsf_47_a0', 'acsf_48_a0',\n",
       "       'acsf_49_a0', 'acsf_50_a0', 'acsf_51_a0', 'acsf_52_a0',\n",
       "       'acsf_53_a0', 'acsf_54_a0', 'acsf_55_a0', 'acsf_56_a0',\n",
       "       'acsf_57_a0', 'acsf_58_a0', 'acsf_59_a0', 'acsf_60_a0',\n",
       "       'acsf_61_a0', 'acsf_62_a0', 'acsf_63_a0', 'acsf_64_a0',\n",
       "       'acsf_65_a0', 'acsf_66_a0', 'acsf_67_a0', 'acsf_68_a0',\n",
       "       'acsf_69_a0', 'acsf_70_a0', 'acsf_71_a0', 'acsf_72_a0',\n",
       "       'acsf_73_a0', 'acsf_74_a0', 'acsf_75_a0', 'acsf_76_a0',\n",
       "       'acsf_77_a0', 'acsf_78_a0', 'acsf_79_a0', 'acsf_80_a0',\n",
       "       'acsf_81_a0', 'acsf_82_a0', 'acsf_83_a0', 'acsf_84_a0',\n",
       "       'acsf_85_a0', 'acsf_86_a0', 'acsf_87_a0', 'acsf_88_a0',\n",
       "       'acsf_89_a0', 'acsf_90_a0', 'acsf_91_a0', 'acsf_92_a0',\n",
       "       'acsf_93_a0', 'acsf_94_a0', 'acsf_95_a0', 'acsf_96_a0',\n",
       "       'acsf_97_a0', 'acsf_98_a0', 'acsf_99_a0', 'acsf_100_a0',\n",
       "       'acsf_101_a0', 'acsf_102_a0', 'acsf_103_a0', 'acsf_104_a0',\n",
       "       'acsf_105_a0', 'acsf_106_a0', 'acsf_107_a0', 'acsf_108_a0',\n",
       "       'acsf_109_a0', 'acsf_110_a0', 'acsf_111_a0', 'acsf_112_a0',\n",
       "       'acsf_113_a0', 'acsf_114_a0', 'acsf_115_a0', 'acsf_116_a0',\n",
       "       'acsf_117_a0', 'acsf_118_a0', 'acsf_119_a0', 'acsf_120_a0',\n",
       "       'acsf_121_a0', 'acsf_122_a0', 'acsf_123_a0', 'acsf_124_a0',\n",
       "       'acsf_0_a1', 'acsf_1_a1', 'acsf_2_a1', 'acsf_3_a1', 'acsf_4_a1',\n",
       "       'acsf_5_a1', 'acsf_6_a1', 'acsf_7_a1', 'acsf_8_a1', 'acsf_9_a1',\n",
       "       'acsf_10_a1', 'acsf_11_a1', 'acsf_12_a1', 'acsf_13_a1',\n",
       "       'acsf_14_a1', 'acsf_15_a1', 'acsf_16_a1', 'acsf_17_a1',\n",
       "       'acsf_18_a1', 'acsf_19_a1', 'acsf_20_a1', 'acsf_21_a1',\n",
       "       'acsf_22_a1', 'acsf_23_a1', 'acsf_24_a1', 'acsf_25_a1',\n",
       "       'acsf_26_a1', 'acsf_27_a1', 'acsf_28_a1', 'acsf_29_a1',\n",
       "       'acsf_30_a1', 'acsf_31_a1', 'acsf_32_a1', 'acsf_33_a1',\n",
       "       'acsf_34_a1', 'acsf_35_a1', 'acsf_36_a1', 'acsf_37_a1',\n",
       "       'acsf_38_a1', 'acsf_39_a1', 'acsf_40_a1', 'acsf_41_a1',\n",
       "       'acsf_42_a1', 'acsf_43_a1', 'acsf_44_a1', 'acsf_45_a1',\n",
       "       'acsf_46_a1', 'acsf_47_a1', 'acsf_48_a1', 'acsf_49_a1',\n",
       "       'acsf_50_a1', 'acsf_51_a1', 'acsf_52_a1', 'acsf_53_a1',\n",
       "       'acsf_54_a1', 'acsf_55_a1', 'acsf_56_a1', 'acsf_57_a1',\n",
       "       'acsf_58_a1', 'acsf_59_a1', 'acsf_60_a1', 'acsf_61_a1',\n",
       "       'acsf_62_a1', 'acsf_63_a1', 'acsf_64_a1', 'acsf_65_a1',\n",
       "       'acsf_66_a1', 'acsf_67_a1', 'acsf_68_a1', 'acsf_69_a1',\n",
       "       'acsf_70_a1', 'acsf_71_a1', 'acsf_72_a1', 'acsf_73_a1',\n",
       "       'acsf_74_a1', 'acsf_75_a1', 'acsf_76_a1', 'acsf_77_a1',\n",
       "       'acsf_78_a1', 'acsf_79_a1', 'acsf_80_a1', 'acsf_81_a1',\n",
       "       'acsf_82_a1', 'acsf_83_a1', 'acsf_84_a1', 'acsf_85_a1',\n",
       "       'acsf_86_a1', 'acsf_87_a1', 'acsf_88_a1', 'acsf_89_a1',\n",
       "       'acsf_90_a1', 'acsf_91_a1', 'acsf_92_a1', 'acsf_93_a1',\n",
       "       'acsf_94_a1', 'acsf_95_a1', 'acsf_96_a1', 'acsf_97_a1',\n",
       "       'acsf_98_a1', 'acsf_99_a1', 'acsf_100_a1', 'acsf_101_a1',\n",
       "       'acsf_102_a1', 'acsf_103_a1', 'acsf_104_a1', 'acsf_105_a1',\n",
       "       'acsf_106_a1', 'acsf_107_a1', 'acsf_108_a1', 'acsf_109_a1',\n",
       "       'acsf_110_a1', 'acsf_111_a1', 'acsf_112_a1', 'acsf_113_a1',\n",
       "       'acsf_114_a1', 'acsf_115_a1', 'acsf_116_a1', 'acsf_117_a1',\n",
       "       'acsf_118_a1', 'acsf_119_a1', 'acsf_120_a1', 'acsf_121_a1',\n",
       "       'acsf_122_a1', 'acsf_123_a1', 'acsf_124_a1', 'atom_2', 'atom_3',\n",
       "       'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9',\n",
       "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
       "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3',\n",
       "       'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2',\n",
       "       'd_7_3', 'd_8_0', 'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1',\n",
       "       'd_9_2', 'd_9_3'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up GPU preferences\n",
    "#config = tf.ConfigProto( device_count = {'GPU': 2 , 'CPU': 1} ) \n",
    "#config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "#sess = tf.Session(config=config) \n",
    "#K.set_session(sess)\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "\n",
    "K.clear_session()\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(1024, activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"linear\")(x)  \n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    return model\n",
    "\n",
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "mol_types=train[\"type\"].unique()\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 2000\n",
    "verbose = 1\n",
    "batch_size = 1024\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain =True\n",
    "\n",
    "start_time=datetime.now()\n",
    "test_prediction=np.zeros(len(test))\n",
    "\n",
    "#del train, test\n",
    "#gc.collect()\n",
    "\n",
    "cols = list(X_train.columns.values)\n",
    "\n",
    "oh_cols = ['atom_0', 'atom_1', 'atom_2', 'atom_3', 'atom_4', 'atom_5', \n",
    "           'atom_6', 'atom_7', 'atom_8','atom_9']\n",
    "           #'tertiary_atom_0', 'tertiary_atom_1', 'tertiary_atom_2', 'tertiary_atom_3',\n",
    "         # 'tertiary_atom_4', 'tertiary_atom_5', 'tertiary_atom_6', 'tertiary_atom_7',\n",
    "         # 'tertiary_atom_8', 'tertiary_atom_9', 'bond_atom']\n",
    "\n",
    "continuous_cols = [col for col in cols if col not in oh_cols]\n",
    "\n",
    "\n",
    "class FeatureTransformer:\n",
    "    def transform(self, dataset, ohe_features=[], continuous_features=[]):\n",
    "        ohe_df = OneHotEncoder().fit_transform(dataset.loc[:, ohe_features]).toarray()\n",
    "        skews = dataset.loc[:, continuous_features].skew().to_dict()\n",
    "        for column, skew in skews.items():\n",
    "            if skew > 1:\n",
    "                dataset[column] = np.log1p(dataset[column])\n",
    "        return np.concatenate([StandardScaler().fit_transform(dataset.loc[:, continuous_features]), ohe_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for col in X_train.columns:\n",
    "    try:\n",
    "        X_train[col].fillna(0, inplace=True)\n",
    "    except:\n",
    "        X_train[col] = X_train[col].cat.add_categories([0])\n",
    "        X_train[col].fillna(0, inplace=True)\n",
    "    \n",
    "for col in X_test.columns:\n",
    "    try:\n",
    "        X_test[col].fillna(0, inplace=True)\n",
    "    except:\n",
    "        X_test[col] = X_test[col].cat.add_categories([0])\n",
    "        X_test[col].fillna(0, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_type = train['type'].values\n",
    "test_type = test['type'].values\n",
    "molecule_name = train['molecule_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 567440 samples, validate on 141976 samples\n",
      "Epoch 1/2000\n",
      "567440/567440 [==============================] - 7s 13us/step - loss: 44.3702 - val_loss: 2.8876\n",
      "Epoch 2/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.8429 - val_loss: 1.6080\n",
      "Epoch 3/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.5910 - val_loss: 1.5128\n",
      "Epoch 4/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.4797 - val_loss: 1.3617\n",
      "Epoch 5/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.4148 - val_loss: 1.2535\n",
      "Epoch 6/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.3645 - val_loss: 1.2485\n",
      "Epoch 7/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.3085 - val_loss: 1.2413\n",
      "Epoch 8/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.2889 - val_loss: 1.2256\n",
      "Epoch 9/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.2450 - val_loss: 1.1344\n",
      "Epoch 10/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.2224 - val_loss: 1.1635\n",
      "Epoch 11/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.2077 - val_loss: 1.0287\n",
      "Epoch 12/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1848 - val_loss: 1.0590\n",
      "Epoch 13/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1613 - val_loss: 1.0715\n",
      "Epoch 14/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1658 - val_loss: 1.0105\n",
      "Epoch 15/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1448 - val_loss: 1.0053\n",
      "Epoch 16/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1313 - val_loss: 1.0999\n",
      "Epoch 17/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1165 - val_loss: 1.0043\n",
      "Epoch 18/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.1085 - val_loss: 0.9279\n",
      "Epoch 19/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0879 - val_loss: 0.9375\n",
      "Epoch 20/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0672 - val_loss: 1.0216\n",
      "Epoch 21/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0772 - val_loss: 0.9297\n",
      "Epoch 22/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0521 - val_loss: 0.9626\n",
      "Epoch 23/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0580 - val_loss: 0.9665\n",
      "Epoch 24/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0462 - val_loss: 0.8832\n",
      "Epoch 25/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0396 - val_loss: 0.9705\n",
      "Epoch 26/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0345 - val_loss: 0.9616\n",
      "Epoch 27/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0163 - val_loss: 1.0519\n",
      "Epoch 28/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0185 - val_loss: 0.8848\n",
      "Epoch 29/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0026 - val_loss: 0.9373\n",
      "Epoch 30/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0018 - val_loss: 0.9704\n",
      "Epoch 31/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 1.0087 - val_loss: 0.8293\n",
      "Epoch 32/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9842 - val_loss: 0.9221\n",
      "Epoch 33/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9726 - val_loss: 0.8289\n",
      "Epoch 34/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9703 - val_loss: 0.9269\n",
      "Epoch 35/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9723 - val_loss: 0.8260\n",
      "Epoch 36/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9732 - val_loss: 0.8642\n",
      "Epoch 37/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9677 - val_loss: 0.8249\n",
      "Epoch 38/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9806 - val_loss: 0.8064\n",
      "Epoch 39/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9522 - val_loss: 0.8656\n",
      "Epoch 40/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9552 - val_loss: 0.8413\n",
      "Epoch 41/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9493 - val_loss: 0.8195\n",
      "Epoch 42/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9379 - val_loss: 0.8590\n",
      "Epoch 43/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9483 - val_loss: 0.8443\n",
      "Epoch 44/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9504 - val_loss: 0.8126\n",
      "Epoch 45/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9358 - val_loss: 0.8146\n",
      "Epoch 46/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9129 - val_loss: 0.8294\n",
      "Epoch 47/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9422 - val_loss: 0.9032\n",
      "Epoch 48/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9088 - val_loss: 0.8397\n",
      "Epoch 49/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9338 - val_loss: 0.8253\n",
      "Epoch 50/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9084 - val_loss: 0.7976\n",
      "Epoch 51/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9106 - val_loss: 0.8339\n",
      "Epoch 52/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9204 - val_loss: 0.8293\n",
      "Epoch 53/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9240 - val_loss: 0.8070\n",
      "Epoch 54/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9021 - val_loss: 0.7776\n",
      "Epoch 55/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9103 - val_loss: 0.8495\n",
      "Epoch 56/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.9077 - val_loss: 0.7781\n",
      "Epoch 57/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8929 - val_loss: 0.8077\n",
      "Epoch 58/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8918 - val_loss: 0.7833\n",
      "Epoch 59/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8716 - val_loss: 0.7832\n",
      "Epoch 60/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8972 - val_loss: 0.8294\n",
      "Epoch 61/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8844 - val_loss: 0.7633\n",
      "Epoch 62/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8891 - val_loss: 0.7685\n",
      "Epoch 63/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8985 - val_loss: 0.7513\n",
      "Epoch 64/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8763 - val_loss: 0.7854\n",
      "Epoch 65/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8615 - val_loss: 0.8948\n",
      "Epoch 66/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8725 - val_loss: 0.7627\n",
      "Epoch 67/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8710 - val_loss: 0.8407\n",
      "Epoch 68/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8555 - val_loss: 0.7389\n",
      "Epoch 69/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8831 - val_loss: 0.8405\n",
      "Epoch 70/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8938 - val_loss: 0.7512\n",
      "Epoch 71/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8676 - val_loss: 0.8513\n",
      "Epoch 72/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8580 - val_loss: 0.7365\n",
      "Epoch 73/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8589 - val_loss: 0.7575\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8706 - val_loss: 0.7249\n",
      "Epoch 75/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8526 - val_loss: 0.7392\n",
      "Epoch 76/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8612 - val_loss: 0.7474\n",
      "Epoch 77/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8742 - val_loss: 0.7764\n",
      "Epoch 78/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8462 - val_loss: 0.7589\n",
      "Epoch 79/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8605 - val_loss: 0.7547\n",
      "Epoch 80/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8516 - val_loss: 0.8753\n",
      "Epoch 81/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8473 - val_loss: 0.8421\n",
      "Epoch 82/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8527 - val_loss: 0.7963\n",
      "Epoch 83/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8399 - val_loss: 0.7536\n",
      "Epoch 84/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8323 - val_loss: 0.7538\n",
      "Epoch 85/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8565 - val_loss: 0.7610\n",
      "Epoch 86/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8712 - val_loss: 0.7269\n",
      "Epoch 87/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8333 - val_loss: 0.7475\n",
      "Epoch 88/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8302 - val_loss: 0.7657\n",
      "Epoch 89/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8466 - val_loss: 0.7157\n",
      "Epoch 90/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8430 - val_loss: 0.7125\n",
      "Epoch 91/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8459 - val_loss: 0.7972\n",
      "Epoch 92/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8294 - val_loss: 0.7094\n",
      "Epoch 93/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8409 - val_loss: 0.7295\n",
      "Epoch 94/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8400 - val_loss: 0.7284\n",
      "Epoch 95/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8417 - val_loss: 0.7863\n",
      "Epoch 96/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8250 - val_loss: 0.7208\n",
      "Epoch 97/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8276 - val_loss: 0.7671\n",
      "Epoch 98/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8073 - val_loss: 0.7032\n",
      "Epoch 99/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8311 - val_loss: 0.7311\n",
      "Epoch 100/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8395 - val_loss: 0.7033\n",
      "Epoch 101/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8470 - val_loss: 0.7167\n",
      "Epoch 102/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8149 - val_loss: 0.8557\n",
      "Epoch 103/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8322 - val_loss: 0.7168\n",
      "Epoch 104/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8217 - val_loss: 0.6958\n",
      "Epoch 105/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8262 - val_loss: 0.7792\n",
      "Epoch 106/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8131 - val_loss: 0.6785\n",
      "Epoch 107/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8092 - val_loss: 0.7804\n",
      "Epoch 108/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8131 - val_loss: 0.8169\n",
      "Epoch 109/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8002 - val_loss: 0.7013\n",
      "Epoch 110/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8063 - val_loss: 0.7613\n",
      "Epoch 111/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8265 - val_loss: 0.7985\n",
      "Epoch 112/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8160 - val_loss: 0.7244\n",
      "Epoch 113/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8338 - val_loss: 0.7837\n",
      "Epoch 114/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8271 - val_loss: 0.8226\n",
      "Epoch 115/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8010 - val_loss: 0.7183\n",
      "Epoch 116/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8118 - val_loss: 0.7616\n",
      "Epoch 117/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8219 - val_loss: 0.7013\n",
      "Epoch 118/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8051 - val_loss: 0.7148\n",
      "Epoch 119/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7903 - val_loss: 0.7060\n",
      "Epoch 120/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8107 - val_loss: 0.6838\n",
      "Epoch 121/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7940 - val_loss: 0.6944\n",
      "Epoch 122/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8113 - val_loss: 0.7315\n",
      "Epoch 123/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7942 - val_loss: 0.7260\n",
      "Epoch 124/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8160 - val_loss: 0.7155\n",
      "Epoch 125/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8103 - val_loss: 0.7601\n",
      "Epoch 126/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8117 - val_loss: 0.7471\n",
      "Epoch 127/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7840 - val_loss: 0.7182\n",
      "Epoch 128/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7953 - val_loss: 0.6922\n",
      "Epoch 129/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7900 - val_loss: 0.6908\n",
      "Epoch 130/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7949 - val_loss: 0.7021\n",
      "Epoch 131/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7997 - val_loss: 0.6689\n",
      "Epoch 132/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7745 - val_loss: 0.8518\n",
      "Epoch 133/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7958 - val_loss: 0.6887\n",
      "Epoch 134/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7994 - val_loss: 0.6995\n",
      "Epoch 135/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7829 - val_loss: 0.6797\n",
      "Epoch 136/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7928 - val_loss: 0.6807\n",
      "Epoch 137/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7944 - val_loss: 0.7220\n",
      "Epoch 138/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7811 - val_loss: 0.7166\n",
      "Epoch 139/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7930 - val_loss: 0.7537\n",
      "Epoch 140/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7901 - val_loss: 0.7018\n",
      "Epoch 141/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7909 - val_loss: 0.7599\n",
      "Epoch 142/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8021 - val_loss: 0.6976\n",
      "Epoch 143/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7738 - val_loss: 0.6813\n",
      "Epoch 144/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7825 - val_loss: 0.7238\n",
      "Epoch 145/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7684 - val_loss: 0.6788\n",
      "Epoch 146/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7664 - val_loss: 0.7259\n",
      "Epoch 147/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7746 - val_loss: 0.7893\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7732 - val_loss: 0.7398\n",
      "Epoch 149/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7729 - val_loss: 0.6688\n",
      "Epoch 150/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7949 - val_loss: 0.7021\n",
      "Epoch 151/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7739 - val_loss: 0.6952\n",
      "Epoch 152/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7790 - val_loss: 0.7569\n",
      "Epoch 153/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7793 - val_loss: 0.6867\n",
      "Epoch 154/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7930 - val_loss: 0.7695\n",
      "Epoch 155/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7565 - val_loss: 0.6853\n",
      "Epoch 156/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7601 - val_loss: 0.6795\n",
      "Epoch 157/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7689 - val_loss: 0.7571\n",
      "Epoch 158/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7641 - val_loss: 0.6650\n",
      "Epoch 159/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7799 - val_loss: 0.7758\n",
      "Epoch 160/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7821 - val_loss: 0.7108\n",
      "Epoch 161/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7515 - val_loss: 0.6617\n",
      "Epoch 162/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7718 - val_loss: 0.6785\n",
      "Epoch 163/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7712 - val_loss: 0.7479\n",
      "Epoch 164/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7776 - val_loss: 0.6550\n",
      "Epoch 165/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7834 - val_loss: 0.7052\n",
      "Epoch 166/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7647 - val_loss: 0.6887\n",
      "Epoch 167/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7690 - val_loss: 0.6810\n",
      "Epoch 168/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.8020 - val_loss: 0.6798\n",
      "Epoch 169/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7916 - val_loss: 0.6886\n",
      "Epoch 170/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7530 - val_loss: 0.6547\n",
      "Epoch 171/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7540 - val_loss: 0.8298\n",
      "Epoch 172/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7700 - val_loss: 0.7136\n",
      "Epoch 173/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7755 - val_loss: 0.7407\n",
      "Epoch 174/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7782 - val_loss: 0.6698\n",
      "Epoch 175/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7449 - val_loss: 0.8243\n",
      "Epoch 176/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7694 - val_loss: 0.8607\n",
      "Epoch 177/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7633 - val_loss: 0.6673\n",
      "Epoch 178/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7661 - val_loss: 0.6859\n",
      "Epoch 179/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7641 - val_loss: 0.6891\n",
      "Epoch 180/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7434 - val_loss: 0.6607\n",
      "Epoch 181/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7912 - val_loss: 0.6728\n",
      "Epoch 182/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7529 - val_loss: 0.8378\n",
      "Epoch 183/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7604 - val_loss: 0.7848\n",
      "Epoch 184/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7657 - val_loss: 0.6931\n",
      "Epoch 185/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7698 - val_loss: 0.6932\n",
      "Epoch 186/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7755 - val_loss: 0.6413\n",
      "Epoch 187/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7624 - val_loss: 0.7091\n",
      "Epoch 188/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7770 - val_loss: 0.7682\n",
      "Epoch 189/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7815 - val_loss: 0.6736\n",
      "Epoch 190/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7510 - val_loss: 0.7428\n",
      "Epoch 191/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7650 - val_loss: 0.7001\n",
      "Epoch 192/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7438 - val_loss: 0.7421\n",
      "Epoch 193/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7542 - val_loss: 0.6824\n",
      "Epoch 194/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7594 - val_loss: 0.7177\n",
      "Epoch 195/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7826 - val_loss: 0.7813\n",
      "Epoch 196/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7496 - val_loss: 0.6635\n",
      "Epoch 197/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7790 - val_loss: 0.6641\n",
      "Epoch 198/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7686 - val_loss: 0.6643\n",
      "Epoch 199/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7610 - val_loss: 0.6863\n",
      "Epoch 200/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7441 - val_loss: 0.6526\n",
      "Epoch 201/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7532 - val_loss: 0.6430\n",
      "Epoch 202/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7500 - val_loss: 0.6628\n",
      "Epoch 203/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7764 - val_loss: 0.6992\n",
      "Epoch 204/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7405 - val_loss: 0.6480\n",
      "Epoch 205/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7543 - val_loss: 0.8368\n",
      "Epoch 206/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7458 - val_loss: 0.7359\n",
      "Epoch 207/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7367 - val_loss: 0.6651\n",
      "Epoch 208/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7504 - val_loss: 0.6947\n",
      "Epoch 209/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7488 - val_loss: 0.6764\n",
      "Epoch 210/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7379 - val_loss: 0.6670\n",
      "Epoch 211/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7404 - val_loss: 0.6386\n",
      "Epoch 212/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7720 - val_loss: 0.6807\n",
      "Epoch 213/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7360 - val_loss: 0.7384\n",
      "Epoch 214/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7350 - val_loss: 0.6682\n",
      "Epoch 215/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7558 - val_loss: 0.6606\n",
      "Epoch 216/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7500 - val_loss: 0.6383\n",
      "Epoch 217/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7512 - val_loss: 0.6463\n",
      "Epoch 218/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7551 - val_loss: 0.6576\n",
      "Epoch 219/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7480 - val_loss: 0.6286\n",
      "Epoch 220/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7137 - val_loss: 0.6702\n",
      "Epoch 221/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7552 - val_loss: 0.6385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7388 - val_loss: 0.8663\n",
      "Epoch 223/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7573 - val_loss: 0.6715\n",
      "Epoch 224/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7405 - val_loss: 0.6421\n",
      "Epoch 225/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7419 - val_loss: 0.8095\n",
      "Epoch 226/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7643 - val_loss: 0.6537\n",
      "Epoch 227/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7594 - val_loss: 0.7639\n",
      "Epoch 228/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7505 - val_loss: 0.6875\n",
      "Epoch 229/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7324 - val_loss: 0.6453\n",
      "Epoch 230/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7353 - val_loss: 0.6500\n",
      "Epoch 231/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7431 - val_loss: 0.6936\n",
      "Epoch 232/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7572 - val_loss: 0.6585\n",
      "Epoch 233/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7568 - val_loss: 0.6513\n",
      "Epoch 234/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7422 - val_loss: 0.6757\n",
      "Epoch 235/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7184 - val_loss: 0.7410\n",
      "Epoch 236/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7320 - val_loss: 0.6840\n",
      "Epoch 237/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7523 - val_loss: 0.8088\n",
      "Epoch 238/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7244 - val_loss: 0.6569\n",
      "Epoch 239/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7387 - val_loss: 0.6379\n",
      "Epoch 240/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7409 - val_loss: 0.6210\n",
      "Epoch 241/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7772 - val_loss: 0.6967\n",
      "Epoch 242/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7449 - val_loss: 0.8686\n",
      "Epoch 243/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7527 - val_loss: 0.6479\n",
      "Epoch 244/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7255 - val_loss: 0.6755\n",
      "Epoch 245/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7388 - val_loss: 0.6205\n",
      "Epoch 246/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7543 - val_loss: 0.6903\n",
      "Epoch 247/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7301 - val_loss: 0.6474\n",
      "Epoch 248/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7383 - val_loss: 0.6385\n",
      "Epoch 249/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7450 - val_loss: 0.6438\n",
      "Epoch 250/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7363 - val_loss: 0.7799\n",
      "Epoch 251/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7189 - val_loss: 0.7904\n",
      "Epoch 252/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7341 - val_loss: 0.6422\n",
      "Epoch 253/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7356 - val_loss: 0.6391\n",
      "Epoch 254/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7478 - val_loss: 0.6311\n",
      "Epoch 255/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7407 - val_loss: 0.6298\n",
      "Epoch 256/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7260 - val_loss: 0.6290\n",
      "Epoch 257/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7479 - val_loss: 0.7837\n",
      "Epoch 258/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7409 - val_loss: 0.6352\n",
      "Epoch 259/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7349 - val_loss: 0.6421\n",
      "Epoch 260/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7245 - val_loss: 0.6221\n",
      "Epoch 261/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7352 - val_loss: 0.6636\n",
      "Epoch 262/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7435 - val_loss: 0.6319\n",
      "Epoch 263/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7429 - val_loss: 0.6514\n",
      "Epoch 264/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7416 - val_loss: 0.6662\n",
      "Epoch 265/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7364 - val_loss: 0.6367\n",
      "Epoch 266/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7166 - val_loss: 0.7582\n",
      "Epoch 267/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7332 - val_loss: 0.6952\n",
      "Epoch 268/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7287 - val_loss: 0.6939\n",
      "Epoch 269/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7420 - val_loss: 0.6202\n",
      "Epoch 270/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7352 - val_loss: 0.6298\n",
      "Epoch 271/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7165 - val_loss: 0.6227\n",
      "Epoch 272/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7304 - val_loss: 0.7059\n",
      "Epoch 273/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7176 - val_loss: 0.6875\n",
      "Epoch 274/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7425 - val_loss: 0.6234\n",
      "Epoch 275/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7431 - val_loss: 0.6583\n",
      "Epoch 276/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7341 - val_loss: 0.8280\n",
      "Epoch 277/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7479 - val_loss: 0.6600\n",
      "Epoch 278/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7371 - val_loss: 0.7602\n",
      "Epoch 279/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7139 - val_loss: 0.6220\n",
      "Epoch 280/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7121 - val_loss: 0.6975\n",
      "Epoch 281/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7412 - val_loss: 0.6224\n",
      "Epoch 282/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7399 - val_loss: 0.6260\n",
      "Epoch 283/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7126 - val_loss: 0.6395\n",
      "Epoch 284/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7242 - val_loss: 0.8707\n",
      "Epoch 285/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7217 - val_loss: 0.6339\n",
      "Epoch 286/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7281 - val_loss: 0.7335\n",
      "Epoch 287/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7366 - val_loss: 0.6260\n",
      "Epoch 288/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7507 - val_loss: 0.8338\n",
      "Epoch 289/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7275 - val_loss: 0.6781\n",
      "Epoch 290/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7495 - val_loss: 0.6503\n",
      "Epoch 291/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7309 - val_loss: 0.7038\n",
      "Epoch 292/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6978 - val_loss: 0.6721\n",
      "Epoch 293/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7136 - val_loss: 0.6797\n",
      "Epoch 294/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7312 - val_loss: 0.6306\n",
      "Epoch 295/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7042 - val_loss: 0.8084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7286 - val_loss: 0.6154\n",
      "Epoch 297/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7311 - val_loss: 0.6175\n",
      "Epoch 298/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7321 - val_loss: 0.6761\n",
      "Epoch 299/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7352 - val_loss: 0.7081\n",
      "Epoch 300/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7246 - val_loss: 0.6229\n",
      "Epoch 301/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7255 - val_loss: 0.7210\n",
      "Epoch 302/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7234 - val_loss: 0.6622\n",
      "Epoch 303/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7181 - val_loss: 0.6478\n",
      "Epoch 304/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7191 - val_loss: 0.6582\n",
      "Epoch 305/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7383 - val_loss: 0.6215\n",
      "Epoch 306/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7141 - val_loss: 0.6240\n",
      "Epoch 307/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7138 - val_loss: 0.6456\n",
      "Epoch 308/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7188 - val_loss: 0.6178\n",
      "Epoch 309/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7182 - val_loss: 0.6799\n",
      "Epoch 310/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7232 - val_loss: 0.7055\n",
      "Epoch 311/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7509 - val_loss: 0.6134\n",
      "Epoch 312/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7218 - val_loss: 0.6739\n",
      "Epoch 313/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6987 - val_loss: 0.6275\n",
      "Epoch 314/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7079 - val_loss: 0.6343\n",
      "Epoch 315/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7234 - val_loss: 0.6191\n",
      "Epoch 316/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7381 - val_loss: 0.6583\n",
      "Epoch 317/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7221 - val_loss: 0.6588\n",
      "Epoch 318/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7177 - val_loss: 0.6703\n",
      "Epoch 319/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7327 - val_loss: 0.6509\n",
      "Epoch 320/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7135 - val_loss: 0.7103\n",
      "Epoch 321/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7014 - val_loss: 0.7031\n",
      "Epoch 322/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7237 - val_loss: 0.6510\n",
      "Epoch 323/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7399 - val_loss: 0.6243\n",
      "Epoch 324/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7109 - val_loss: 0.6168\n",
      "Epoch 325/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7199 - val_loss: 0.6421\n",
      "Epoch 326/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7234 - val_loss: 0.7077\n",
      "Epoch 327/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7288 - val_loss: 0.6429\n",
      "Epoch 328/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7069 - val_loss: 0.6177\n",
      "Epoch 329/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7168 - val_loss: 0.6182\n",
      "Epoch 330/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7432 - val_loss: 0.7545\n",
      "Epoch 331/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7324 - val_loss: 1.1216\n",
      "Epoch 332/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7189 - val_loss: 0.6326\n",
      "Epoch 333/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7144 - val_loss: 0.6566\n",
      "Epoch 334/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7096 - val_loss: 0.6156\n",
      "Epoch 335/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7254 - val_loss: 0.6155\n",
      "Epoch 336/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7109 - val_loss: 0.6190\n",
      "Epoch 337/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7312 - val_loss: 0.6344\n",
      "Epoch 338/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7329 - val_loss: 0.6396\n",
      "Epoch 339/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7074 - val_loss: 0.9796\n",
      "Epoch 340/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7289 - val_loss: 0.7050\n",
      "Epoch 341/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7063 - val_loss: 0.6066\n",
      "Epoch 342/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7325 - val_loss: 0.6634\n",
      "Epoch 343/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6938 - val_loss: 0.6311\n",
      "Epoch 344/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7244 - val_loss: 0.6136\n",
      "Epoch 345/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7003 - val_loss: 0.6227\n",
      "Epoch 346/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6989 - val_loss: 0.6396\n",
      "Epoch 347/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7168 - val_loss: 0.6035\n",
      "Epoch 348/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7122 - val_loss: 0.6276\n",
      "Epoch 349/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7314 - val_loss: 0.7179\n",
      "Epoch 350/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7215 - val_loss: 0.6886\n",
      "Epoch 351/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7108 - val_loss: 0.6017\n",
      "Epoch 352/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7167 - val_loss: 0.6263\n",
      "Epoch 353/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7099 - val_loss: 0.6125\n",
      "Epoch 354/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6967 - val_loss: 0.6691\n",
      "Epoch 355/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7104 - val_loss: 0.6338\n",
      "Epoch 356/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7313 - val_loss: 0.6044\n",
      "Epoch 357/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7117 - val_loss: 0.7277\n",
      "Epoch 358/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6968 - val_loss: 0.6098\n",
      "Epoch 359/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6982 - val_loss: 0.7040\n",
      "Epoch 360/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7305 - val_loss: 0.7216\n",
      "Epoch 361/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7024 - val_loss: 0.6085\n",
      "Epoch 362/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7112 - val_loss: 0.6339\n",
      "Epoch 363/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7272 - val_loss: 0.6074\n",
      "Epoch 364/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7050 - val_loss: 0.6108\n",
      "Epoch 365/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7104 - val_loss: 0.6378\n",
      "Epoch 366/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7258 - val_loss: 0.6116\n",
      "Epoch 367/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6911 - val_loss: 0.8922\n",
      "Epoch 368/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7226 - val_loss: 0.6899\n",
      "Epoch 369/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7026 - val_loss: 0.6048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7035 - val_loss: 0.7239\n",
      "Epoch 371/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6996 - val_loss: 0.6001\n",
      "Epoch 372/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7099 - val_loss: 0.6194\n",
      "Epoch 373/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7273 - val_loss: 0.6230\n",
      "Epoch 374/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7384 - val_loss: 0.6216\n",
      "Epoch 375/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6907 - val_loss: 0.6602\n",
      "Epoch 376/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7122 - val_loss: 0.7327\n",
      "Epoch 377/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7175 - val_loss: 0.6523\n",
      "Epoch 378/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7105 - val_loss: 0.6110\n",
      "Epoch 379/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7005 - val_loss: 0.6070\n",
      "Epoch 380/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6944 - val_loss: 0.6213\n",
      "Epoch 381/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7239 - val_loss: 0.8986\n",
      "Epoch 382/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7115 - val_loss: 0.6413\n",
      "Epoch 383/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6955 - val_loss: 0.7003\n",
      "Epoch 384/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6967 - val_loss: 0.6327\n",
      "Epoch 385/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7122 - val_loss: 0.7360\n",
      "Epoch 386/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7159 - val_loss: 0.6075\n",
      "Epoch 387/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7089 - val_loss: 0.6098\n",
      "Epoch 388/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7120 - val_loss: 0.6095\n",
      "Epoch 389/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6746 - val_loss: 0.6258\n",
      "Epoch 390/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7286 - val_loss: 0.8812\n",
      "Epoch 391/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7016 - val_loss: 0.6033\n",
      "Epoch 392/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7052 - val_loss: 0.7412\n",
      "Epoch 393/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6848 - val_loss: 0.6897\n",
      "Epoch 394/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7125 - val_loss: 0.6302\n",
      "Epoch 395/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7027 - val_loss: 0.6167\n",
      "Epoch 396/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7164 - val_loss: 0.6170\n",
      "Epoch 397/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7175 - val_loss: 0.6222\n",
      "Epoch 398/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7197 - val_loss: 0.6295\n",
      "Epoch 399/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7062 - val_loss: 0.5995\n",
      "Epoch 400/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7006 - val_loss: 0.6890\n",
      "Epoch 401/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7126 - val_loss: 0.8044\n",
      "Epoch 402/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7094 - val_loss: 0.6014\n",
      "Epoch 403/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7013 - val_loss: 0.6371\n",
      "Epoch 404/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7089 - val_loss: 0.6852\n",
      "Epoch 405/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7065 - val_loss: 0.7705\n",
      "Epoch 406/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6925 - val_loss: 0.7719\n",
      "Epoch 407/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7074 - val_loss: 0.7312\n",
      "Epoch 408/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6981 - val_loss: 0.6595\n",
      "Epoch 409/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6981 - val_loss: 0.5993\n",
      "Epoch 410/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7095 - val_loss: 0.6206\n",
      "Epoch 411/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6862 - val_loss: 0.6127\n",
      "Epoch 412/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7174 - val_loss: 0.6681\n",
      "Epoch 413/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7178 - val_loss: 0.6919\n",
      "Epoch 414/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6851 - val_loss: 0.6205\n",
      "Epoch 415/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7212 - val_loss: 0.6360\n",
      "Epoch 416/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7381 - val_loss: 0.6761\n",
      "Epoch 417/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7219 - val_loss: 0.7591\n",
      "Epoch 418/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7052 - val_loss: 0.6143\n",
      "Epoch 419/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7138 - val_loss: 0.6951\n",
      "Epoch 420/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6862 - val_loss: 0.5995\n",
      "Epoch 421/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6909 - val_loss: 0.6483\n",
      "Epoch 422/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7337 - val_loss: 0.6897\n",
      "Epoch 423/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6805 - val_loss: 0.6819\n",
      "Epoch 424/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7037 - val_loss: 0.7917\n",
      "Epoch 425/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6984 - val_loss: 0.6156\n",
      "Epoch 426/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6855 - val_loss: 0.6151\n",
      "Epoch 427/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6937 - val_loss: 0.6139\n",
      "Epoch 428/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7148 - val_loss: 0.7330\n",
      "Epoch 429/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6959 - val_loss: 0.6697\n",
      "Epoch 430/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7185 - val_loss: 0.6069\n",
      "Epoch 431/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6911 - val_loss: 0.6077\n",
      "Epoch 432/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7100 - val_loss: 0.6391\n",
      "Epoch 433/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7111 - val_loss: 0.6744\n",
      "Epoch 434/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6798 - val_loss: 0.5955\n",
      "Epoch 435/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7142 - val_loss: 0.7893\n",
      "Epoch 436/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6965 - val_loss: 0.6038\n",
      "Epoch 437/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7111 - val_loss: 0.6226\n",
      "Epoch 438/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6890 - val_loss: 0.6099\n",
      "Epoch 439/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7045 - val_loss: 0.7103\n",
      "Epoch 440/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6875 - val_loss: 0.6644\n",
      "Epoch 441/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6778 - val_loss: 0.5955\n",
      "Epoch 442/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6809 - val_loss: 0.6318\n",
      "Epoch 443/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6936 - val_loss: 0.6799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6962 - val_loss: 0.6143\n",
      "Epoch 445/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7014 - val_loss: 0.6122\n",
      "Epoch 446/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6780 - val_loss: 0.5904\n",
      "Epoch 447/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7073 - val_loss: 0.6071\n",
      "Epoch 448/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6983 - val_loss: 0.6032\n",
      "Epoch 449/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6887 - val_loss: 0.5958\n",
      "Epoch 450/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6896 - val_loss: 0.6051\n",
      "Epoch 451/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6988 - val_loss: 0.6182\n",
      "Epoch 452/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7014 - val_loss: 0.6221\n",
      "Epoch 453/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6845 - val_loss: 0.6572\n",
      "Epoch 454/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6841 - val_loss: 0.7893\n",
      "Epoch 455/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7014 - val_loss: 0.6171\n",
      "Epoch 456/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7132 - val_loss: 0.6056\n",
      "Epoch 457/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7089 - val_loss: 0.6521\n",
      "Epoch 458/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7005 - val_loss: 0.6801\n",
      "Epoch 459/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7026 - val_loss: 0.7416\n",
      "Epoch 460/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6854 - val_loss: 0.6191\n",
      "Epoch 461/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6808 - val_loss: 0.5995\n",
      "Epoch 462/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7016 - val_loss: 0.5985\n",
      "Epoch 463/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6803 - val_loss: 0.5943\n",
      "Epoch 464/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6673 - val_loss: 0.6205\n",
      "Epoch 465/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6870 - val_loss: 0.6364\n",
      "Epoch 466/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6915 - val_loss: 0.6684\n",
      "Epoch 467/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7146 - val_loss: 0.6318\n",
      "Epoch 468/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7008 - val_loss: 0.6192\n",
      "Epoch 469/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6978 - val_loss: 0.7369\n",
      "Epoch 470/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7071 - val_loss: 0.7019\n",
      "Epoch 471/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7040 - val_loss: 0.7249\n",
      "Epoch 472/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6947 - val_loss: 0.6451\n",
      "Epoch 473/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7309 - val_loss: 0.8361\n",
      "Epoch 474/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7023 - val_loss: 0.5895\n",
      "Epoch 475/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6775 - val_loss: 0.6799\n",
      "Epoch 476/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6924 - val_loss: 0.6040\n",
      "Epoch 477/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7021 - val_loss: 0.6612\n",
      "Epoch 478/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6829 - val_loss: 0.5991\n",
      "Epoch 479/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6749 - val_loss: 0.7494\n",
      "Epoch 480/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7004 - val_loss: 0.6305\n",
      "Epoch 481/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6877 - val_loss: 0.6038\n",
      "Epoch 482/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7010 - val_loss: 0.5970\n",
      "Epoch 483/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6671 - val_loss: 0.6147\n",
      "Epoch 484/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6777 - val_loss: 0.6036\n",
      "Epoch 485/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6770 - val_loss: 0.6211\n",
      "Epoch 486/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6895 - val_loss: 0.6693\n",
      "Epoch 487/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7238 - val_loss: 0.7413\n",
      "Epoch 488/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6887 - val_loss: 0.7124\n",
      "Epoch 489/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7239 - val_loss: 0.5962\n",
      "Epoch 490/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6710 - val_loss: 0.6070\n",
      "Epoch 491/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6958 - val_loss: 0.6004\n",
      "Epoch 492/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6775 - val_loss: 0.6643\n",
      "Epoch 493/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6663 - val_loss: 0.5950\n",
      "Epoch 494/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6658 - val_loss: 0.6494\n",
      "Epoch 495/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6838 - val_loss: 0.5978\n",
      "Epoch 496/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6971 - val_loss: 0.8156\n",
      "Epoch 497/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7018 - val_loss: 0.6529\n",
      "Epoch 498/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6711 - val_loss: 0.5947\n",
      "Epoch 499/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6901 - val_loss: 0.6034\n",
      "Epoch 500/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7004 - val_loss: 0.7966\n",
      "Epoch 501/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6949 - val_loss: 0.5948\n",
      "Epoch 502/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6848 - val_loss: 0.5957\n",
      "Epoch 503/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7015 - val_loss: 0.6387\n",
      "Epoch 504/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6809 - val_loss: 0.5881\n",
      "Epoch 505/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6847 - val_loss: 0.6537\n",
      "Epoch 506/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6979 - val_loss: 0.5934\n",
      "Epoch 507/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7031 - val_loss: 0.6284\n",
      "Epoch 508/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6892 - val_loss: 0.6014\n",
      "Epoch 509/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6680 - val_loss: 0.6807\n",
      "Epoch 510/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6973 - val_loss: 0.7947\n",
      "Epoch 511/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6897 - val_loss: 0.6401\n",
      "Epoch 512/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6677 - val_loss: 0.6015\n",
      "Epoch 513/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6852 - val_loss: 0.6281\n",
      "Epoch 514/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6971 - val_loss: 0.6136\n",
      "Epoch 515/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6813 - val_loss: 0.6667\n",
      "Epoch 516/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6999 - val_loss: 0.6185\n",
      "Epoch 517/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6691 - val_loss: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6973 - val_loss: 0.6073\n",
      "Epoch 519/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7037 - val_loss: 0.5993\n",
      "Epoch 520/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7063 - val_loss: 0.6735\n",
      "Epoch 521/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6916 - val_loss: 0.5914\n",
      "Epoch 522/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6736 - val_loss: 0.8558\n",
      "Epoch 523/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7020 - val_loss: 0.6787\n",
      "Epoch 524/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6980 - val_loss: 0.5842\n",
      "Epoch 525/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6979 - val_loss: 0.7304\n",
      "Epoch 526/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6943 - val_loss: 0.5953\n",
      "Epoch 527/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6753 - val_loss: 0.5932\n",
      "Epoch 528/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7203 - val_loss: 0.6093\n",
      "Epoch 529/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6841 - val_loss: 0.5986\n",
      "Epoch 530/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6913 - val_loss: 0.5935\n",
      "Epoch 531/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6804 - val_loss: 0.6486\n",
      "Epoch 532/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6804 - val_loss: 0.5907\n",
      "Epoch 533/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6795 - val_loss: 0.6248\n",
      "Epoch 534/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6998 - val_loss: 0.6135\n",
      "Epoch 535/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7153 - val_loss: 0.5844\n",
      "Epoch 536/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7039 - val_loss: 0.7171\n",
      "Epoch 537/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6845 - val_loss: 0.6028\n",
      "Epoch 538/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6836 - val_loss: 0.6376\n",
      "Epoch 539/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6600 - val_loss: 0.6247\n",
      "Epoch 540/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6803 - val_loss: 0.6174\n",
      "Epoch 541/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6478 - val_loss: 0.5922\n",
      "Epoch 542/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7055 - val_loss: 0.6793\n",
      "Epoch 543/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6959 - val_loss: 0.7768\n",
      "Epoch 544/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6930 - val_loss: 0.5900\n",
      "Epoch 545/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6947 - val_loss: 0.6185\n",
      "Epoch 546/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6815 - val_loss: 0.6056\n",
      "Epoch 547/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6908 - val_loss: 0.8173\n",
      "Epoch 548/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7003 - val_loss: 0.6084\n",
      "Epoch 549/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7079 - val_loss: 0.6099\n",
      "Epoch 550/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7039 - val_loss: 0.6169\n",
      "Epoch 551/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6800 - val_loss: 0.6124\n",
      "Epoch 552/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6741 - val_loss: 0.5877\n",
      "Epoch 553/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.7055 - val_loss: 0.5935\n",
      "Epoch 554/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6897 - val_loss: 0.7352\n",
      "\n",
      "Epoch 00554: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 555/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6477 - val_loss: 0.5848\n",
      "Epoch 556/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6500 - val_loss: 0.6064\n",
      "Epoch 557/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6391 - val_loss: 0.6032\n",
      "Epoch 558/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6474 - val_loss: 0.6348\n",
      "Epoch 559/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6641 - val_loss: 0.6088\n",
      "Epoch 560/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6476 - val_loss: 0.5687\n",
      "Epoch 561/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6554 - val_loss: 0.5715\n",
      "Epoch 562/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6260 - val_loss: 0.5721\n",
      "Epoch 563/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6454 - val_loss: 0.5896\n",
      "Epoch 564/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6504 - val_loss: 0.5794\n",
      "Epoch 565/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6500 - val_loss: 0.7096\n",
      "Epoch 566/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6452 - val_loss: 0.5821\n",
      "Epoch 567/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6485 - val_loss: 0.5739\n",
      "Epoch 568/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6403 - val_loss: 0.5875\n",
      "Epoch 569/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6469 - val_loss: 0.5598\n",
      "Epoch 570/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6314 - val_loss: 0.5665\n",
      "Epoch 571/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6346 - val_loss: 0.5945\n",
      "Epoch 572/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6478 - val_loss: 0.5733\n",
      "Epoch 573/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6377 - val_loss: 0.5914\n",
      "Epoch 574/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6487 - val_loss: 0.6242\n",
      "Epoch 575/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6451 - val_loss: 0.6847\n",
      "Epoch 576/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6387 - val_loss: 0.5652\n",
      "Epoch 577/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6383 - val_loss: 0.5631\n",
      "Epoch 578/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6461 - val_loss: 0.5862\n",
      "Epoch 579/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6325 - val_loss: 0.6049\n",
      "Epoch 580/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6444 - val_loss: 0.5925\n",
      "Epoch 581/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6414 - val_loss: 0.5704\n",
      "Epoch 582/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6356 - val_loss: 0.6408\n",
      "Epoch 583/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6402 - val_loss: 0.6369\n",
      "Epoch 584/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6308 - val_loss: 0.5761\n",
      "Epoch 585/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6376 - val_loss: 0.5652\n",
      "Epoch 586/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6257 - val_loss: 0.5701\n",
      "Epoch 587/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6573 - val_loss: 0.6808\n",
      "Epoch 588/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6455 - val_loss: 0.5686\n",
      "Epoch 589/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6555 - val_loss: 0.5642\n",
      "Epoch 590/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6491 - val_loss: 0.5965\n",
      "Epoch 591/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6374 - val_loss: 0.6126\n",
      "Epoch 592/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6541 - val_loss: 0.5759\n",
      "Epoch 593/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6475 - val_loss: 0.7280\n",
      "Epoch 594/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6700 - val_loss: 0.6313\n",
      "Epoch 595/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6279 - val_loss: 0.6191\n",
      "Epoch 596/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6541 - val_loss: 0.6590\n",
      "Epoch 597/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6550 - val_loss: 0.5839\n",
      "Epoch 598/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6453 - val_loss: 0.5737\n",
      "Epoch 599/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6570 - val_loss: 0.5639\n",
      "\n",
      "Epoch 00599: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 600/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6288 - val_loss: 0.5717\n",
      "Epoch 601/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6325 - val_loss: 0.5565\n",
      "Epoch 602/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6058 - val_loss: 0.5570\n",
      "Epoch 603/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6227 - val_loss: 0.5769\n",
      "Epoch 604/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6121 - val_loss: 0.5548\n",
      "Epoch 605/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6087 - val_loss: 0.5540\n",
      "Epoch 606/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6181 - val_loss: 0.5720\n",
      "Epoch 607/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6077 - val_loss: 0.5847\n",
      "Epoch 608/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6385 - val_loss: 0.5915\n",
      "Epoch 609/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6275 - val_loss: 0.5951\n",
      "Epoch 610/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6092 - val_loss: 0.5936\n",
      "Epoch 611/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6163 - val_loss: 0.6027\n",
      "Epoch 612/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6136 - val_loss: 0.5518\n",
      "Epoch 613/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6042 - val_loss: 0.5540\n",
      "Epoch 614/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6294 - val_loss: 0.5837\n",
      "Epoch 615/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6160 - val_loss: 0.5534\n",
      "Epoch 616/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6267 - val_loss: 0.6215\n",
      "Epoch 617/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6067 - val_loss: 0.6199\n",
      "Epoch 618/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6308 - val_loss: 0.5498\n",
      "Epoch 619/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6070 - val_loss: 0.5749\n",
      "Epoch 620/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6313 - val_loss: 0.5717\n",
      "Epoch 621/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6188 - val_loss: 0.5607\n",
      "Epoch 622/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6221 - val_loss: 0.5602\n",
      "Epoch 623/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6099 - val_loss: 0.5606\n",
      "Epoch 624/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6359 - val_loss: 0.5503\n",
      "Epoch 625/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6246 - val_loss: 0.5501\n",
      "Epoch 626/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6219 - val_loss: 0.5736\n",
      "Epoch 627/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6180 - val_loss: 0.6558\n",
      "Epoch 628/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6314 - val_loss: 0.5547\n",
      "Epoch 629/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6064 - val_loss: 0.5511\n",
      "Epoch 630/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6175 - val_loss: 0.5740\n",
      "Epoch 631/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6337 - val_loss: 0.5664\n",
      "Epoch 632/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6256 - val_loss: 0.5621\n",
      "Epoch 633/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6126 - val_loss: 0.6044\n",
      "Epoch 634/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6185 - val_loss: 0.5494\n",
      "Epoch 635/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6176 - val_loss: 0.5558\n",
      "Epoch 636/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6187 - val_loss: 0.5539\n",
      "Epoch 637/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6278 - val_loss: 0.5752\n",
      "Epoch 638/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6121 - val_loss: 0.5873\n",
      "Epoch 639/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6305 - val_loss: 0.5582\n",
      "Epoch 640/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6173 - val_loss: 0.5574\n",
      "Epoch 641/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6221 - val_loss: 0.5519\n",
      "Epoch 642/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6238 - val_loss: 0.5539\n",
      "Epoch 643/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6087 - val_loss: 0.5478\n",
      "Epoch 644/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5971 - val_loss: 0.5531\n",
      "Epoch 645/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6072 - val_loss: 0.5749\n",
      "Epoch 646/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6147 - val_loss: 0.5621\n",
      "Epoch 647/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6243 - val_loss: 0.6442\n",
      "Epoch 648/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6255 - val_loss: 0.5490\n",
      "Epoch 649/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6120 - val_loss: 0.5505\n",
      "Epoch 650/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6114 - val_loss: 0.5509\n",
      "Epoch 651/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6460 - val_loss: 0.5512\n",
      "Epoch 652/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6255 - val_loss: 0.5613\n",
      "Epoch 653/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6190 - val_loss: 0.5686\n",
      "Epoch 654/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6241 - val_loss: 0.5718\n",
      "Epoch 655/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6053 - val_loss: 0.5474\n",
      "Epoch 656/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6243 - val_loss: 0.5505\n",
      "Epoch 657/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6143 - val_loss: 0.6420\n",
      "Epoch 658/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6113 - val_loss: 0.5573\n",
      "Epoch 659/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6032 - val_loss: 0.5518\n",
      "Epoch 660/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6083 - val_loss: 0.5560\n",
      "Epoch 661/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6515 - val_loss: 0.6085\n",
      "Epoch 662/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5934 - val_loss: 0.5563\n",
      "Epoch 663/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6238 - val_loss: 0.5523\n",
      "Epoch 664/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6054 - val_loss: 0.5788\n",
      "Epoch 665/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6253 - val_loss: 0.6074\n",
      "Epoch 666/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6233 - val_loss: 0.5575\n",
      "Epoch 667/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6348 - val_loss: 0.5520\n",
      "Epoch 668/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6304 - val_loss: 0.5617\n",
      "Epoch 669/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5973 - val_loss: 0.5511\n",
      "Epoch 670/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6136 - val_loss: 0.5517\n",
      "Epoch 671/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6006 - val_loss: 0.5520\n",
      "Epoch 672/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6271 - val_loss: 0.5868\n",
      "Epoch 673/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6068 - val_loss: 0.5582\n",
      "Epoch 674/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6231 - val_loss: 0.5591\n",
      "Epoch 675/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6223 - val_loss: 0.5563\n",
      "Epoch 676/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6223 - val_loss: 0.5523\n",
      "Epoch 677/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6258 - val_loss: 0.5525\n",
      "Epoch 678/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6155 - val_loss: 0.5574\n",
      "Epoch 679/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6076 - val_loss: 0.5562\n",
      "Epoch 680/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6238 - val_loss: 0.5578\n",
      "Epoch 681/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6089 - val_loss: 0.5654\n",
      "Epoch 682/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6142 - val_loss: 0.5673\n",
      "Epoch 683/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6226 - val_loss: 0.5640\n",
      "Epoch 684/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5951 - val_loss: 0.5568\n",
      "Epoch 685/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6255 - val_loss: 0.5839\n",
      "\n",
      "Epoch 00685: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 686/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6088 - val_loss: 0.5741\n",
      "Epoch 687/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6156 - val_loss: 0.5484\n",
      "Epoch 688/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6006 - val_loss: 0.5469\n",
      "Epoch 689/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6130 - val_loss: 0.5447\n",
      "Epoch 690/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5947 - val_loss: 0.5550\n",
      "Epoch 691/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5948 - val_loss: 0.5549\n",
      "Epoch 692/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5989 - val_loss: 0.5594\n",
      "Epoch 693/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5961 - val_loss: 0.5471\n",
      "Epoch 694/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6055 - val_loss: 0.5545\n",
      "Epoch 695/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5869 - val_loss: 0.5501\n",
      "Epoch 696/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6194 - val_loss: 0.5487\n",
      "Epoch 697/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6173 - val_loss: 0.5533\n",
      "Epoch 698/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6095 - val_loss: 0.5460\n",
      "Epoch 699/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6084 - val_loss: 0.5737\n",
      "Epoch 700/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6122 - val_loss: 0.5492\n",
      "Epoch 701/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6002 - val_loss: 0.5485\n",
      "Epoch 702/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6034 - val_loss: 0.5462\n",
      "Epoch 703/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6116 - val_loss: 0.5564\n",
      "Epoch 704/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5919 - val_loss: 0.5509\n",
      "Epoch 705/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6306 - val_loss: 0.5447\n",
      "Epoch 706/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6229 - val_loss: 0.5440\n",
      "Epoch 707/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6192 - val_loss: 0.5491\n",
      "Epoch 708/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5979 - val_loss: 0.5431\n",
      "Epoch 709/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6096 - val_loss: 0.5476\n",
      "Epoch 710/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6066 - val_loss: 0.5661\n",
      "Epoch 711/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6066 - val_loss: 0.5549\n",
      "Epoch 712/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5915 - val_loss: 0.5587\n",
      "Epoch 713/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5939 - val_loss: 0.5486\n",
      "Epoch 714/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6175 - val_loss: 0.5447\n",
      "Epoch 715/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5925 - val_loss: 0.5424\n",
      "Epoch 716/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6203 - val_loss: 0.5558\n",
      "Epoch 717/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5921 - val_loss: 0.5463\n",
      "Epoch 718/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6057 - val_loss: 0.5462\n",
      "Epoch 719/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5864 - val_loss: 0.5589\n",
      "Epoch 720/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5951 - val_loss: 0.5456\n",
      "Epoch 721/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5972 - val_loss: 0.5432\n",
      "Epoch 722/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5969 - val_loss: 0.5467\n",
      "Epoch 723/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5945 - val_loss: 0.5576\n",
      "Epoch 724/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6120 - val_loss: 0.5607\n",
      "Epoch 725/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6138 - val_loss: 0.5475\n",
      "Epoch 726/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6209 - val_loss: 0.5638\n",
      "Epoch 727/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6056 - val_loss: 0.5429\n",
      "Epoch 728/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6085 - val_loss: 0.5526\n",
      "Epoch 729/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6070 - val_loss: 0.5622\n",
      "Epoch 730/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6076 - val_loss: 0.5474\n",
      "Epoch 731/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6192 - val_loss: 0.5466\n",
      "Epoch 732/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6076 - val_loss: 0.5748\n",
      "Epoch 733/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5971 - val_loss: 0.5432\n",
      "Epoch 734/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6134 - val_loss: 0.5529\n",
      "Epoch 735/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6004 - val_loss: 0.5503\n",
      "Epoch 736/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6034 - val_loss: 0.5425\n",
      "Epoch 737/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6107 - val_loss: 0.5501\n",
      "Epoch 738/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6045 - val_loss: 0.5502\n",
      "Epoch 739/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6023 - val_loss: 0.5600\n",
      "Epoch 740/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6088 - val_loss: 0.5569\n",
      "Epoch 741/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5995 - val_loss: 0.5427\n",
      "Epoch 742/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5929 - val_loss: 0.5437\n",
      "Epoch 743/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6118 - val_loss: 0.5421\n",
      "Epoch 744/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6059 - val_loss: 0.5485\n",
      "Epoch 745/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6024 - val_loss: 0.5511\n",
      "Epoch 746/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5938 - val_loss: 0.5492\n",
      "Epoch 747/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6141 - val_loss: 0.5512\n",
      "Epoch 748/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6156 - val_loss: 0.5477\n",
      "Epoch 749/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6051 - val_loss: 0.5482\n",
      "Epoch 750/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6029 - val_loss: 0.5483\n",
      "Epoch 751/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5766 - val_loss: 0.5539\n",
      "Epoch 752/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5988 - val_loss: 0.5445\n",
      "Epoch 753/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6190 - val_loss: 0.5754\n",
      "Epoch 754/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6033 - val_loss: 0.5709\n",
      "Epoch 755/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6156 - val_loss: 0.5490\n",
      "Epoch 756/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5985 - val_loss: 0.5533\n",
      "Epoch 757/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6142 - val_loss: 0.5501\n",
      "Epoch 758/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6104 - val_loss: 0.5490\n",
      "Epoch 759/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5973 - val_loss: 0.5510\n",
      "Epoch 760/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5975 - val_loss: 0.5441\n",
      "Epoch 761/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5760 - val_loss: 0.5444\n",
      "Epoch 762/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5977 - val_loss: 0.5646\n",
      "Epoch 763/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6100 - val_loss: 0.5433\n",
      "Epoch 764/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5979 - val_loss: 0.5465\n",
      "Epoch 765/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6101 - val_loss: 0.5623\n",
      "Epoch 766/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5910 - val_loss: 0.6098\n",
      "Epoch 767/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6088 - val_loss: 0.5471\n",
      "Epoch 768/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5988 - val_loss: 0.5839\n",
      "Epoch 769/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6069 - val_loss: 0.5507\n",
      "Epoch 770/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6004 - val_loss: 0.5443\n",
      "Epoch 771/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6071 - val_loss: 0.5637\n",
      "Epoch 772/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6133 - val_loss: 0.5581\n",
      "Epoch 773/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6064 - val_loss: 0.5503\n",
      "\n",
      "Epoch 00773: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 774/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5893 - val_loss: 0.5431\n",
      "Epoch 775/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5932 - val_loss: 0.5551\n",
      "Epoch 776/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5874 - val_loss: 0.5488\n",
      "Epoch 777/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5943 - val_loss: 0.5436\n",
      "Epoch 778/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6038 - val_loss: 0.5452\n",
      "Epoch 779/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5770 - val_loss: 0.5436\n",
      "Epoch 780/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5952 - val_loss: 0.5402\n",
      "Epoch 781/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5846 - val_loss: 0.5433\n",
      "Epoch 782/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5949 - val_loss: 0.5584\n",
      "Epoch 783/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6026 - val_loss: 0.5619\n",
      "Epoch 784/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5668 - val_loss: 0.5474\n",
      "Epoch 785/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5899 - val_loss: 0.5409\n",
      "Epoch 786/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5980 - val_loss: 0.5476\n",
      "Epoch 787/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5800 - val_loss: 0.5414\n",
      "Epoch 788/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6076 - val_loss: 0.5434\n",
      "Epoch 789/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6015 - val_loss: 0.5436\n",
      "Epoch 790/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5915 - val_loss: 0.5444\n",
      "Epoch 791/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6049 - val_loss: 0.5417\n",
      "Epoch 792/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5841 - val_loss: 0.5420\n",
      "Epoch 793/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6100 - val_loss: 0.5486\n",
      "Epoch 794/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6251 - val_loss: 0.5427\n",
      "Epoch 795/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5980 - val_loss: 0.5468\n",
      "Epoch 796/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5913 - val_loss: 0.5436\n",
      "Epoch 797/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5868 - val_loss: 0.5504\n",
      "Epoch 798/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6039 - val_loss: 0.5443\n",
      "Epoch 799/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5806 - val_loss: 0.5506\n",
      "Epoch 800/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6073 - val_loss: 0.5407\n",
      "Epoch 801/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5932 - val_loss: 0.5507\n",
      "Epoch 802/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5882 - val_loss: 0.5513\n",
      "Epoch 803/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5786 - val_loss: 0.5425\n",
      "Epoch 804/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6241 - val_loss: 0.5412\n",
      "Epoch 805/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5956 - val_loss: 0.5486\n",
      "Epoch 806/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5801 - val_loss: 0.5412\n",
      "Epoch 807/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5897 - val_loss: 0.5469\n",
      "Epoch 808/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6018 - val_loss: 0.5411\n",
      "Epoch 809/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5977 - val_loss: 0.5468\n",
      "Epoch 810/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6083 - val_loss: 0.5415\n",
      "\n",
      "Epoch 00810: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 811/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5807 - val_loss: 0.5403\n",
      "Epoch 812/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6077 - val_loss: 0.5407\n",
      "Epoch 813/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5830 - val_loss: 0.5441\n",
      "Epoch 814/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5820 - val_loss: 0.5445\n",
      "Epoch 815/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5858 - val_loss: 0.5414\n",
      "Epoch 816/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6038 - val_loss: 0.5436\n",
      "Epoch 817/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5957 - val_loss: 0.5431\n",
      "Epoch 818/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5968 - val_loss: 0.5401\n",
      "Epoch 819/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5882 - val_loss: 0.5468\n",
      "Epoch 820/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5891 - val_loss: 0.5406\n",
      "Epoch 821/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6020 - val_loss: 0.5392\n",
      "Epoch 822/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5950 - val_loss: 0.5402\n",
      "Epoch 823/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5936 - val_loss: 0.5399\n",
      "Epoch 824/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5693 - val_loss: 0.5397\n",
      "Epoch 825/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6012 - val_loss: 0.5417\n",
      "Epoch 826/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5871 - val_loss: 0.5447\n",
      "Epoch 827/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5726 - val_loss: 0.5389\n",
      "Epoch 828/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5857 - val_loss: 0.5432\n",
      "Epoch 829/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5917 - val_loss: 0.5436\n",
      "Epoch 830/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5889 - val_loss: 0.5478\n",
      "Epoch 831/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5909 - val_loss: 0.5401\n",
      "Epoch 832/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5832 - val_loss: 0.5396\n",
      "Epoch 833/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6089 - val_loss: 0.5476\n",
      "Epoch 834/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5841 - val_loss: 0.5403\n",
      "Epoch 835/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5825 - val_loss: 0.5406\n",
      "Epoch 836/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5761 - val_loss: 0.5445\n",
      "Epoch 837/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6054 - val_loss: 0.5394\n",
      "Epoch 838/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5741 - val_loss: 0.5474\n",
      "Epoch 839/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6122 - val_loss: 0.5447\n",
      "Epoch 840/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6065 - val_loss: 0.5434\n",
      "Epoch 841/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6001 - val_loss: 0.5423\n",
      "Epoch 842/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5832 - val_loss: 0.5389\n",
      "Epoch 843/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5837 - val_loss: 0.5432\n",
      "Epoch 844/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6184 - val_loss: 0.5444\n",
      "Epoch 845/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6083 - val_loss: 0.5392\n",
      "Epoch 846/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5914 - val_loss: 0.5434\n",
      "Epoch 847/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5883 - val_loss: 0.5407\n",
      "Epoch 848/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5730 - val_loss: 0.5394\n",
      "Epoch 849/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5915 - val_loss: 0.5471\n",
      "Epoch 850/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6017 - val_loss: 0.5490\n",
      "Epoch 851/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5932 - val_loss: 0.5395\n",
      "Epoch 852/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5793 - val_loss: 0.5462\n",
      "Epoch 853/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6086 - val_loss: 0.5405\n",
      "Epoch 854/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6043 - val_loss: 0.5401\n",
      "Epoch 855/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6069 - val_loss: 0.5418\n",
      "Epoch 856/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5851 - val_loss: 0.5416\n",
      "Epoch 857/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5908 - val_loss: 0.5427\n",
      "\n",
      "Epoch 00857: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 858/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5928 - val_loss: 0.5426\n",
      "Epoch 859/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5591 - val_loss: 0.5385\n",
      "Epoch 860/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6081 - val_loss: 0.5434\n",
      "Epoch 861/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5756 - val_loss: 0.5395\n",
      "Epoch 862/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5718 - val_loss: 0.5463\n",
      "Epoch 863/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5959 - val_loss: 0.5444\n",
      "Epoch 864/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6110 - val_loss: 0.5425\n",
      "Epoch 865/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6017 - val_loss: 0.5393\n",
      "Epoch 866/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5744 - val_loss: 0.5397\n",
      "Epoch 867/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6003 - val_loss: 0.5393\n",
      "Epoch 868/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5883 - val_loss: 0.5422\n",
      "Epoch 869/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5750 - val_loss: 0.5387\n",
      "Epoch 870/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5973 - val_loss: 0.5422\n",
      "Epoch 871/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5890 - val_loss: 0.5403\n",
      "Epoch 872/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5891 - val_loss: 0.5433\n",
      "Epoch 873/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6018 - val_loss: 0.5407\n",
      "Epoch 874/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6049 - val_loss: 0.5441\n",
      "Epoch 875/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5874 - val_loss: 0.5402\n",
      "Epoch 876/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6051 - val_loss: 0.5395\n",
      "Epoch 877/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6068 - val_loss: 0.5399\n",
      "Epoch 878/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5798 - val_loss: 0.5383\n",
      "Epoch 879/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5926 - val_loss: 0.5388\n",
      "Epoch 880/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5953 - val_loss: 0.5392\n",
      "Epoch 881/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5938 - val_loss: 0.5419\n",
      "Epoch 882/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5773 - val_loss: 0.5437\n",
      "Epoch 883/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5885 - val_loss: 0.5428\n",
      "Epoch 884/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5862 - val_loss: 0.5391\n",
      "Epoch 885/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5889 - val_loss: 0.5400\n",
      "Epoch 886/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5842 - val_loss: 0.5425\n",
      "Epoch 887/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6011 - val_loss: 0.5427\n",
      "Epoch 888/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5928 - val_loss: 0.5396\n",
      "Epoch 889/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5702 - val_loss: 0.5406\n",
      "Epoch 890/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5942 - val_loss: 0.5397\n",
      "Epoch 891/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5948 - val_loss: 0.5414\n",
      "Epoch 892/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5994 - val_loss: 0.5464\n",
      "Epoch 893/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5898 - val_loss: 0.5476\n",
      "Epoch 894/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5866 - val_loss: 0.5427\n",
      "Epoch 895/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5712 - val_loss: 0.5422\n",
      "Epoch 896/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5890 - val_loss: 0.5418\n",
      "Epoch 897/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5700 - val_loss: 0.5392\n",
      "Epoch 898/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5795 - val_loss: 0.5409\n",
      "Epoch 899/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5920 - val_loss: 0.5391\n",
      "Epoch 900/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6080 - val_loss: 0.5438\n",
      "Epoch 901/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5758 - val_loss: 0.5418\n",
      "Epoch 902/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5953 - val_loss: 0.5440\n",
      "Epoch 903/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5880 - val_loss: 0.5404\n",
      "Epoch 904/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5823 - val_loss: 0.5421\n",
      "Epoch 905/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5949 - val_loss: 0.5393\n",
      "Epoch 906/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5977 - val_loss: 0.5407\n",
      "Epoch 907/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5835 - val_loss: 0.5417\n",
      "Epoch 908/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5937 - val_loss: 0.5448\n",
      "\n",
      "Epoch 00908: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 909/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5886 - val_loss: 0.5388\n",
      "Epoch 910/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5801 - val_loss: 0.5412\n",
      "Epoch 911/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5868 - val_loss: 0.5400\n",
      "Epoch 912/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6148 - val_loss: 0.5480\n",
      "Epoch 913/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6008 - val_loss: 0.5461\n",
      "Epoch 914/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5939 - val_loss: 0.5391\n",
      "Epoch 915/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5860 - val_loss: 0.5414\n",
      "Epoch 916/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5871 - val_loss: 0.5390\n",
      "Epoch 917/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5918 - val_loss: 0.5422\n",
      "Epoch 918/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5958 - val_loss: 0.5442\n",
      "Epoch 919/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5890 - val_loss: 0.5392\n",
      "Epoch 920/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5804 - val_loss: 0.5413\n",
      "Epoch 921/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5906 - val_loss: 0.5398\n",
      "Epoch 922/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5924 - val_loss: 0.5470\n",
      "Epoch 923/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5805 - val_loss: 0.5436\n",
      "Epoch 924/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5791 - val_loss: 0.5387\n",
      "Epoch 925/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5936 - val_loss: 0.5405\n",
      "Epoch 926/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5702 - val_loss: 0.5392\n",
      "Epoch 927/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.5943 - val_loss: 0.5383\n",
      "Epoch 928/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 0.6033 - val_loss: 0.5435\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00928: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHzym7fnz8f+pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.619390266039144\n",
      "Training 2JHH out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 301863 samples, validate on 76173 samples\n",
      "Epoch 1/2000\n",
      "301863/301863 [==============================] - 4s 13us/step - loss: 3.4960 - val_loss: 0.7797\n",
      "Epoch 2/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.5712 - val_loss: 0.5424\n",
      "Epoch 3/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.4829 - val_loss: 0.4470\n",
      "Epoch 4/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.4360 - val_loss: 0.4178\n",
      "Epoch 5/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.4081 - val_loss: 0.3927\n",
      "Epoch 6/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3894 - val_loss: 0.4841\n",
      "Epoch 7/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3673 - val_loss: 0.3776\n",
      "Epoch 8/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3606 - val_loss: 0.3460\n",
      "Epoch 9/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3492 - val_loss: 0.4175\n",
      "Epoch 10/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3372 - val_loss: 0.3161\n",
      "Epoch 11/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3288 - val_loss: 0.2902\n",
      "Epoch 12/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3166 - val_loss: 0.3575\n",
      "Epoch 13/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3134 - val_loss: 0.3999\n",
      "Epoch 14/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.3097 - val_loss: 0.2815\n",
      "Epoch 15/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2996 - val_loss: 0.3365\n",
      "Epoch 16/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2970 - val_loss: 0.3020\n",
      "Epoch 17/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2938 - val_loss: 0.2809\n",
      "Epoch 18/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2957 - val_loss: 0.2842\n",
      "Epoch 19/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2833 - val_loss: 0.3059\n",
      "Epoch 20/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2800 - val_loss: 0.2578\n",
      "Epoch 21/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2771 - val_loss: 0.2714\n",
      "Epoch 22/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2714 - val_loss: 0.2794\n",
      "Epoch 23/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2768 - val_loss: 0.2782\n",
      "Epoch 24/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2728 - val_loss: 0.2682\n",
      "Epoch 25/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2638 - val_loss: 0.2350\n",
      "Epoch 26/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2563 - val_loss: 0.2610\n",
      "Epoch 27/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2573 - val_loss: 0.2966\n",
      "Epoch 28/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2601 - val_loss: 0.3607\n",
      "Epoch 29/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2523 - val_loss: 0.2566\n",
      "Epoch 30/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2483 - val_loss: 0.2720\n",
      "Epoch 31/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2529 - val_loss: 0.2468\n",
      "Epoch 32/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2518 - val_loss: 0.2520\n",
      "Epoch 33/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2498 - val_loss: 0.2553\n",
      "Epoch 34/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2436 - val_loss: 0.2511\n",
      "Epoch 35/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2507 - val_loss: 0.2977\n",
      "Epoch 36/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2401 - val_loss: 0.2529\n",
      "Epoch 37/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2364 - val_loss: 0.2332\n",
      "Epoch 38/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2456 - val_loss: 0.2102\n",
      "Epoch 39/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2426 - val_loss: 0.2907\n",
      "Epoch 40/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2352 - val_loss: 0.2079\n",
      "Epoch 41/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2362 - val_loss: 0.2255\n",
      "Epoch 42/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2357 - val_loss: 0.2485\n",
      "Epoch 43/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2324 - val_loss: 0.2101\n",
      "Epoch 44/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2302 - val_loss: 0.2341\n",
      "Epoch 45/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2309 - val_loss: 0.2144\n",
      "Epoch 46/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2232 - val_loss: 0.2870\n",
      "Epoch 47/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2282 - val_loss: 0.3204\n",
      "Epoch 48/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2284 - val_loss: 0.2197\n",
      "Epoch 49/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2319 - val_loss: 0.2689\n",
      "Epoch 50/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2252 - val_loss: 0.2128\n",
      "Epoch 51/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2263 - val_loss: 0.2157\n",
      "Epoch 52/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2281 - val_loss: 0.2279\n",
      "Epoch 53/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2144 - val_loss: 0.2907\n",
      "Epoch 54/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2166 - val_loss: 0.2549\n",
      "Epoch 55/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2188 - val_loss: 0.2064\n",
      "Epoch 56/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2154 - val_loss: 0.2173\n",
      "Epoch 57/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2206 - val_loss: 0.2227\n",
      "Epoch 58/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2196 - val_loss: 0.2502\n",
      "Epoch 59/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2241 - val_loss: 0.2036\n",
      "Epoch 60/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2126 - val_loss: 0.1935\n",
      "Epoch 61/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2119 - val_loss: 0.2828\n",
      "Epoch 62/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2122 - val_loss: 0.2694\n",
      "Epoch 63/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2155 - val_loss: 0.2227\n",
      "Epoch 64/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2167 - val_loss: 0.1986\n",
      "Epoch 65/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2082 - val_loss: 0.1912\n",
      "Epoch 66/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2095 - val_loss: 0.2255\n",
      "Epoch 67/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2110 - val_loss: 0.2383\n",
      "Epoch 68/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2165 - val_loss: 0.2374\n",
      "Epoch 69/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2185 - val_loss: 0.1817\n",
      "Epoch 70/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2125 - val_loss: 0.1713\n",
      "Epoch 71/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2113 - val_loss: 0.2180\n",
      "Epoch 72/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2106 - val_loss: 0.2249\n",
      "Epoch 73/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2084 - val_loss: 0.2015\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2079 - val_loss: 0.2064\n",
      "Epoch 75/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2070 - val_loss: 0.2589\n",
      "Epoch 76/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2061 - val_loss: 0.2775\n",
      "Epoch 77/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2045 - val_loss: 0.2318\n",
      "Epoch 78/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2098 - val_loss: 0.1805\n",
      "Epoch 79/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2030 - val_loss: 0.2227\n",
      "Epoch 80/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2045 - val_loss: 0.1738\n",
      "Epoch 81/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1996 - val_loss: 0.1886\n",
      "Epoch 82/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2000 - val_loss: 0.1927\n",
      "Epoch 83/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2062 - val_loss: 0.1929\n",
      "Epoch 84/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1962 - val_loss: 0.2521\n",
      "Epoch 85/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2009 - val_loss: 0.2012\n",
      "Epoch 86/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1908 - val_loss: 0.1856\n",
      "Epoch 87/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1962 - val_loss: 0.2315\n",
      "Epoch 88/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2004 - val_loss: 0.2085\n",
      "Epoch 89/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1972 - val_loss: 0.2054\n",
      "Epoch 90/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1923 - val_loss: 0.1738\n",
      "Epoch 91/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2013 - val_loss: 0.2030\n",
      "Epoch 92/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.2021 - val_loss: 0.1855\n",
      "Epoch 93/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1983 - val_loss: 0.2060\n",
      "Epoch 94/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1994 - val_loss: 0.1771\n",
      "Epoch 95/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1918 - val_loss: 0.1968\n",
      "Epoch 96/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1966 - val_loss: 0.1811\n",
      "Epoch 97/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1939 - val_loss: 0.1756\n",
      "Epoch 98/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1907 - val_loss: 0.1986\n",
      "Epoch 99/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1900 - val_loss: 0.1952\n",
      "Epoch 100/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1937 - val_loss: 0.2471\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 101/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1743 - val_loss: 0.1615\n",
      "Epoch 102/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1749 - val_loss: 0.1819\n",
      "Epoch 103/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1689 - val_loss: 0.1886\n",
      "Epoch 104/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1800 - val_loss: 0.2003\n",
      "Epoch 105/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1725 - val_loss: 0.1486\n",
      "Epoch 106/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1713 - val_loss: 0.2368\n",
      "Epoch 107/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1734 - val_loss: 0.1717\n",
      "Epoch 108/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1740 - val_loss: 0.1824\n",
      "Epoch 109/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1735 - val_loss: 0.1629\n",
      "Epoch 110/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1742 - val_loss: 0.1585\n",
      "Epoch 111/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1720 - val_loss: 0.2059\n",
      "Epoch 112/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1812 - val_loss: 0.1610\n",
      "Epoch 113/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1669 - val_loss: 0.2264\n",
      "Epoch 114/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1726 - val_loss: 0.1703\n",
      "Epoch 115/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1680 - val_loss: 0.1601\n",
      "Epoch 116/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1748 - val_loss: 0.1501\n",
      "Epoch 117/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1696 - val_loss: 0.1683\n",
      "Epoch 118/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1724 - val_loss: 0.1732\n",
      "Epoch 119/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1712 - val_loss: 0.1513\n",
      "Epoch 120/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1710 - val_loss: 0.2055\n",
      "Epoch 121/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1717 - val_loss: 0.1685\n",
      "Epoch 122/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1648 - val_loss: 0.1799\n",
      "Epoch 123/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1657 - val_loss: 0.1751\n",
      "Epoch 124/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1701 - val_loss: 0.1836\n",
      "Epoch 125/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1682 - val_loss: 0.1640\n",
      "Epoch 126/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1625 - val_loss: 0.1577\n",
      "Epoch 127/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1668 - val_loss: 0.1713\n",
      "Epoch 128/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1732 - val_loss: 0.1538\n",
      "Epoch 129/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1703 - val_loss: 0.2213\n",
      "Epoch 130/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1688 - val_loss: 0.1638\n",
      "Epoch 131/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1645 - val_loss: 0.1494\n",
      "Epoch 132/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1670 - val_loss: 0.1440\n",
      "Epoch 133/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1684 - val_loss: 0.1511\n",
      "Epoch 134/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1630 - val_loss: 0.1466\n",
      "Epoch 135/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1718 - val_loss: 0.1765\n",
      "Epoch 136/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1677 - val_loss: 0.1583\n",
      "Epoch 137/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1692 - val_loss: 0.1560\n",
      "Epoch 138/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1605 - val_loss: 0.2172\n",
      "Epoch 139/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1689 - val_loss: 0.1602\n",
      "Epoch 140/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1676 - val_loss: 0.1462\n",
      "Epoch 141/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1663 - val_loss: 0.1683\n",
      "Epoch 142/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1663 - val_loss: 0.1990\n",
      "Epoch 143/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1689 - val_loss: 0.1753\n",
      "Epoch 144/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1632 - val_loss: 0.1509\n",
      "Epoch 145/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1701 - val_loss: 0.1518\n",
      "Epoch 146/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1640 - val_loss: 0.1586\n",
      "Epoch 147/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1626 - val_loss: 0.1504\n",
      "Epoch 148/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1733 - val_loss: 0.1912\n",
      "Epoch 149/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1647 - val_loss: 0.1946\n",
      "Epoch 150/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1691 - val_loss: 0.1561\n",
      "Epoch 151/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1651 - val_loss: 0.1712\n",
      "Epoch 152/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1643 - val_loss: 0.2068\n",
      "Epoch 153/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1645 - val_loss: 0.2781\n",
      "Epoch 154/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1600 - val_loss: 0.1580\n",
      "Epoch 155/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1643 - val_loss: 0.1536\n",
      "Epoch 156/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1675 - val_loss: 0.1571\n",
      "Epoch 157/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1613 - val_loss: 0.1566\n",
      "Epoch 158/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1632 - val_loss: 0.1595\n",
      "Epoch 159/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1675 - val_loss: 0.2133\n",
      "Epoch 160/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1587 - val_loss: 0.1517\n",
      "Epoch 161/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1726 - val_loss: 0.1583\n",
      "Epoch 162/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1679 - val_loss: 0.1573\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 163/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1562 - val_loss: 0.1772\n",
      "Epoch 164/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1528 - val_loss: 0.1492\n",
      "Epoch 165/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1526 - val_loss: 0.1656\n",
      "Epoch 166/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1527 - val_loss: 0.1462\n",
      "Epoch 167/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1512 - val_loss: 0.1495\n",
      "Epoch 168/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1513 - val_loss: 0.1569\n",
      "Epoch 169/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1519 - val_loss: 0.1556\n",
      "Epoch 170/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1521 - val_loss: 0.1479\n",
      "Epoch 171/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1567 - val_loss: 0.1657\n",
      "Epoch 172/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1473 - val_loss: 0.1449\n",
      "Epoch 173/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1539 - val_loss: 0.1559\n",
      "Epoch 174/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1496 - val_loss: 0.1640\n",
      "Epoch 175/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1536 - val_loss: 0.1524\n",
      "Epoch 176/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1518 - val_loss: 0.1465\n",
      "Epoch 177/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1487 - val_loss: 0.1694\n",
      "Epoch 178/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1507 - val_loss: 0.1345\n",
      "Epoch 179/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1600 - val_loss: 0.1358\n",
      "Epoch 180/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1546 - val_loss: 0.1427\n",
      "Epoch 181/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1561 - val_loss: 0.1838\n",
      "Epoch 182/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1563 - val_loss: 0.1373\n",
      "Epoch 183/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1523 - val_loss: 0.1451\n",
      "Epoch 184/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1452 - val_loss: 0.1491\n",
      "Epoch 185/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1517 - val_loss: 0.1659\n",
      "Epoch 186/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1479 - val_loss: 0.1342\n",
      "Epoch 187/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1501 - val_loss: 0.1386\n",
      "Epoch 188/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1562 - val_loss: 0.1591\n",
      "Epoch 189/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1490 - val_loss: 0.1530\n",
      "Epoch 190/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1471 - val_loss: 0.1415\n",
      "Epoch 191/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1487 - val_loss: 0.1453\n",
      "Epoch 192/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1508 - val_loss: 0.1317\n",
      "Epoch 193/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1498 - val_loss: 0.1557\n",
      "Epoch 194/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1460 - val_loss: 0.1503\n",
      "Epoch 195/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1512 - val_loss: 0.1545\n",
      "Epoch 196/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1471 - val_loss: 0.1585\n",
      "Epoch 197/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1539 - val_loss: 0.1491\n",
      "Epoch 198/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1542 - val_loss: 0.1532\n",
      "Epoch 199/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1546 - val_loss: 0.1853\n",
      "Epoch 200/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1477 - val_loss: 0.1454\n",
      "Epoch 201/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1513 - val_loss: 0.1529\n",
      "Epoch 202/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1471 - val_loss: 0.1710\n",
      "Epoch 203/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1453 - val_loss: 0.1478\n",
      "Epoch 204/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1507 - val_loss: 0.1524\n",
      "Epoch 205/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1533 - val_loss: 0.1444\n",
      "Epoch 206/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1519 - val_loss: 0.1405\n",
      "Epoch 207/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1505 - val_loss: 0.1353\n",
      "Epoch 208/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1460 - val_loss: 0.1779\n",
      "Epoch 209/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1454 - val_loss: 0.1699\n",
      "Epoch 210/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1521 - val_loss: 0.1540\n",
      "Epoch 211/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1471 - val_loss: 0.1383\n",
      "Epoch 212/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1527 - val_loss: 0.1330\n",
      "Epoch 213/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1457 - val_loss: 0.1459\n",
      "Epoch 214/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1443 - val_loss: 0.1337\n",
      "Epoch 215/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1487 - val_loss: 0.1486\n",
      "Epoch 216/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1499 - val_loss: 0.1487\n",
      "Epoch 217/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1522 - val_loss: 0.1423\n",
      "Epoch 218/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1512 - val_loss: 0.1475\n",
      "Epoch 219/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1497 - val_loss: 0.1861\n",
      "Epoch 220/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1487 - val_loss: 0.1298\n",
      "Epoch 221/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1538 - val_loss: 0.1448\n",
      "Epoch 222/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1432 - val_loss: 0.1490\n",
      "Epoch 223/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1501 - val_loss: 0.1368\n",
      "Epoch 224/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1463 - val_loss: 0.1351\n",
      "Epoch 225/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1483 - val_loss: 0.1492\n",
      "Epoch 226/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1510 - val_loss: 0.1407\n",
      "Epoch 227/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1562 - val_loss: 0.1653\n",
      "Epoch 228/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1566 - val_loss: 0.1818\n",
      "Epoch 229/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1505 - val_loss: 0.1379\n",
      "Epoch 230/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1479 - val_loss: 0.1452\n",
      "Epoch 231/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1494 - val_loss: 0.1394\n",
      "Epoch 232/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1499 - val_loss: 0.1505\n",
      "Epoch 233/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1497 - val_loss: 0.1467\n",
      "Epoch 234/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1430 - val_loss: 0.1517\n",
      "Epoch 235/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1533 - val_loss: 0.1386\n",
      "Epoch 236/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1495 - val_loss: 0.1509\n",
      "Epoch 237/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1484 - val_loss: 0.1384\n",
      "Epoch 238/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1493 - val_loss: 0.1311\n",
      "Epoch 239/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1468 - val_loss: 0.1391\n",
      "Epoch 240/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1483 - val_loss: 0.1500\n",
      "Epoch 241/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1512 - val_loss: 0.1464\n",
      "Epoch 242/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1487 - val_loss: 0.1369\n",
      "Epoch 243/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1456 - val_loss: 0.1530\n",
      "Epoch 244/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1513 - val_loss: 0.1430\n",
      "Epoch 245/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1488 - val_loss: 0.1452\n",
      "Epoch 246/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1447 - val_loss: 0.1558\n",
      "Epoch 247/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1466 - val_loss: 0.1317\n",
      "Epoch 248/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1518 - val_loss: 0.1386\n",
      "Epoch 249/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1450 - val_loss: 0.1399\n",
      "Epoch 250/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1454 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00250: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 251/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1417 - val_loss: 0.1503\n",
      "Epoch 252/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1449 - val_loss: 0.1331\n",
      "Epoch 253/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1383 - val_loss: 0.1457\n",
      "Epoch 254/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1439 - val_loss: 0.1473\n",
      "Epoch 255/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1457 - val_loss: 0.1485\n",
      "Epoch 256/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1361 - val_loss: 0.1448\n",
      "Epoch 257/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1384 - val_loss: 0.1309\n",
      "Epoch 258/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1442 - val_loss: 0.1318\n",
      "Epoch 259/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1422 - val_loss: 0.1423\n",
      "Epoch 260/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1413 - val_loss: 0.1299\n",
      "Epoch 261/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1379 - val_loss: 0.1381\n",
      "Epoch 262/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1408 - val_loss: 0.1325\n",
      "Epoch 263/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1432 - val_loss: 0.1326\n",
      "Epoch 264/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1463 - val_loss: 0.1367\n",
      "Epoch 265/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1446 - val_loss: 0.1412\n",
      "Epoch 266/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1424 - val_loss: 0.1405\n",
      "Epoch 267/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1502 - val_loss: 0.1404\n",
      "Epoch 268/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1359 - val_loss: 0.1308\n",
      "Epoch 269/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1485 - val_loss: 0.1309\n",
      "Epoch 270/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 0.1407 - val_loss: 0.1522\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00270: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0415940668304176\n",
      "Training 1JHN out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 34764 samples, validate on 8599 samples\n",
      "Epoch 1/2000\n",
      "34764/34764 [==============================] - 1s 26us/step - loss: 47.1240 - val_loss: 46.3167\n",
      "Epoch 2/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 45.6202 - val_loss: 44.1651\n",
      "Epoch 3/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 42.8313 - val_loss: 28.4776\n",
      "Epoch 4/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 38.5410 - val_loss: 34.2446\n",
      "Epoch 5/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 32.6799 - val_loss: 23.2136\n",
      "Epoch 6/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 25.2076 - val_loss: 27.4405\n",
      "Epoch 7/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 16.1297 - val_loss: 9.4616\n",
      "Epoch 8/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 5.8986 - val_loss: 13.6828\n",
      "Epoch 9/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 1.4995 - val_loss: 8.9911\n",
      "Epoch 10/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 1.1600 - val_loss: 5.2936\n",
      "Epoch 11/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 1.0310 - val_loss: 3.4793\n",
      "Epoch 12/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.9050 - val_loss: 2.2497\n",
      "Epoch 13/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.8536 - val_loss: 1.8368\n",
      "Epoch 14/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.8390 - val_loss: 1.3374\n",
      "Epoch 15/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.8425 - val_loss: 1.2675\n",
      "Epoch 16/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7838 - val_loss: 1.1522\n",
      "Epoch 17/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7298 - val_loss: 0.8639\n",
      "Epoch 18/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7947 - val_loss: 1.0710\n",
      "Epoch 19/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7474 - val_loss: 0.9656\n",
      "Epoch 20/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7332 - val_loss: 1.2833\n",
      "Epoch 21/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7317 - val_loss: 1.0113\n",
      "Epoch 22/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7242 - val_loss: 0.6753\n",
      "Epoch 23/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6768 - val_loss: 0.7081\n",
      "Epoch 24/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6876 - val_loss: 0.6939\n",
      "Epoch 25/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7007 - val_loss: 0.7028\n",
      "Epoch 26/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.7076 - val_loss: 0.8056\n",
      "Epoch 27/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6572 - val_loss: 0.6755\n",
      "Epoch 28/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6579 - val_loss: 0.7807\n",
      "Epoch 29/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6751 - val_loss: 0.6132\n",
      "Epoch 30/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6189 - val_loss: 0.6136\n",
      "Epoch 31/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6468 - val_loss: 0.5437\n",
      "Epoch 32/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6417 - val_loss: 0.6480\n",
      "Epoch 33/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6144 - val_loss: 0.6056\n",
      "Epoch 34/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5982 - val_loss: 0.7068\n",
      "Epoch 35/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5804 - val_loss: 0.6629\n",
      "Epoch 36/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6205 - val_loss: 0.5988\n",
      "Epoch 37/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6537 - val_loss: 0.5550\n",
      "Epoch 38/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6144 - val_loss: 0.6060\n",
      "Epoch 39/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6245 - val_loss: 0.5140\n",
      "Epoch 40/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5501 - val_loss: 0.6415\n",
      "Epoch 41/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6743 - val_loss: 0.6664\n",
      "Epoch 42/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6157 - val_loss: 0.5494\n",
      "Epoch 43/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6051 - val_loss: 0.6086\n",
      "Epoch 44/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6482 - val_loss: 0.5878\n",
      "Epoch 45/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6064 - val_loss: 0.6986\n",
      "Epoch 46/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5932 - val_loss: 0.5789\n",
      "Epoch 47/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5769 - val_loss: 0.6665\n",
      "Epoch 48/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6100 - val_loss: 0.5224\n",
      "Epoch 49/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5874 - val_loss: 0.5386\n",
      "Epoch 50/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5480 - val_loss: 0.6706\n",
      "Epoch 51/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5993 - val_loss: 0.5534\n",
      "Epoch 52/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5805 - val_loss: 0.4589\n",
      "Epoch 53/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5874 - val_loss: 0.6451\n",
      "Epoch 54/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5571 - val_loss: 0.4668\n",
      "Epoch 55/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5974 - val_loss: 0.5221\n",
      "Epoch 56/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.6032 - val_loss: 0.5569\n",
      "Epoch 57/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5656 - val_loss: 0.5038\n",
      "Epoch 58/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5468 - val_loss: 0.5668\n",
      "Epoch 59/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5275 - val_loss: 0.7194\n",
      "Epoch 60/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5288 - val_loss: 0.5792\n",
      "Epoch 61/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5555 - val_loss: 0.4598\n",
      "Epoch 62/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5495 - val_loss: 0.5854\n",
      "Epoch 63/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5567 - val_loss: 0.4957\n",
      "Epoch 64/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5541 - val_loss: 0.4936\n",
      "Epoch 65/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5916 - val_loss: 0.4636\n",
      "Epoch 66/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5498 - val_loss: 0.4771\n",
      "Epoch 67/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5363 - val_loss: 0.6416\n",
      "Epoch 68/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5759 - val_loss: 0.4897\n",
      "Epoch 69/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5300 - val_loss: 0.8415\n",
      "Epoch 70/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5939 - val_loss: 0.4728\n",
      "Epoch 71/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5476 - val_loss: 0.5002\n",
      "Epoch 72/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5504 - val_loss: 0.5002\n",
      "Epoch 73/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5210 - val_loss: 0.4960\n",
      "Epoch 74/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5312 - val_loss: 0.5424\n",
      "Epoch 75/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5123 - val_loss: 0.4754\n",
      "Epoch 76/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5173 - val_loss: 0.4837\n",
      "Epoch 77/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4996 - val_loss: 0.6951\n",
      "Epoch 78/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5278 - val_loss: 0.4782\n",
      "Epoch 79/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4762 - val_loss: 0.4852\n",
      "Epoch 80/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5515 - val_loss: 0.4855\n",
      "Epoch 81/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5299 - val_loss: 0.4708\n",
      "Epoch 82/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5612 - val_loss: 0.5096\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 83/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4940 - val_loss: 0.4750\n",
      "Epoch 84/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4711 - val_loss: 0.4864\n",
      "Epoch 85/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4295 - val_loss: 0.4155\n",
      "Epoch 86/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4616 - val_loss: 0.4473\n",
      "Epoch 87/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4823 - val_loss: 0.4057\n",
      "Epoch 88/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4427 - val_loss: 0.4130\n",
      "Epoch 89/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4485 - val_loss: 0.4908\n",
      "Epoch 90/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4558 - val_loss: 0.4099\n",
      "Epoch 91/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4837 - val_loss: 0.4062\n",
      "Epoch 92/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4253 - val_loss: 0.4088\n",
      "Epoch 93/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4629 - val_loss: 0.4695\n",
      "Epoch 94/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4633 - val_loss: 0.4109\n",
      "Epoch 95/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4867 - val_loss: 0.4206\n",
      "Epoch 96/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4495 - val_loss: 0.5370\n",
      "Epoch 97/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4761 - val_loss: 0.4320\n",
      "Epoch 98/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5040 - val_loss: 0.4291\n",
      "Epoch 99/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4695 - val_loss: 0.4278\n",
      "Epoch 100/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4709 - val_loss: 0.4220\n",
      "Epoch 101/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4399 - val_loss: 0.4307\n",
      "Epoch 102/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4691 - val_loss: 0.4338\n",
      "Epoch 103/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4888 - val_loss: 0.4646\n",
      "Epoch 104/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4272 - val_loss: 0.3990\n",
      "Epoch 105/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4409 - val_loss: 0.4118\n",
      "Epoch 106/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4287 - val_loss: 0.4162\n",
      "Epoch 107/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4394 - val_loss: 0.3791\n",
      "Epoch 108/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4337 - val_loss: 0.4103\n",
      "Epoch 109/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4790 - val_loss: 0.3837\n",
      "Epoch 110/2000\n",
      "34764/34764 [==============================] - ETA: 0s - loss: 0.421 - 0s 11us/step - loss: 0.4185 - val_loss: 0.3997\n",
      "Epoch 111/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4756 - val_loss: 0.4102\n",
      "Epoch 112/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3681 - val_loss: 0.4311\n",
      "Epoch 113/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4473 - val_loss: 0.3767\n",
      "Epoch 114/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4511 - val_loss: 0.4195\n",
      "Epoch 115/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.5225 - val_loss: 0.4290\n",
      "Epoch 116/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4649 - val_loss: 0.4078\n",
      "Epoch 117/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4391 - val_loss: 0.3889\n",
      "Epoch 118/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4720 - val_loss: 0.4118\n",
      "Epoch 119/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4274 - val_loss: 0.3999\n",
      "Epoch 120/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4358 - val_loss: 0.4444\n",
      "Epoch 121/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4423 - val_loss: 0.4149\n",
      "Epoch 122/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4359 - val_loss: 0.4043\n",
      "Epoch 123/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4895 - val_loss: 0.4258\n",
      "Epoch 124/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4658 - val_loss: 0.4099\n",
      "Epoch 125/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4436 - val_loss: 0.3876\n",
      "Epoch 126/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4218 - val_loss: 0.4078\n",
      "Epoch 127/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4167 - val_loss: 0.4207\n",
      "Epoch 128/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4232 - val_loss: 0.4519\n",
      "Epoch 129/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4463 - val_loss: 0.4474\n",
      "Epoch 130/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4294 - val_loss: 0.3914\n",
      "Epoch 131/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4081 - val_loss: 0.4183\n",
      "Epoch 132/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4377 - val_loss: 0.4400\n",
      "Epoch 133/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4134 - val_loss: 0.4845\n",
      "Epoch 134/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4648 - val_loss: 0.3918\n",
      "Epoch 135/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4558 - val_loss: 0.3908\n",
      "Epoch 136/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4555 - val_loss: 0.3828\n",
      "Epoch 137/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3983 - val_loss: 0.3806\n",
      "Epoch 138/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3744 - val_loss: 0.4412\n",
      "Epoch 139/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4505 - val_loss: 0.4343\n",
      "Epoch 140/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4717 - val_loss: 0.4490\n",
      "Epoch 141/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4665 - val_loss: 0.3905\n",
      "Epoch 142/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4272 - val_loss: 0.4011\n",
      "Epoch 143/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4536 - val_loss: 0.5270\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 144/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4194 - val_loss: 0.3663\n",
      "Epoch 145/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4053 - val_loss: 0.3632\n",
      "Epoch 146/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4191 - val_loss: 0.3935\n",
      "Epoch 147/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3930 - val_loss: 0.3623\n",
      "Epoch 148/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4262 - val_loss: 0.3543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4214 - val_loss: 0.3543\n",
      "Epoch 150/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3963 - val_loss: 0.3644\n",
      "Epoch 151/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3874 - val_loss: 0.3656\n",
      "Epoch 152/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4368 - val_loss: 0.3727\n",
      "Epoch 153/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4226 - val_loss: 0.3880\n",
      "Epoch 154/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4586 - val_loss: 0.3687\n",
      "Epoch 155/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4267 - val_loss: 0.3596\n",
      "Epoch 156/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4567 - val_loss: 0.3575\n",
      "Epoch 157/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3996 - val_loss: 0.3828\n",
      "Epoch 158/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3985 - val_loss: 0.3664\n",
      "Epoch 159/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3752 - val_loss: 0.3626\n",
      "Epoch 160/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3622 - val_loss: 0.3644\n",
      "Epoch 161/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3476 - val_loss: 0.3586\n",
      "Epoch 162/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3925 - val_loss: 0.3692\n",
      "Epoch 163/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3713 - val_loss: 0.3776\n",
      "Epoch 164/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3592 - val_loss: 0.3690\n",
      "Epoch 165/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3791 - val_loss: 0.3662\n",
      "Epoch 166/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3786 - val_loss: 0.3519\n",
      "Epoch 167/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4145 - val_loss: 0.3733\n",
      "Epoch 168/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3541 - val_loss: 0.3599\n",
      "Epoch 169/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3537 - val_loss: 0.3397\n",
      "Epoch 170/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3908 - val_loss: 0.3576\n",
      "Epoch 171/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3596 - val_loss: 0.3519\n",
      "Epoch 172/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3696 - val_loss: 0.4188\n",
      "Epoch 173/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3964 - val_loss: 0.3611\n",
      "Epoch 174/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4188 - val_loss: 0.3563\n",
      "Epoch 175/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4189 - val_loss: 0.3649\n",
      "Epoch 176/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3619 - val_loss: 0.3505\n",
      "Epoch 177/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3612 - val_loss: 0.3823\n",
      "Epoch 178/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3707 - val_loss: 0.4003\n",
      "Epoch 179/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3761 - val_loss: 0.3450\n",
      "Epoch 180/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3988 - val_loss: 0.3760\n",
      "Epoch 181/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3573 - val_loss: 0.3558\n",
      "Epoch 182/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3889 - val_loss: 0.3803\n",
      "Epoch 183/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3817 - val_loss: 0.3688\n",
      "Epoch 184/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3750 - val_loss: 0.3487\n",
      "Epoch 185/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3571 - val_loss: 0.3528\n",
      "Epoch 186/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4206 - val_loss: 0.3573\n",
      "Epoch 187/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3847 - val_loss: 0.3475\n",
      "Epoch 188/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3725 - val_loss: 0.3755\n",
      "Epoch 189/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4013 - val_loss: 0.3714\n",
      "Epoch 190/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3822 - val_loss: 0.3550\n",
      "Epoch 191/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3504 - val_loss: 0.3511\n",
      "Epoch 192/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3911 - val_loss: 0.3872\n",
      "Epoch 193/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4306 - val_loss: 0.3675\n",
      "Epoch 194/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3827 - val_loss: 0.3663\n",
      "Epoch 195/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3608 - val_loss: 0.3482\n",
      "Epoch 196/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3826 - val_loss: 0.3781\n",
      "Epoch 197/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3967 - val_loss: 0.3536\n",
      "Epoch 198/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3744 - val_loss: 0.3821\n",
      "Epoch 199/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4578 - val_loss: 0.3454\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 200/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3235 - val_loss: 0.3442\n",
      "Epoch 201/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3931 - val_loss: 0.3433\n",
      "Epoch 202/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3699 - val_loss: 0.3400\n",
      "Epoch 203/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3608 - val_loss: 0.3607\n",
      "Epoch 204/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3770 - val_loss: 0.3478\n",
      "Epoch 205/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3754 - val_loss: 0.3512\n",
      "Epoch 206/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3871 - val_loss: 0.3626\n",
      "Epoch 207/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3708 - val_loss: 0.3447\n",
      "Epoch 208/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3904 - val_loss: 0.3414\n",
      "Epoch 209/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4038 - val_loss: 0.3436\n",
      "Epoch 210/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3115 - val_loss: 0.3438\n",
      "Epoch 211/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3728 - val_loss: 0.3328\n",
      "Epoch 212/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3919 - val_loss: 0.3400\n",
      "Epoch 213/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3823 - val_loss: 0.3320\n",
      "Epoch 214/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4392 - val_loss: 0.3416\n",
      "Epoch 215/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3613 - val_loss: 0.3514\n",
      "Epoch 216/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3650 - val_loss: 0.3444\n",
      "Epoch 217/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3842 - val_loss: 0.3439\n",
      "Epoch 218/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3602 - val_loss: 0.3363\n",
      "Epoch 219/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3611 - val_loss: 0.3307\n",
      "Epoch 220/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3490 - val_loss: 0.3319\n",
      "Epoch 221/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3956 - val_loss: 0.3451\n",
      "Epoch 222/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3427 - val_loss: 0.3378\n",
      "Epoch 223/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3283 - val_loss: 0.3405\n",
      "Epoch 224/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3492 - val_loss: 0.3424\n",
      "Epoch 225/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4077 - val_loss: 0.3321\n",
      "Epoch 226/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3657 - val_loss: 0.3358\n",
      "Epoch 227/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3553 - val_loss: 0.3353\n",
      "Epoch 228/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3258 - val_loss: 0.3356\n",
      "Epoch 229/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3909 - val_loss: 0.3676\n",
      "Epoch 230/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3734 - val_loss: 0.3515\n",
      "Epoch 231/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3636 - val_loss: 0.3394\n",
      "Epoch 232/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3717 - val_loss: 0.3530\n",
      "Epoch 233/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3282 - val_loss: 0.3367\n",
      "Epoch 234/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3723 - val_loss: 0.3485\n",
      "Epoch 235/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3651 - val_loss: 0.3343\n",
      "Epoch 236/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3640 - val_loss: 0.3599\n",
      "Epoch 237/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3698 - val_loss: 0.3346\n",
      "Epoch 238/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3422 - val_loss: 0.3350\n",
      "Epoch 239/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3520 - val_loss: 0.3334\n",
      "Epoch 240/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3368 - val_loss: 0.3343\n",
      "Epoch 241/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3597 - val_loss: 0.3331\n",
      "Epoch 242/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3592 - val_loss: 0.3344\n",
      "Epoch 243/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3832 - val_loss: 0.3444\n",
      "Epoch 244/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3328 - val_loss: 0.3492\n",
      "Epoch 245/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3419 - val_loss: 0.3715\n",
      "Epoch 246/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3456 - val_loss: 0.3452\n",
      "Epoch 247/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3818 - val_loss: 0.3464\n",
      "Epoch 248/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3135 - val_loss: 0.3585\n",
      "Epoch 249/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3604 - val_loss: 0.3395\n",
      "\n",
      "Epoch 00249: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 250/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3917 - val_loss: 0.3265\n",
      "Epoch 251/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3135 - val_loss: 0.3282\n",
      "Epoch 252/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3592 - val_loss: 0.3310\n",
      "Epoch 253/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3845 - val_loss: 0.3277\n",
      "Epoch 254/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3552 - val_loss: 0.3268\n",
      "Epoch 255/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3708 - val_loss: 0.3296\n",
      "Epoch 256/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3459 - val_loss: 0.3265\n",
      "Epoch 257/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3434 - val_loss: 0.3275\n",
      "Epoch 258/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3477 - val_loss: 0.3310\n",
      "Epoch 259/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3851 - val_loss: 0.3355\n",
      "Epoch 260/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3591 - val_loss: 0.3381\n",
      "Epoch 261/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3496 - val_loss: 0.3283\n",
      "Epoch 262/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2834 - val_loss: 0.3269\n",
      "Epoch 263/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3278 - val_loss: 0.3277\n",
      "Epoch 264/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3648 - val_loss: 0.3295\n",
      "Epoch 265/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3685 - val_loss: 0.3290\n",
      "Epoch 266/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3365 - val_loss: 0.3278\n",
      "Epoch 267/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3359 - val_loss: 0.3275\n",
      "Epoch 268/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3369 - val_loss: 0.3358\n",
      "Epoch 269/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3302 - val_loss: 0.3305\n",
      "Epoch 270/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3237 - val_loss: 0.3385\n",
      "Epoch 271/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3353 - val_loss: 0.3375\n",
      "Epoch 272/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3910 - val_loss: 0.3353\n",
      "Epoch 273/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3539 - val_loss: 0.3259\n",
      "Epoch 274/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3695 - val_loss: 0.3365\n",
      "Epoch 275/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2998 - val_loss: 0.3339\n",
      "Epoch 276/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3067 - val_loss: 0.3306\n",
      "Epoch 277/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3277 - val_loss: 0.3275\n",
      "Epoch 278/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3073 - val_loss: 0.3396\n",
      "Epoch 279/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3751 - val_loss: 0.3357\n",
      "Epoch 280/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3549 - val_loss: 0.3281\n",
      "Epoch 281/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3781 - val_loss: 0.3304\n",
      "Epoch 282/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3392 - val_loss: 0.3288\n",
      "Epoch 283/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3788 - val_loss: 0.3261\n",
      "Epoch 284/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3069 - val_loss: 0.3258\n",
      "Epoch 285/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3403 - val_loss: 0.3324\n",
      "Epoch 286/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3404 - val_loss: 0.3413\n",
      "Epoch 287/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3944 - val_loss: 0.3262\n",
      "Epoch 288/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3905 - val_loss: 0.3272\n",
      "Epoch 289/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3856 - val_loss: 0.3279\n",
      "Epoch 290/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3523 - val_loss: 0.3266\n",
      "Epoch 291/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3205 - val_loss: 0.3283\n",
      "Epoch 292/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3449 - val_loss: 0.3319\n",
      "Epoch 293/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3610 - val_loss: 0.3316\n",
      "Epoch 294/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3736 - val_loss: 0.3386\n",
      "Epoch 295/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3883 - val_loss: 0.3312\n",
      "Epoch 296/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3761 - val_loss: 0.3331\n",
      "Epoch 297/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3540 - val_loss: 0.3274\n",
      "Epoch 298/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3375 - val_loss: 0.3307\n",
      "Epoch 299/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3304 - val_loss: 0.3366\n",
      "Epoch 300/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2995 - val_loss: 0.3317\n",
      "Epoch 301/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3367 - val_loss: 0.3273\n",
      "Epoch 302/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3508 - val_loss: 0.3306\n",
      "Epoch 303/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4051 - val_loss: 0.3268\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 304/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3570 - val_loss: 0.3254\n",
      "Epoch 305/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3716 - val_loss: 0.3279\n",
      "Epoch 306/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3893 - val_loss: 0.3229\n",
      "Epoch 307/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3318 - val_loss: 0.3264\n",
      "Epoch 308/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4030 - val_loss: 0.3267\n",
      "Epoch 309/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3072 - val_loss: 0.3222\n",
      "Epoch 310/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3282 - val_loss: 0.3227\n",
      "Epoch 311/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3552 - val_loss: 0.3234\n",
      "Epoch 312/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3066 - val_loss: 0.3258\n",
      "Epoch 313/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3350 - val_loss: 0.3290\n",
      "Epoch 314/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3233 - val_loss: 0.3286\n",
      "Epoch 315/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3385 - val_loss: 0.3283\n",
      "Epoch 316/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4068 - val_loss: 0.3223\n",
      "Epoch 317/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3444 - val_loss: 0.3292\n",
      "Epoch 318/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3559 - val_loss: 0.3251\n",
      "Epoch 319/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3851 - val_loss: 0.3248\n",
      "Epoch 320/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3181 - val_loss: 0.3251\n",
      "Epoch 321/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3591 - val_loss: 0.3259\n",
      "Epoch 322/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3166 - val_loss: 0.3208\n",
      "Epoch 323/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3392 - val_loss: 0.3238\n",
      "Epoch 324/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3411 - val_loss: 0.3238\n",
      "Epoch 325/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3525 - val_loss: 0.3239\n",
      "Epoch 326/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3239 - val_loss: 0.3247\n",
      "Epoch 327/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3030 - val_loss: 0.3287\n",
      "Epoch 328/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2982 - val_loss: 0.3231\n",
      "Epoch 329/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3401 - val_loss: 0.3225\n",
      "Epoch 330/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3615 - val_loss: 0.3219\n",
      "Epoch 331/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3047 - val_loss: 0.3220\n",
      "Epoch 332/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3239 - val_loss: 0.3218\n",
      "Epoch 333/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3616 - val_loss: 0.3306\n",
      "Epoch 334/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3382 - val_loss: 0.3217\n",
      "Epoch 335/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3123 - val_loss: 0.3215\n",
      "Epoch 336/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3140 - val_loss: 0.3239\n",
      "Epoch 337/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3286 - val_loss: 0.3232\n",
      "Epoch 338/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3310 - val_loss: 0.3241\n",
      "Epoch 339/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2950 - val_loss: 0.3205\n",
      "Epoch 340/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3530 - val_loss: 0.3240\n",
      "Epoch 341/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.4135 - val_loss: 0.3211\n",
      "Epoch 342/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3441 - val_loss: 0.3267\n",
      "Epoch 343/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3324 - val_loss: 0.3239\n",
      "Epoch 344/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2752 - val_loss: 0.3351\n",
      "Epoch 345/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3980 - val_loss: 0.3272\n",
      "Epoch 346/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3482 - val_loss: 0.3284\n",
      "Epoch 347/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3702 - val_loss: 0.3220\n",
      "Epoch 348/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3452 - val_loss: 0.3240\n",
      "Epoch 349/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3503 - val_loss: 0.3236\n",
      "Epoch 350/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3023 - val_loss: 0.3220\n",
      "Epoch 351/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2857 - val_loss: 0.3229\n",
      "Epoch 352/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3309 - val_loss: 0.3409\n",
      "Epoch 353/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3198 - val_loss: 0.3273\n",
      "Epoch 354/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3629 - val_loss: 0.3286\n",
      "Epoch 355/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3639 - val_loss: 0.3225\n",
      "Epoch 356/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3564 - val_loss: 0.3250\n",
      "Epoch 357/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3412 - val_loss: 0.3242\n",
      "Epoch 358/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3548 - val_loss: 0.3242\n",
      "Epoch 359/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3585 - val_loss: 0.3222\n",
      "Epoch 360/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3310 - val_loss: 0.3305\n",
      "Epoch 361/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3585 - val_loss: 0.3229\n",
      "Epoch 362/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3250 - val_loss: 0.3282\n",
      "Epoch 363/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3319 - val_loss: 0.3247\n",
      "Epoch 364/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3867 - val_loss: 0.3234\n",
      "Epoch 365/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3179 - val_loss: 0.3236\n",
      "Epoch 366/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2802 - val_loss: 0.3241\n",
      "Epoch 367/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3375 - val_loss: 0.3235\n",
      "Epoch 368/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3614 - val_loss: 0.3220\n",
      "Epoch 369/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3320 - val_loss: 0.3209\n",
      "\n",
      "Epoch 00369: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 370/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3468 - val_loss: 0.3216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3642 - val_loss: 0.3230\n",
      "Epoch 372/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3603 - val_loss: 0.3206\n",
      "Epoch 373/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3400 - val_loss: 0.3208\n",
      "Epoch 374/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3596 - val_loss: 0.3205\n",
      "Epoch 375/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3638 - val_loss: 0.3206\n",
      "Epoch 376/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3298 - val_loss: 0.3209\n",
      "Epoch 377/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3189 - val_loss: 0.3209\n",
      "Epoch 378/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3438 - val_loss: 0.3196\n",
      "Epoch 379/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3715 - val_loss: 0.3219\n",
      "Epoch 380/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3717 - val_loss: 0.3207\n",
      "Epoch 381/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3506 - val_loss: 0.3214\n",
      "Epoch 382/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3046 - val_loss: 0.3215\n",
      "Epoch 383/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3859 - val_loss: 0.3206\n",
      "Epoch 384/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3975 - val_loss: 0.3202\n",
      "Epoch 385/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3328 - val_loss: 0.3225\n",
      "Epoch 386/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3492 - val_loss: 0.3206\n",
      "Epoch 387/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3399 - val_loss: 0.3206\n",
      "Epoch 388/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3312 - val_loss: 0.3204\n",
      "Epoch 389/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3447 - val_loss: 0.3194\n",
      "Epoch 390/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3717 - val_loss: 0.3207\n",
      "Epoch 391/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3136 - val_loss: 0.3202\n",
      "Epoch 392/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2931 - val_loss: 0.3200\n",
      "Epoch 393/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3536 - val_loss: 0.3233\n",
      "Epoch 394/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3644 - val_loss: 0.3191\n",
      "Epoch 395/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3581 - val_loss: 0.3233\n",
      "Epoch 396/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3541 - val_loss: 0.3233\n",
      "Epoch 397/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3244 - val_loss: 0.3247\n",
      "Epoch 398/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3425 - val_loss: 0.3207\n",
      "Epoch 399/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3018 - val_loss: 0.3200\n",
      "Epoch 400/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3240 - val_loss: 0.3221\n",
      "Epoch 401/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3622 - val_loss: 0.3194\n",
      "Epoch 402/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3500 - val_loss: 0.3194\n",
      "Epoch 403/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3354 - val_loss: 0.3206\n",
      "Epoch 404/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3347 - val_loss: 0.3222\n",
      "Epoch 405/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3431 - val_loss: 0.3194\n",
      "Epoch 406/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3418 - val_loss: 0.3228\n",
      "Epoch 407/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3522 - val_loss: 0.3206\n",
      "Epoch 408/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3403 - val_loss: 0.3218\n",
      "Epoch 409/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2964 - val_loss: 0.3208\n",
      "Epoch 410/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3079 - val_loss: 0.3220\n",
      "Epoch 411/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3665 - val_loss: 0.3219\n",
      "Epoch 412/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2922 - val_loss: 0.3209\n",
      "Epoch 413/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3485 - val_loss: 0.3209\n",
      "Epoch 414/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3045 - val_loss: 0.3206\n",
      "Epoch 415/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3875 - val_loss: 0.3205\n",
      "Epoch 416/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3567 - val_loss: 0.3203\n",
      "Epoch 417/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2902 - val_loss: 0.3199\n",
      "Epoch 418/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2879 - val_loss: 0.3249\n",
      "Epoch 419/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3543 - val_loss: 0.3202\n",
      "Epoch 420/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3519 - val_loss: 0.3233\n",
      "Epoch 421/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3315 - val_loss: 0.3210\n",
      "Epoch 422/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3170 - val_loss: 0.3214\n",
      "Epoch 423/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2823 - val_loss: 0.3196\n",
      "Epoch 424/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3518 - val_loss: 0.3212\n",
      "\n",
      "Epoch 00424: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 425/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3206 - val_loss: 0.3202\n",
      "Epoch 426/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3304 - val_loss: 0.3203\n",
      "Epoch 427/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3066 - val_loss: 0.3214\n",
      "Epoch 428/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3457 - val_loss: 0.3199\n",
      "Epoch 429/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3231 - val_loss: 0.3226\n",
      "Epoch 430/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3114 - val_loss: 0.3207\n",
      "Epoch 431/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3295 - val_loss: 0.3201\n",
      "Epoch 432/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3208 - val_loss: 0.3205\n",
      "Epoch 433/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3522 - val_loss: 0.3200\n",
      "Epoch 434/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3656 - val_loss: 0.3192\n",
      "Epoch 435/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.2987 - val_loss: 0.3203\n",
      "Epoch 436/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3366 - val_loss: 0.3196\n",
      "Epoch 437/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3434 - val_loss: 0.3200\n",
      "Epoch 438/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3443 - val_loss: 0.3198\n",
      "Epoch 439/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3470 - val_loss: 0.3196\n",
      "Epoch 440/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3359 - val_loss: 0.3201\n",
      "Epoch 441/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3403 - val_loss: 0.3195\n",
      "Epoch 442/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3588 - val_loss: 0.3198\n",
      "Epoch 443/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3404 - val_loss: 0.3198\n",
      "Epoch 444/2000\n",
      "34764/34764 [==============================] - 0s 11us/step - loss: 0.3274 - val_loss: 0.3194\n",
      "Restoring model weights from the end of the best epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00444: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuvunXrEQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1423994693074864\n",
      "Training 2JHN out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 95787 samples, validate on 23466 samples\n",
      "Epoch 1/2000\n",
      "95787/95787 [==============================] - 2s 17us/step - loss: 1.8666 - val_loss: 1.1945\n",
      "Epoch 2/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.4893 - val_loss: 0.5993\n",
      "Epoch 3/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.3794 - val_loss: 0.3640\n",
      "Epoch 4/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.3439 - val_loss: 0.3827\n",
      "Epoch 5/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.3211 - val_loss: 0.3137\n",
      "Epoch 6/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.3059 - val_loss: 0.3271\n",
      "Epoch 7/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2841 - val_loss: 0.2566\n",
      "Epoch 8/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2818 - val_loss: 0.2710\n",
      "Epoch 9/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2609 - val_loss: 0.2401\n",
      "Epoch 10/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2636 - val_loss: 0.2937\n",
      "Epoch 11/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2525 - val_loss: 0.2461\n",
      "Epoch 12/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2470 - val_loss: 0.2217\n",
      "Epoch 13/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2433 - val_loss: 0.2466\n",
      "Epoch 14/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2434 - val_loss: 0.2315\n",
      "Epoch 15/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2244 - val_loss: 0.2107\n",
      "Epoch 16/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2300 - val_loss: 0.2164\n",
      "Epoch 17/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2277 - val_loss: 0.2306\n",
      "Epoch 18/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2245 - val_loss: 0.2170\n",
      "Epoch 19/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2284 - val_loss: 0.1992\n",
      "Epoch 20/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2248 - val_loss: 0.2306\n",
      "Epoch 21/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2330 - val_loss: 0.2250\n",
      "Epoch 22/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2196 - val_loss: 0.1970\n",
      "Epoch 23/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2161 - val_loss: 0.2026\n",
      "Epoch 24/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2164 - val_loss: 0.2084\n",
      "Epoch 25/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2119 - val_loss: 0.2277\n",
      "Epoch 26/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2150 - val_loss: 0.2549\n",
      "Epoch 27/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2166 - val_loss: 0.2048\n",
      "Epoch 28/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2049 - val_loss: 0.2025\n",
      "Epoch 29/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2045 - val_loss: 0.2008\n",
      "Epoch 30/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2029 - val_loss: 0.1946\n",
      "Epoch 31/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2051 - val_loss: 0.1787\n",
      "Epoch 32/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2019 - val_loss: 0.1943\n",
      "Epoch 33/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2065 - val_loss: 0.1996\n",
      "Epoch 34/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.2014 - val_loss: 0.1717\n",
      "Epoch 35/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1935 - val_loss: 0.1850\n",
      "Epoch 36/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1971 - val_loss: 0.1963\n",
      "Epoch 37/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1960 - val_loss: 0.2050\n",
      "Epoch 38/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1915 - val_loss: 0.1798\n",
      "Epoch 39/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1929 - val_loss: 0.1768\n",
      "Epoch 40/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1891 - val_loss: 0.1821\n",
      "Epoch 41/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1940 - val_loss: 0.1922\n",
      "Epoch 42/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1895 - val_loss: 0.1678\n",
      "Epoch 43/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1906 - val_loss: 0.1656\n",
      "Epoch 44/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1895 - val_loss: 0.1795\n",
      "Epoch 45/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1869 - val_loss: 0.1827\n",
      "Epoch 46/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1926 - val_loss: 0.1903\n",
      "Epoch 47/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1752 - val_loss: 0.2003\n",
      "Epoch 48/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1893 - val_loss: 0.1830\n",
      "Epoch 49/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1781 - val_loss: 0.1784\n",
      "Epoch 50/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1805 - val_loss: 0.1832\n",
      "Epoch 51/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1865 - val_loss: 0.1888\n",
      "Epoch 52/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1732 - val_loss: 0.1789\n",
      "Epoch 53/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1871 - val_loss: 0.1761\n",
      "Epoch 54/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1825 - val_loss: 0.1817\n",
      "Epoch 55/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1742 - val_loss: 0.1668\n",
      "Epoch 56/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1738 - val_loss: 0.1640\n",
      "Epoch 57/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1852 - val_loss: 0.1724\n",
      "Epoch 58/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1663 - val_loss: 0.1622\n",
      "Epoch 59/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1800 - val_loss: 0.1692\n",
      "Epoch 60/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1708 - val_loss: 0.1713\n",
      "Epoch 61/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1841 - val_loss: 0.1612\n",
      "Epoch 62/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1730 - val_loss: 0.1885\n",
      "Epoch 63/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1705 - val_loss: 0.1791\n",
      "Epoch 64/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1679 - val_loss: 0.1901\n",
      "Epoch 65/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1655 - val_loss: 0.1619\n",
      "Epoch 66/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1740 - val_loss: 0.1633\n",
      "Epoch 67/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1744 - val_loss: 0.1745\n",
      "Epoch 68/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1835 - val_loss: 0.1739\n",
      "Epoch 69/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1729 - val_loss: 0.1640\n",
      "Epoch 70/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1706 - val_loss: 0.1522\n",
      "Epoch 71/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1669 - val_loss: 0.1671\n",
      "Epoch 72/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1721 - val_loss: 0.1715\n",
      "Epoch 73/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1677 - val_loss: 0.1570\n",
      "Epoch 74/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1652 - val_loss: 0.1620\n",
      "Epoch 75/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1635 - val_loss: 0.1596\n",
      "Epoch 76/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1688 - val_loss: 0.1623\n",
      "Epoch 77/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1562 - val_loss: 0.1511\n",
      "Epoch 78/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1549 - val_loss: 0.1612\n",
      "Epoch 79/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1586 - val_loss: 0.1607\n",
      "Epoch 80/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1623 - val_loss: 0.1644\n",
      "Epoch 81/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1625 - val_loss: 0.1565\n",
      "Epoch 82/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1709 - val_loss: 0.1516\n",
      "Epoch 83/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1561 - val_loss: 0.1601\n",
      "Epoch 84/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1655 - val_loss: 0.1642\n",
      "Epoch 85/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1662 - val_loss: 0.1671\n",
      "Epoch 86/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1627 - val_loss: 0.1596\n",
      "Epoch 87/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1652 - val_loss: 0.1659\n",
      "Epoch 88/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1607 - val_loss: 0.1509\n",
      "Epoch 89/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1704 - val_loss: 0.1929\n",
      "Epoch 90/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1573 - val_loss: 0.1686\n",
      "Epoch 91/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1567 - val_loss: 0.1490\n",
      "Epoch 92/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1774 - val_loss: 0.1572\n",
      "Epoch 93/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1581 - val_loss: 0.1504\n",
      "Epoch 94/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1569 - val_loss: 0.1628\n",
      "Epoch 95/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1616 - val_loss: 0.1555\n",
      "Epoch 96/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1594 - val_loss: 0.1686\n",
      "Epoch 97/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1538 - val_loss: 0.1501\n",
      "Epoch 98/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1674 - val_loss: 0.1491\n",
      "Epoch 99/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1568 - val_loss: 0.1506\n",
      "Epoch 100/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1471 - val_loss: 0.1639\n",
      "Epoch 101/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1592 - val_loss: 0.1561\n",
      "Epoch 102/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1536 - val_loss: 0.1686\n",
      "Epoch 103/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1578 - val_loss: 0.1538\n",
      "Epoch 104/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1609 - val_loss: 0.1603\n",
      "Epoch 105/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1540 - val_loss: 0.1653\n",
      "Epoch 106/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1550 - val_loss: 0.1453\n",
      "Epoch 107/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1594 - val_loss: 0.1484\n",
      "Epoch 108/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1628 - val_loss: 0.1637\n",
      "Epoch 109/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1771 - val_loss: 0.1717\n",
      "Epoch 110/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1493 - val_loss: 0.1662\n",
      "Epoch 111/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1569 - val_loss: 0.1446\n",
      "Epoch 112/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1598 - val_loss: 0.1443\n",
      "Epoch 113/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1587 - val_loss: 0.1524\n",
      "Epoch 114/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1511 - val_loss: 0.1526\n",
      "Epoch 115/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1571 - val_loss: 0.1745\n",
      "Epoch 116/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1603 - val_loss: 0.1551\n",
      "Epoch 117/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1488 - val_loss: 0.1632\n",
      "Epoch 118/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1570 - val_loss: 0.1633\n",
      "Epoch 119/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1581 - val_loss: 0.1564\n",
      "Epoch 120/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1527 - val_loss: 0.1938\n",
      "Epoch 121/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1593 - val_loss: 0.1537\n",
      "Epoch 122/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1584 - val_loss: 0.1468\n",
      "Epoch 123/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1449 - val_loss: 0.1516\n",
      "Epoch 124/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1515 - val_loss: 0.1444\n",
      "Epoch 125/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1451 - val_loss: 0.1564\n",
      "Epoch 126/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1376 - val_loss: 0.1505\n",
      "Epoch 127/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1539 - val_loss: 0.1459\n",
      "Epoch 128/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1485 - val_loss: 0.1504\n",
      "Epoch 129/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1594 - val_loss: 0.1434\n",
      "Epoch 130/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1453 - val_loss: 0.1600\n",
      "Epoch 131/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1438 - val_loss: 0.1452\n",
      "Epoch 132/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1598 - val_loss: 0.1633\n",
      "Epoch 133/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1509 - val_loss: 0.1488\n",
      "Epoch 134/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1579 - val_loss: 0.1404\n",
      "Epoch 135/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1397 - val_loss: 0.1370\n",
      "Epoch 136/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1418 - val_loss: 0.1533\n",
      "Epoch 137/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1575 - val_loss: 0.1716\n",
      "Epoch 138/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1462 - val_loss: 0.1425\n",
      "Epoch 139/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1472 - val_loss: 0.1513\n",
      "Epoch 140/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1520 - val_loss: 0.1513\n",
      "Epoch 141/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1487 - val_loss: 0.1374\n",
      "Epoch 142/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1444 - val_loss: 0.1505\n",
      "Epoch 143/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1387 - val_loss: 0.1395\n",
      "Epoch 144/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1495 - val_loss: 0.1637\n",
      "Epoch 145/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1450 - val_loss: 0.1592\n",
      "Epoch 146/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1448 - val_loss: 0.1527\n",
      "Epoch 147/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1380 - val_loss: 0.1462\n",
      "Epoch 148/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1503 - val_loss: 0.1340\n",
      "Epoch 149/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1465 - val_loss: 0.1468\n",
      "Epoch 150/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1518 - val_loss: 0.1800\n",
      "Epoch 151/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1390 - val_loss: 0.1421\n",
      "Epoch 152/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1406 - val_loss: 0.1375\n",
      "Epoch 153/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1453 - val_loss: 0.1424\n",
      "Epoch 154/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1563 - val_loss: 0.1622\n",
      "Epoch 155/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1401 - val_loss: 0.1323\n",
      "Epoch 156/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1479 - val_loss: 0.1455\n",
      "Epoch 157/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1430 - val_loss: 0.1554\n",
      "Epoch 158/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1444 - val_loss: 0.1376\n",
      "Epoch 159/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1496 - val_loss: 0.1414\n",
      "Epoch 160/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1538 - val_loss: 0.1365\n",
      "Epoch 161/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1464 - val_loss: 0.1482\n",
      "Epoch 162/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1513 - val_loss: 0.1507\n",
      "Epoch 163/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1491 - val_loss: 0.1405\n",
      "Epoch 164/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1472 - val_loss: 0.1405\n",
      "Epoch 165/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1478 - val_loss: 0.1408\n",
      "Epoch 166/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1509 - val_loss: 0.1483\n",
      "Epoch 167/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1405 - val_loss: 0.1423\n",
      "Epoch 168/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1450 - val_loss: 0.1409\n",
      "Epoch 169/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1419 - val_loss: 0.1449\n",
      "Epoch 170/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1380 - val_loss: 0.1459\n",
      "Epoch 171/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1444 - val_loss: 0.1511\n",
      "Epoch 172/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1458 - val_loss: 0.1390\n",
      "Epoch 173/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1352 - val_loss: 0.1567\n",
      "Epoch 174/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1452 - val_loss: 0.2013\n",
      "Epoch 175/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1489 - val_loss: 0.1355\n",
      "Epoch 176/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1419 - val_loss: 0.1427\n",
      "Epoch 177/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1405 - val_loss: 0.1328\n",
      "Epoch 178/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1477 - val_loss: 0.2054\n",
      "Epoch 179/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1413 - val_loss: 0.1402\n",
      "Epoch 180/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1471 - val_loss: 0.1525\n",
      "Epoch 181/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1445 - val_loss: 0.1409\n",
      "Epoch 182/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1412 - val_loss: 0.1452\n",
      "Epoch 183/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1414 - val_loss: 0.1370\n",
      "Epoch 184/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1335 - val_loss: 0.1309\n",
      "Epoch 185/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1408 - val_loss: 0.1477\n",
      "Epoch 186/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1493 - val_loss: 0.1361\n",
      "Epoch 187/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1435 - val_loss: 0.1918\n",
      "Epoch 188/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1347 - val_loss: 0.1378\n",
      "Epoch 189/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1398 - val_loss: 0.1397\n",
      "Epoch 190/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1398 - val_loss: 0.1404\n",
      "Epoch 191/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1387 - val_loss: 0.1513\n",
      "Epoch 192/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1385 - val_loss: 0.1315\n",
      "Epoch 193/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1386 - val_loss: 0.1836\n",
      "Epoch 194/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1363 - val_loss: 0.1333\n",
      "Epoch 195/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1392 - val_loss: 0.1406\n",
      "Epoch 196/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1339 - val_loss: 0.1419\n",
      "Epoch 197/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1445 - val_loss: 0.1588\n",
      "Epoch 198/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1370 - val_loss: 0.1403\n",
      "Epoch 199/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1365 - val_loss: 0.1366\n",
      "Epoch 200/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1398 - val_loss: 0.1304\n",
      "Epoch 201/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1448 - val_loss: 0.1359\n",
      "Epoch 202/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1382 - val_loss: 0.1308\n",
      "Epoch 203/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1304 - val_loss: 0.1372\n",
      "Epoch 204/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1391 - val_loss: 0.1755\n",
      "Epoch 205/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1486 - val_loss: 0.1363\n",
      "Epoch 206/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1315 - val_loss: 0.1357\n",
      "Epoch 207/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1392 - val_loss: 0.1373\n",
      "Epoch 208/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1480 - val_loss: 0.1505\n",
      "Epoch 209/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1333 - val_loss: 0.1485\n",
      "Epoch 210/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1482 - val_loss: 0.1386\n",
      "Epoch 211/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1468 - val_loss: 0.1322\n",
      "Epoch 212/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1465 - val_loss: 0.1325\n",
      "Epoch 213/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1383 - val_loss: 0.1397\n",
      "Epoch 214/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1352 - val_loss: 0.1279\n",
      "Epoch 215/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1398 - val_loss: 0.1389\n",
      "Epoch 216/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1386 - val_loss: 0.1349\n",
      "Epoch 217/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1443 - val_loss: 0.1353\n",
      "Epoch 218/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1325 - val_loss: 0.1275\n",
      "Epoch 219/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1222 - val_loss: 0.1376\n",
      "Epoch 220/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1429 - val_loss: 0.1505\n",
      "Epoch 221/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1347 - val_loss: 0.1341\n",
      "Epoch 222/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1215 - val_loss: 0.1309\n",
      "Epoch 223/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1329 - val_loss: 0.1491\n",
      "Epoch 224/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1378 - val_loss: 0.1318\n",
      "Epoch 225/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1408 - val_loss: 0.1472\n",
      "Epoch 226/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1339 - val_loss: 0.1298\n",
      "Epoch 227/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1434 - val_loss: 0.1477\n",
      "Epoch 228/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1346 - val_loss: 0.1287\n",
      "Epoch 229/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1295 - val_loss: 0.1345\n",
      "Epoch 230/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1356 - val_loss: 0.1476\n",
      "Epoch 231/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1373 - val_loss: 0.1281\n",
      "Epoch 232/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1399 - val_loss: 0.1473\n",
      "Epoch 233/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1324 - val_loss: 0.1394\n",
      "Epoch 234/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1375 - val_loss: 0.1274\n",
      "Epoch 235/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1391 - val_loss: 0.1365\n",
      "Epoch 236/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1345 - val_loss: 0.1352\n",
      "Epoch 237/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1358 - val_loss: 0.1280\n",
      "Epoch 238/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1313 - val_loss: 0.1338\n",
      "Epoch 239/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1374 - val_loss: 0.1836\n",
      "Epoch 240/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1494 - val_loss: 0.1429\n",
      "Epoch 241/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1360 - val_loss: 0.1621\n",
      "Epoch 242/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1381 - val_loss: 0.1329\n",
      "Epoch 243/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1207 - val_loss: 0.1317\n",
      "Epoch 244/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1330 - val_loss: 0.1354\n",
      "Epoch 245/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1424 - val_loss: 0.1244\n",
      "Epoch 246/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1310 - val_loss: 0.1310\n",
      "Epoch 247/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1252 - val_loss: 0.1383\n",
      "Epoch 248/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1258 - val_loss: 0.1380\n",
      "Epoch 249/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1345 - val_loss: 0.1629\n",
      "Epoch 250/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1413 - val_loss: 0.1554\n",
      "Epoch 251/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1315 - val_loss: 0.1464\n",
      "Epoch 252/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1320 - val_loss: 0.1309\n",
      "Epoch 253/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1341 - val_loss: 0.1380\n",
      "Epoch 254/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1414 - val_loss: 0.1389\n",
      "Epoch 255/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1438 - val_loss: 0.1462\n",
      "Epoch 256/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1335 - val_loss: 0.1512\n",
      "Epoch 257/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1344 - val_loss: 0.1519\n",
      "Epoch 258/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1407 - val_loss: 0.1372\n",
      "Epoch 259/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1280 - val_loss: 0.1352\n",
      "Epoch 260/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1323 - val_loss: 0.1326\n",
      "Epoch 261/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1326 - val_loss: 0.1262\n",
      "Epoch 262/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1277 - val_loss: 0.1332\n",
      "Epoch 263/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1219 - val_loss: 0.1321\n",
      "Epoch 264/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1381 - val_loss: 0.1283\n",
      "Epoch 265/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1297 - val_loss: 0.1646\n",
      "Epoch 266/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1373 - val_loss: 0.1501\n",
      "Epoch 267/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1345 - val_loss: 0.1644\n",
      "Epoch 268/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1384 - val_loss: 0.2284\n",
      "Epoch 269/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1252 - val_loss: 0.1324\n",
      "Epoch 270/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1440 - val_loss: 0.1351\n",
      "Epoch 271/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1288 - val_loss: 0.1369\n",
      "Epoch 272/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1417 - val_loss: 0.1296\n",
      "Epoch 273/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1319 - val_loss: 0.1321\n",
      "Epoch 274/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1361 - val_loss: 0.1353\n",
      "Epoch 275/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1295 - val_loss: 0.1316\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 276/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1171 - val_loss: 0.1182\n",
      "Epoch 277/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1174 - val_loss: 0.1163\n",
      "Epoch 278/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1136 - val_loss: 0.1245\n",
      "Epoch 279/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1119 - val_loss: 0.1243\n",
      "Epoch 280/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1251 - val_loss: 0.1165\n",
      "Epoch 281/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1163 - val_loss: 0.1193\n",
      "Epoch 282/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1189 - val_loss: 0.1210\n",
      "Epoch 283/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1227 - val_loss: 0.1252\n",
      "Epoch 284/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1131 - val_loss: 0.1352\n",
      "Epoch 285/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1198 - val_loss: 0.1293\n",
      "Epoch 286/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1200 - val_loss: 0.1278\n",
      "Epoch 287/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1168 - val_loss: 0.1236\n",
      "Epoch 288/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1332 - val_loss: 0.1175\n",
      "Epoch 289/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1203 - val_loss: 0.1182\n",
      "Epoch 290/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1221 - val_loss: 0.1196\n",
      "Epoch 291/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1062 - val_loss: 0.1296\n",
      "Epoch 292/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1142 - val_loss: 0.1185\n",
      "Epoch 293/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1249 - val_loss: 0.1379\n",
      "Epoch 294/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1222 - val_loss: 0.1404\n",
      "Epoch 295/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1143 - val_loss: 0.1242\n",
      "Epoch 296/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1196 - val_loss: 0.1182\n",
      "Epoch 297/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1119 - val_loss: 0.1174\n",
      "Epoch 298/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1163 - val_loss: 0.1177\n",
      "Epoch 299/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1161 - val_loss: 0.1251\n",
      "Epoch 300/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1237 - val_loss: 0.1335\n",
      "Epoch 301/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1228 - val_loss: 0.1192\n",
      "Epoch 302/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1127 - val_loss: 0.1482\n",
      "Epoch 303/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1196 - val_loss: 0.1209\n",
      "Epoch 304/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1113 - val_loss: 0.1314\n",
      "Epoch 305/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1231 - val_loss: 0.1153\n",
      "Epoch 306/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1170 - val_loss: 0.1174\n",
      "Epoch 307/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1107 - val_loss: 0.1277\n",
      "Epoch 308/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1225 - val_loss: 0.1319\n",
      "Epoch 309/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1151 - val_loss: 0.1180\n",
      "Epoch 310/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1152 - val_loss: 0.1172\n",
      "Epoch 311/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1233 - val_loss: 0.1185\n",
      "Epoch 312/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1165 - val_loss: 0.1198\n",
      "Epoch 313/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1213 - val_loss: 0.1195\n",
      "Epoch 314/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1163 - val_loss: 0.1256\n",
      "Epoch 315/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1288 - val_loss: 0.1215\n",
      "Epoch 316/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1083 - val_loss: 0.1229\n",
      "Epoch 317/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1195 - val_loss: 0.1180\n",
      "Epoch 318/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1116 - val_loss: 0.1157\n",
      "Epoch 319/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1185 - val_loss: 0.1178\n",
      "Epoch 320/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1162 - val_loss: 0.1157\n",
      "Epoch 321/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1216 - val_loss: 0.1309\n",
      "Epoch 322/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1214 - val_loss: 0.1219\n",
      "Epoch 323/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1132 - val_loss: 0.1170\n",
      "Epoch 324/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1176 - val_loss: 0.1159\n",
      "Epoch 325/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1232 - val_loss: 0.1349\n",
      "Epoch 326/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1260 - val_loss: 0.1200\n",
      "Epoch 327/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1105 - val_loss: 0.1335\n",
      "Epoch 328/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1161 - val_loss: 0.1181\n",
      "Epoch 329/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1223 - val_loss: 0.1198\n",
      "Epoch 330/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1106 - val_loss: 0.1213\n",
      "Epoch 331/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1146 - val_loss: 0.1154\n",
      "Epoch 332/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1103 - val_loss: 0.1158\n",
      "Epoch 333/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1113 - val_loss: 0.1170\n",
      "Epoch 334/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1214 - val_loss: 0.1259\n",
      "Epoch 335/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1180 - val_loss: 0.1276\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 336/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1107 - val_loss: 0.1113\n",
      "Epoch 337/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1084 - val_loss: 0.1132\n",
      "Epoch 338/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1169 - val_loss: 0.1119\n",
      "Epoch 339/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1095 - val_loss: 0.1165\n",
      "Epoch 340/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1116 - val_loss: 0.1112\n",
      "Epoch 341/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1068 - val_loss: 0.1121\n",
      "Epoch 342/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1160 - val_loss: 0.1114\n",
      "Epoch 343/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1130 - val_loss: 0.1148\n",
      "Epoch 344/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1177 - val_loss: 0.1123\n",
      "Epoch 345/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1071 - val_loss: 0.1116\n",
      "Epoch 346/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1169 - val_loss: 0.1165\n",
      "Epoch 347/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1099 - val_loss: 0.1235\n",
      "Epoch 348/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1118 - val_loss: 0.1127\n",
      "Epoch 349/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1105 - val_loss: 0.1108\n",
      "Epoch 350/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1156 - val_loss: 0.1158\n",
      "Epoch 351/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1161 - val_loss: 0.1209\n",
      "Epoch 352/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1091 - val_loss: 0.1099\n",
      "Epoch 353/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1036 - val_loss: 0.1130\n",
      "Epoch 354/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1174 - val_loss: 0.1125\n",
      "Epoch 355/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1064 - val_loss: 0.1108\n",
      "Epoch 356/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1153 - val_loss: 0.1132\n",
      "Epoch 357/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1114 - val_loss: 0.1125\n",
      "Epoch 358/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1156 - val_loss: 0.1171\n",
      "Epoch 359/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1028 - val_loss: 0.1105\n",
      "Epoch 360/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1053 - val_loss: 0.1128\n",
      "Epoch 361/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1084 - val_loss: 0.1115\n",
      "Epoch 362/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1151 - val_loss: 0.1120\n",
      "Epoch 363/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1202 - val_loss: 0.1103\n",
      "Epoch 364/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1186 - val_loss: 0.1141\n",
      "Epoch 365/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1145 - val_loss: 0.1214\n",
      "Epoch 366/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1108 - val_loss: 0.1152\n",
      "Epoch 367/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1112 - val_loss: 0.1111\n",
      "Epoch 368/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1201 - val_loss: 0.1115\n",
      "Epoch 369/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1101 - val_loss: 0.1101\n",
      "Epoch 370/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1155 - val_loss: 0.1112\n",
      "Epoch 371/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1186 - val_loss: 0.1168\n",
      "Epoch 372/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1038 - val_loss: 0.1144\n",
      "Epoch 373/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1103 - val_loss: 0.1104\n",
      "Epoch 374/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1079 - val_loss: 0.1102\n",
      "Epoch 375/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1097 - val_loss: 0.1124\n",
      "Epoch 376/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1157 - val_loss: 0.1111\n",
      "Epoch 377/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1201 - val_loss: 0.1169\n",
      "Epoch 378/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1133 - val_loss: 0.1244\n",
      "Epoch 379/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1147 - val_loss: 0.1156\n",
      "Epoch 380/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1069 - val_loss: 0.1135\n",
      "Epoch 381/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1157 - val_loss: 0.1130\n",
      "Epoch 382/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1114 - val_loss: 0.1115\n",
      "\n",
      "Epoch 00382: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 383/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1102 - val_loss: 0.1094\n",
      "Epoch 384/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1074 - val_loss: 0.1101\n",
      "Epoch 385/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1086 - val_loss: 0.1126\n",
      "Epoch 386/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0973 - val_loss: 0.1099\n",
      "Epoch 387/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1133 - val_loss: 0.1094\n",
      "Epoch 388/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1003 - val_loss: 0.1093\n",
      "Epoch 389/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0960 - val_loss: 0.1086\n",
      "Epoch 390/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1112 - val_loss: 0.1090\n",
      "Epoch 391/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1044 - val_loss: 0.1087\n",
      "Epoch 392/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1009 - val_loss: 0.1084\n",
      "Epoch 393/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1009 - val_loss: 0.1109\n",
      "Epoch 394/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1045 - val_loss: 0.1092\n",
      "Epoch 395/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1073 - val_loss: 0.1093\n",
      "Epoch 396/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1142 - val_loss: 0.1082\n",
      "Epoch 397/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1031 - val_loss: 0.1099\n",
      "Epoch 398/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1096 - val_loss: 0.1097\n",
      "Epoch 399/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1045 - val_loss: 0.1085\n",
      "Epoch 400/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0939 - val_loss: 0.1089\n",
      "Epoch 401/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0914 - val_loss: 0.1083\n",
      "Epoch 402/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1086 - val_loss: 0.1120\n",
      "Epoch 403/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0975 - val_loss: 0.1101\n",
      "Epoch 404/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1145 - val_loss: 0.1091\n",
      "Epoch 405/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1087 - val_loss: 0.1088\n",
      "Epoch 406/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1048 - val_loss: 0.1085\n",
      "Epoch 407/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1062 - val_loss: 0.1090\n",
      "Epoch 408/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1112 - val_loss: 0.1095\n",
      "Epoch 409/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1080 - val_loss: 0.1098\n",
      "Epoch 410/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1050 - val_loss: 0.1105\n",
      "Epoch 411/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1156 - val_loss: 0.1100\n",
      "Epoch 412/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1014 - val_loss: 0.1089\n",
      "Epoch 413/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1063 - val_loss: 0.1094\n",
      "Epoch 414/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1067 - val_loss: 0.1116\n",
      "Epoch 415/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1051 - val_loss: 0.1086\n",
      "Epoch 416/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1100 - val_loss: 0.1087\n",
      "Epoch 417/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1057 - val_loss: 0.1092\n",
      "Epoch 418/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1118 - val_loss: 0.1112\n",
      "Epoch 419/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1072 - val_loss: 0.1095\n",
      "Epoch 420/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1118 - val_loss: 0.1092\n",
      "Epoch 421/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1126 - val_loss: 0.1093\n",
      "Epoch 422/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1164 - val_loss: 0.1091\n",
      "Epoch 423/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1102 - val_loss: 0.1085\n",
      "Epoch 424/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1020 - val_loss: 0.1082\n",
      "Epoch 425/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1196 - val_loss: 0.1084\n",
      "Epoch 426/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0953 - val_loss: 0.1103\n",
      "\n",
      "Epoch 00426: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 427/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1090 - val_loss: 0.1075\n",
      "Epoch 428/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1056 - val_loss: 0.1081\n",
      "Epoch 429/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0996 - val_loss: 0.1075\n",
      "Epoch 430/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1061 - val_loss: 0.1086\n",
      "Epoch 431/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1112 - val_loss: 0.1077\n",
      "Epoch 432/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1104 - val_loss: 0.1075\n",
      "Epoch 433/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1005 - val_loss: 0.1074\n",
      "Epoch 434/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1065 - val_loss: 0.1072\n",
      "Epoch 435/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1050 - val_loss: 0.1071\n",
      "Epoch 436/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1037 - val_loss: 0.1085\n",
      "Epoch 437/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1036 - val_loss: 0.1078\n",
      "Epoch 438/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1061 - val_loss: 0.1077\n",
      "Epoch 439/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1031 - val_loss: 0.1076\n",
      "Epoch 440/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1030 - val_loss: 0.1073\n",
      "Epoch 441/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1126 - val_loss: 0.1080\n",
      "Epoch 442/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1056 - val_loss: 0.1072\n",
      "Epoch 443/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1111 - val_loss: 0.1074\n",
      "Epoch 444/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1097 - val_loss: 0.1077\n",
      "Epoch 445/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0948 - val_loss: 0.1082\n",
      "Epoch 446/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1041 - val_loss: 0.1084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0996 - val_loss: 0.1072\n",
      "Epoch 448/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1070 - val_loss: 0.1075\n",
      "Epoch 449/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1028 - val_loss: 0.1078\n",
      "Epoch 450/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0964 - val_loss: 0.1075\n",
      "Epoch 451/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1084 - val_loss: 0.1084\n",
      "Epoch 452/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1037 - val_loss: 0.1074\n",
      "Epoch 453/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1130 - val_loss: 0.1076\n",
      "Epoch 454/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1007 - val_loss: 0.1075\n",
      "Epoch 455/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0943 - val_loss: 0.1075\n",
      "Epoch 456/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1010 - val_loss: 0.1074\n",
      "Epoch 457/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0949 - val_loss: 0.1074\n",
      "Epoch 458/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1219 - val_loss: 0.1079\n",
      "Epoch 459/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1085 - val_loss: 0.1072\n",
      "Epoch 460/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1082 - val_loss: 0.1076\n",
      "Epoch 461/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1051 - val_loss: 0.1075\n",
      "Epoch 462/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1076 - val_loss: 0.1078\n",
      "Epoch 463/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1094 - val_loss: 0.1075\n",
      "Epoch 464/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1092 - val_loss: 0.1089\n",
      "Epoch 465/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1101 - val_loss: 0.1076\n",
      "\n",
      "Epoch 00465: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 466/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1085 - val_loss: 0.1074\n",
      "Epoch 467/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0904 - val_loss: 0.1075\n",
      "Epoch 468/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1003 - val_loss: 0.1070\n",
      "Epoch 469/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0938 - val_loss: 0.1067\n",
      "Epoch 470/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1046 - val_loss: 0.1072\n",
      "Epoch 471/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1093 - val_loss: 0.1070\n",
      "Epoch 472/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0984 - val_loss: 0.1067\n",
      "Epoch 473/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1057 - val_loss: 0.1070\n",
      "Epoch 474/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1069 - val_loss: 0.1070\n",
      "Epoch 475/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1148 - val_loss: 0.1068\n",
      "Epoch 476/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1008 - val_loss: 0.1067\n",
      "Epoch 477/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1114 - val_loss: 0.1070\n",
      "Epoch 478/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1065 - val_loss: 0.1073\n",
      "Epoch 479/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1120 - val_loss: 0.1069\n",
      "Epoch 480/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1043 - val_loss: 0.1067\n",
      "Epoch 481/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1112 - val_loss: 0.1066\n",
      "Epoch 482/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1061 - val_loss: 0.1068\n",
      "Epoch 483/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1128 - val_loss: 0.1067\n",
      "Epoch 484/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1062 - val_loss: 0.1066\n",
      "Epoch 485/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1009 - val_loss: 0.1066\n",
      "Epoch 486/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1089 - val_loss: 0.1068\n",
      "Epoch 487/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1035 - val_loss: 0.1066\n",
      "Epoch 488/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1045 - val_loss: 0.1068\n",
      "Epoch 489/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1122 - val_loss: 0.1067\n",
      "Epoch 490/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1024 - val_loss: 0.1068\n",
      "Epoch 491/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1005 - val_loss: 0.1066\n",
      "Epoch 492/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1029 - val_loss: 0.1068\n",
      "Epoch 493/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1107 - val_loss: 0.1065\n",
      "Epoch 494/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1052 - val_loss: 0.1068\n",
      "Epoch 495/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1079 - val_loss: 0.1067\n",
      "Epoch 496/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0979 - val_loss: 0.1070\n",
      "Epoch 497/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1026 - val_loss: 0.1071\n",
      "Epoch 498/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1094 - val_loss: 0.1067\n",
      "Epoch 499/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1048 - val_loss: 0.1067\n",
      "Epoch 500/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 501/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0988 - val_loss: 0.1065\n",
      "Epoch 502/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1033 - val_loss: 0.1068\n",
      "Epoch 503/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1069 - val_loss: 0.1069\n",
      "Epoch 504/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1018 - val_loss: 0.1066\n",
      "Epoch 505/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1020 - val_loss: 0.1073\n",
      "Epoch 506/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1075 - val_loss: 0.1070\n",
      "Epoch 507/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0999 - val_loss: 0.1066\n",
      "Epoch 508/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1040 - val_loss: 0.1065\n",
      "Epoch 509/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1054 - val_loss: 0.1068\n",
      "Epoch 510/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0904 - val_loss: 0.1073\n",
      "Epoch 511/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1077 - val_loss: 0.1066\n",
      "\n",
      "Epoch 00511: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 512/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0982 - val_loss: 0.1065\n",
      "Epoch 513/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1092 - val_loss: 0.1065\n",
      "Epoch 514/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0991 - val_loss: 0.1065\n",
      "Epoch 515/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1125 - val_loss: 0.1067\n",
      "Epoch 516/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1040 - val_loss: 0.1072\n",
      "Epoch 517/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1067 - val_loss: 0.1067\n",
      "Epoch 518/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1218 - val_loss: 0.1071\n",
      "Epoch 519/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1047 - val_loss: 0.1068\n",
      "Epoch 520/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0978 - val_loss: 0.1067\n",
      "Epoch 521/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1061 - val_loss: 0.1065\n",
      "Epoch 522/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1081 - val_loss: 0.1069\n",
      "Epoch 523/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1035 - val_loss: 0.1067\n",
      "Epoch 524/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1002 - val_loss: 0.1064\n",
      "Epoch 525/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1063 - val_loss: 0.1065\n",
      "Epoch 526/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1017 - val_loss: 0.1064\n",
      "Epoch 527/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1122 - val_loss: 0.1068\n",
      "Epoch 528/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1068 - val_loss: 0.1066\n",
      "Epoch 529/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0992 - val_loss: 0.1068\n",
      "Epoch 530/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1053 - val_loss: 0.1065\n",
      "Epoch 531/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1094 - val_loss: 0.1064\n",
      "Epoch 532/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1011 - val_loss: 0.1064\n",
      "Epoch 533/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1019 - val_loss: 0.1063\n",
      "Epoch 534/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1066 - val_loss: 0.1066\n",
      "Epoch 535/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1070 - val_loss: 0.1064\n",
      "Epoch 536/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0988 - val_loss: 0.1066\n",
      "Epoch 537/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0980 - val_loss: 0.1066\n",
      "Epoch 538/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0955 - val_loss: 0.1064\n",
      "Epoch 539/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1112 - val_loss: 0.1064\n",
      "Epoch 540/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1123 - val_loss: 0.1066\n",
      "Epoch 541/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0960 - val_loss: 0.1066\n",
      "Epoch 542/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1033 - val_loss: 0.1066\n",
      "Epoch 543/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1029 - val_loss: 0.1066\n",
      "Epoch 544/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1006 - val_loss: 0.1066\n",
      "Epoch 545/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0998 - val_loss: 0.1067\n",
      "Epoch 546/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1139 - val_loss: 0.1064\n",
      "Epoch 547/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1169 - val_loss: 0.1070\n",
      "Epoch 548/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 549/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1092 - val_loss: 0.1067\n",
      "Epoch 550/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1010 - val_loss: 0.1067\n",
      "Epoch 551/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1025 - val_loss: 0.1065\n",
      "Epoch 552/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1056 - val_loss: 0.1064\n",
      "Epoch 553/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0988 - val_loss: 0.1065\n",
      "Epoch 554/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0900 - val_loss: 0.1065\n",
      "Epoch 555/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1014 - val_loss: 0.1065\n",
      "Epoch 556/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0992 - val_loss: 0.1064\n",
      "Epoch 557/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1027 - val_loss: 0.1065\n",
      "Epoch 558/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1010 - val_loss: 0.1064\n",
      "Epoch 559/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0964 - val_loss: 0.1066\n",
      "Epoch 560/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0996 - val_loss: 0.1065\n",
      "Epoch 561/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1013 - val_loss: 0.1064\n",
      "Epoch 562/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1002 - val_loss: 0.1067\n",
      "Epoch 563/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1015 - val_loss: 0.1065\n",
      "\n",
      "Epoch 00563: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 564/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1059 - val_loss: 0.1065\n",
      "Epoch 565/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1213 - val_loss: 0.1064\n",
      "Epoch 566/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1141 - val_loss: 0.1064\n",
      "Epoch 567/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0991 - val_loss: 0.1064\n",
      "Epoch 568/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.0991 - val_loss: 0.1063\n",
      "Epoch 569/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1079 - val_loss: 0.1063\n",
      "Epoch 570/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1186 - val_loss: 0.1065\n",
      "Epoch 571/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1008 - val_loss: 0.1063\n",
      "Epoch 572/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1099 - val_loss: 0.1063\n",
      "Epoch 573/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1070 - val_loss: 0.1063\n",
      "Epoch 574/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1041 - val_loss: 0.1064\n",
      "Epoch 575/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1028 - val_loss: 0.1063\n",
      "Epoch 576/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1094 - val_loss: 0.1064\n",
      "Epoch 577/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1029 - val_loss: 0.1064\n",
      "Epoch 578/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1038 - val_loss: 0.1065\n",
      "Epoch 579/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1084 - val_loss: 0.1063\n",
      "Epoch 580/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1104 - val_loss: 0.1063\n",
      "Epoch 581/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1015 - val_loss: 0.1063\n",
      "Epoch 582/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1054 - val_loss: 0.1064\n",
      "Epoch 583/2000\n",
      "95787/95787 [==============================] - 1s 11us/step - loss: 0.1037 - val_loss: 0.1065\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00583: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOuwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.241552452835503\n",
      "Training 2JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 911004 samples, validate on 229670 samples\n",
      "Epoch 1/2000\n",
      "911004/911004 [==============================] - 11s 12us/step - loss: 0.8060 - val_loss: 0.5989\n",
      "Epoch 2/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.5835 - val_loss: 0.5110\n",
      "Epoch 3/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.5222 - val_loss: 0.4623\n",
      "Epoch 4/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.4874 - val_loss: 0.4412\n",
      "Epoch 5/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.4621 - val_loss: 0.4170\n",
      "Epoch 6/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.4417 - val_loss: 0.4008\n",
      "Epoch 7/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.4266 - val_loss: 0.3804\n",
      "Epoch 8/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.4121 - val_loss: 0.3792\n",
      "Epoch 9/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.4003 - val_loss: 0.3674\n",
      "Epoch 10/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3892 - val_loss: 0.3583\n",
      "Epoch 11/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3807 - val_loss: 0.3502\n",
      "Epoch 12/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3730 - val_loss: 0.3426\n",
      "Epoch 13/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3660 - val_loss: 0.3393\n",
      "Epoch 14/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3584 - val_loss: 0.3471\n",
      "Epoch 15/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3520 - val_loss: 0.3359\n",
      "Epoch 16/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3438 - val_loss: 0.3207\n",
      "Epoch 17/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3394 - val_loss: 0.3198\n",
      "Epoch 18/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3361 - val_loss: 0.3097\n",
      "Epoch 19/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3334 - val_loss: 0.3082\n",
      "Epoch 20/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3295 - val_loss: 0.3015\n",
      "Epoch 21/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3252 - val_loss: 0.2989\n",
      "Epoch 22/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3216 - val_loss: 0.2952\n",
      "Epoch 23/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3196 - val_loss: 0.2977\n",
      "Epoch 24/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3143 - val_loss: 0.2882\n",
      "Epoch 25/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3106 - val_loss: 0.2862\n",
      "Epoch 26/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3089 - val_loss: 0.2887\n",
      "Epoch 27/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3033 - val_loss: 0.2787\n",
      "Epoch 28/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3022 - val_loss: 0.2908\n",
      "Epoch 29/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3029 - val_loss: 0.2794\n",
      "Epoch 30/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.3013 - val_loss: 0.2694\n",
      "Epoch 31/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2960 - val_loss: 0.2730\n",
      "Epoch 32/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2961 - val_loss: 0.2692\n",
      "Epoch 33/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2944 - val_loss: 0.2665\n",
      "Epoch 34/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2920 - val_loss: 0.2661\n",
      "Epoch 35/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2885 - val_loss: 0.2774\n",
      "Epoch 36/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2890 - val_loss: 0.2715\n",
      "Epoch 37/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2865 - val_loss: 0.2670\n",
      "Epoch 38/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2860 - val_loss: 0.2639\n",
      "Epoch 39/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2834 - val_loss: 0.2675\n",
      "Epoch 40/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2799 - val_loss: 0.2734\n",
      "Epoch 41/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2798 - val_loss: 0.2542\n",
      "Epoch 42/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2794 - val_loss: 0.2593\n",
      "Epoch 43/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2770 - val_loss: 0.2674\n",
      "Epoch 44/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2749 - val_loss: 0.2606\n",
      "Epoch 45/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2748 - val_loss: 0.2525\n",
      "Epoch 46/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2724 - val_loss: 0.2530\n",
      "Epoch 47/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2721 - val_loss: 0.2587\n",
      "Epoch 48/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2723 - val_loss: 0.2534\n",
      "Epoch 49/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2694 - val_loss: 0.2513\n",
      "Epoch 50/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2695 - val_loss: 0.2500\n",
      "Epoch 51/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2671 - val_loss: 0.2515\n",
      "Epoch 52/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2677 - val_loss: 0.2450\n",
      "Epoch 53/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2678 - val_loss: 0.2482\n",
      "Epoch 54/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2666 - val_loss: 0.2482\n",
      "Epoch 55/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2637 - val_loss: 0.2442\n",
      "Epoch 56/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2638 - val_loss: 0.2433\n",
      "Epoch 57/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2632 - val_loss: 0.2441\n",
      "Epoch 58/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2634 - val_loss: 0.2432\n",
      "Epoch 59/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2625 - val_loss: 0.2474\n",
      "Epoch 60/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2644 - val_loss: 0.2428\n",
      "Epoch 61/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2595 - val_loss: 0.2524\n",
      "Epoch 62/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2603 - val_loss: 0.2436\n",
      "Epoch 63/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2583 - val_loss: 0.2379\n",
      "Epoch 64/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2601 - val_loss: 0.2396\n",
      "Epoch 65/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2599 - val_loss: 0.2410\n",
      "Epoch 66/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2580 - val_loss: 0.2370\n",
      "Epoch 67/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2551 - val_loss: 0.2381\n",
      "Epoch 68/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2565 - val_loss: 0.2373\n",
      "Epoch 69/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2550 - val_loss: 0.2343\n",
      "Epoch 70/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2554 - val_loss: 0.2386\n",
      "Epoch 71/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2525 - val_loss: 0.2345\n",
      "Epoch 72/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2546 - val_loss: 0.2333\n",
      "Epoch 73/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2512 - val_loss: 0.2414\n",
      "Epoch 74/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2515 - val_loss: 0.2366\n",
      "Epoch 75/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2519 - val_loss: 0.2409\n",
      "Epoch 76/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2479 - val_loss: 0.2335\n",
      "Epoch 77/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2526 - val_loss: 0.2350\n",
      "Epoch 78/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2496 - val_loss: 0.2356\n",
      "Epoch 79/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2500 - val_loss: 0.2290\n",
      "Epoch 80/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2481 - val_loss: 0.2304\n",
      "Epoch 81/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2489 - val_loss: 0.2314\n",
      "Epoch 82/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2490 - val_loss: 0.2347\n",
      "Epoch 83/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2491 - val_loss: 0.2321\n",
      "Epoch 84/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2479 - val_loss: 0.2298\n",
      "Epoch 85/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2448 - val_loss: 0.2282\n",
      "Epoch 86/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2427 - val_loss: 0.2332\n",
      "Epoch 87/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2456 - val_loss: 0.2310\n",
      "Epoch 88/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2464 - val_loss: 0.2279\n",
      "Epoch 89/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2428 - val_loss: 0.2306\n",
      "Epoch 90/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2413 - val_loss: 0.2283\n",
      "Epoch 91/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2455 - val_loss: 0.2348\n",
      "Epoch 92/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2417 - val_loss: 0.2263\n",
      "Epoch 93/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2419 - val_loss: 0.2258\n",
      "Epoch 94/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2418 - val_loss: 0.2279\n",
      "Epoch 95/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2402 - val_loss: 0.2267\n",
      "Epoch 96/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2442 - val_loss: 0.2276\n",
      "Epoch 97/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2441 - val_loss: 0.2262\n",
      "Epoch 98/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2395 - val_loss: 0.2248\n",
      "Epoch 99/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2432 - val_loss: 0.2324\n",
      "Epoch 100/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2405 - val_loss: 0.2258\n",
      "Epoch 101/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2389 - val_loss: 0.2311\n",
      "Epoch 102/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2403 - val_loss: 0.2249\n",
      "Epoch 103/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2399 - val_loss: 0.2226\n",
      "Epoch 104/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2402 - val_loss: 0.2275\n",
      "Epoch 105/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2376 - val_loss: 0.2214\n",
      "Epoch 106/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2359 - val_loss: 0.2252\n",
      "Epoch 107/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2408 - val_loss: 0.2224\n",
      "Epoch 108/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2366 - val_loss: 0.2290\n",
      "Epoch 109/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2380 - val_loss: 0.2203\n",
      "Epoch 110/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2346 - val_loss: 0.2220\n",
      "Epoch 111/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2369 - val_loss: 0.2296\n",
      "Epoch 112/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2371 - val_loss: 0.2214\n",
      "Epoch 113/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2340 - val_loss: 0.2237\n",
      "Epoch 114/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2325 - val_loss: 0.2211\n",
      "Epoch 115/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2339 - val_loss: 0.2224\n",
      "Epoch 116/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2360 - val_loss: 0.2189\n",
      "Epoch 117/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2337 - val_loss: 0.2233\n",
      "Epoch 118/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2332 - val_loss: 0.2224\n",
      "Epoch 119/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2342 - val_loss: 0.2169\n",
      "Epoch 120/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2338 - val_loss: 0.2214\n",
      "Epoch 121/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2311 - val_loss: 0.2203\n",
      "Epoch 122/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2329 - val_loss: 0.2151\n",
      "Epoch 123/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2317 - val_loss: 0.2210\n",
      "Epoch 124/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2332 - val_loss: 0.2249\n",
      "Epoch 125/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2297 - val_loss: 0.2179\n",
      "Epoch 126/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2304 - val_loss: 0.2177\n",
      "Epoch 127/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2316 - val_loss: 0.2186\n",
      "Epoch 128/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2319 - val_loss: 0.2202\n",
      "Epoch 129/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2309 - val_loss: 0.2211\n",
      "Epoch 130/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2307 - val_loss: 0.2212\n",
      "Epoch 131/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2321 - val_loss: 0.2143\n",
      "Epoch 132/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2292 - val_loss: 0.2183\n",
      "Epoch 133/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2308 - val_loss: 0.2151\n",
      "Epoch 134/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2312 - val_loss: 0.2292\n",
      "Epoch 135/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2296 - val_loss: 0.2145\n",
      "Epoch 136/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2277 - val_loss: 0.2230\n",
      "Epoch 137/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2301 - val_loss: 0.2160\n",
      "Epoch 138/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2271 - val_loss: 0.2184\n",
      "Epoch 139/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2273 - val_loss: 0.2132\n",
      "Epoch 140/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2271 - val_loss: 0.2173\n",
      "Epoch 141/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2308 - val_loss: 0.2145\n",
      "Epoch 142/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2293 - val_loss: 0.2146\n",
      "Epoch 143/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2274 - val_loss: 0.2130\n",
      "Epoch 144/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2247 - val_loss: 0.2251\n",
      "Epoch 145/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2279 - val_loss: 0.2140\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2285 - val_loss: 0.2128\n",
      "Epoch 147/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2269 - val_loss: 0.2194\n",
      "Epoch 148/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2262 - val_loss: 0.2160\n",
      "Epoch 149/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2224 - val_loss: 0.2169\n",
      "Epoch 150/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2267 - val_loss: 0.2157\n",
      "Epoch 151/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2257 - val_loss: 0.2157\n",
      "Epoch 152/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2256 - val_loss: 0.2147\n",
      "Epoch 153/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2211 - val_loss: 0.2241\n",
      "Epoch 154/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2257 - val_loss: 0.2101\n",
      "Epoch 155/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2282 - val_loss: 0.2087\n",
      "Epoch 156/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2259 - val_loss: 0.2180\n",
      "Epoch 157/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2251 - val_loss: 0.2158\n",
      "Epoch 158/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2244 - val_loss: 0.2140\n",
      "Epoch 159/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2232 - val_loss: 0.2168\n",
      "Epoch 160/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2227 - val_loss: 0.2104\n",
      "Epoch 161/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2226 - val_loss: 0.2113\n",
      "Epoch 162/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2230 - val_loss: 0.2109\n",
      "Epoch 163/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2236 - val_loss: 0.2121\n",
      "Epoch 164/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2234 - val_loss: 0.2154\n",
      "Epoch 165/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2255 - val_loss: 0.2127\n",
      "Epoch 166/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2222 - val_loss: 0.2151\n",
      "Epoch 167/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2246 - val_loss: 0.2155\n",
      "Epoch 168/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2249 - val_loss: 0.2113\n",
      "Epoch 169/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2205 - val_loss: 0.2072\n",
      "Epoch 170/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2224 - val_loss: 0.2109\n",
      "Epoch 171/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2215 - val_loss: 0.2137\n",
      "Epoch 172/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2233 - val_loss: 0.2125\n",
      "Epoch 173/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2218 - val_loss: 0.2131\n",
      "Epoch 174/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2232 - val_loss: 0.2094\n",
      "Epoch 175/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2167 - val_loss: 0.2131\n",
      "Epoch 176/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2228 - val_loss: 0.2142\n",
      "Epoch 177/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2204 - val_loss: 0.2109\n",
      "Epoch 178/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2206 - val_loss: 0.2098\n",
      "Epoch 179/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2210 - val_loss: 0.2075\n",
      "Epoch 180/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2228 - val_loss: 0.2062\n",
      "Epoch 181/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2196 - val_loss: 0.2088\n",
      "Epoch 182/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2208 - val_loss: 0.2141\n",
      "Epoch 183/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2205 - val_loss: 0.2101\n",
      "Epoch 184/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2200 - val_loss: 0.2088\n",
      "Epoch 185/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2182 - val_loss: 0.2094\n",
      "Epoch 186/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2209 - val_loss: 0.2033\n",
      "Epoch 187/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2193 - val_loss: 0.2096\n",
      "Epoch 188/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2191 - val_loss: 0.2074\n",
      "Epoch 189/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2165 - val_loss: 0.2050\n",
      "Epoch 190/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2195 - val_loss: 0.2196\n",
      "Epoch 191/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2152 - val_loss: 0.2094\n",
      "Epoch 192/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2162 - val_loss: 0.2091\n",
      "Epoch 193/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2175 - val_loss: 0.2070\n",
      "Epoch 194/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2178 - val_loss: 0.2119\n",
      "Epoch 195/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2192 - val_loss: 0.2124\n",
      "Epoch 196/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2197 - val_loss: 0.2070\n",
      "Epoch 197/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2201 - val_loss: 0.2062\n",
      "Epoch 198/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2185 - val_loss: 0.2071\n",
      "Epoch 199/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2185 - val_loss: 0.2043\n",
      "Epoch 200/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2191 - val_loss: 0.2061\n",
      "Epoch 201/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2182 - val_loss: 0.2067\n",
      "Epoch 202/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2174 - val_loss: 0.2074\n",
      "Epoch 203/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2172 - val_loss: 0.2182\n",
      "Epoch 204/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2176 - val_loss: 0.2041\n",
      "Epoch 205/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2163 - val_loss: 0.2157\n",
      "Epoch 206/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2183 - val_loss: 0.2067\n",
      "Epoch 207/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2177 - val_loss: 0.2078\n",
      "Epoch 208/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2178 - val_loss: 0.2102\n",
      "Epoch 209/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2160 - val_loss: 0.2067\n",
      "Epoch 210/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2135 - val_loss: 0.2080\n",
      "Epoch 211/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2190 - val_loss: 0.2045\n",
      "Epoch 212/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2185 - val_loss: 0.2094\n",
      "Epoch 213/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2175 - val_loss: 0.2113\n",
      "Epoch 214/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2137 - val_loss: 0.2081\n",
      "Epoch 215/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2173 - val_loss: 0.2032\n",
      "Epoch 216/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2151 - val_loss: 0.2069\n",
      "Epoch 217/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2155 - val_loss: 0.2035\n",
      "Epoch 218/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2158 - val_loss: 0.2141\n",
      "Epoch 219/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2159 - val_loss: 0.2064\n",
      "Epoch 220/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2171 - val_loss: 0.2043\n",
      "Epoch 221/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2164 - val_loss: 0.2052\n",
      "Epoch 222/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2162 - val_loss: 0.2052\n",
      "Epoch 223/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2152 - val_loss: 0.2119\n",
      "Epoch 224/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2144 - val_loss: 0.2043\n",
      "Epoch 225/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2142 - val_loss: 0.2047\n",
      "Epoch 226/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2158 - val_loss: 0.2131\n",
      "Epoch 227/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2175 - val_loss: 0.2028\n",
      "Epoch 228/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2157 - val_loss: 0.2087\n",
      "Epoch 229/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2137 - val_loss: 0.2013\n",
      "Epoch 230/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2142 - val_loss: 0.2041\n",
      "Epoch 231/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2115 - val_loss: 0.2168\n",
      "Epoch 232/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2156 - val_loss: 0.2065\n",
      "Epoch 233/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2114 - val_loss: 0.2151\n",
      "Epoch 234/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2156 - val_loss: 0.2043\n",
      "Epoch 235/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2139 - val_loss: 0.2047\n",
      "Epoch 236/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2146 - val_loss: 0.2054\n",
      "Epoch 237/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2154 - val_loss: 0.2032\n",
      "Epoch 238/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2137 - val_loss: 0.2014\n",
      "Epoch 239/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2143 - val_loss: 0.2057\n",
      "Epoch 240/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2132 - val_loss: 0.2013\n",
      "Epoch 241/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2143 - val_loss: 0.2048\n",
      "Epoch 242/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2134 - val_loss: 0.2040\n",
      "Epoch 243/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2109 - val_loss: 0.2055\n",
      "Epoch 244/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2122 - val_loss: 0.2032\n",
      "Epoch 245/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2136 - val_loss: 0.2103\n",
      "Epoch 246/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2146 - val_loss: 0.2074\n",
      "Epoch 247/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2128 - val_loss: 0.2031\n",
      "Epoch 248/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2127 - val_loss: 0.2050\n",
      "Epoch 249/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2174 - val_loss: 0.2027\n",
      "Epoch 250/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2104 - val_loss: 0.2108\n",
      "Epoch 251/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2137 - val_loss: 0.2055\n",
      "Epoch 252/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2132 - val_loss: 0.2040\n",
      "Epoch 253/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2117 - val_loss: 0.2035\n",
      "Epoch 254/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2121 - val_loss: 0.2021\n",
      "Epoch 255/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2126 - val_loss: 0.2007\n",
      "Epoch 256/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2117 - val_loss: 0.2018\n",
      "Epoch 257/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2115 - val_loss: 0.2006\n",
      "Epoch 258/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2123 - val_loss: 0.1998\n",
      "Epoch 259/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2097 - val_loss: 0.2005\n",
      "Epoch 260/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2118 - val_loss: 0.2084\n",
      "Epoch 261/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2115 - val_loss: 0.1994\n",
      "Epoch 262/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2115 - val_loss: 0.2105\n",
      "Epoch 263/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2098 - val_loss: 0.2059\n",
      "Epoch 264/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2095 - val_loss: 0.2006\n",
      "Epoch 265/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2079 - val_loss: 0.2006\n",
      "Epoch 266/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2095 - val_loss: 0.2050\n",
      "Epoch 267/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2131 - val_loss: 0.1994\n",
      "Epoch 268/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2117 - val_loss: 0.2037\n",
      "Epoch 269/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2126 - val_loss: 0.2034\n",
      "Epoch 270/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2105 - val_loss: 0.1977\n",
      "Epoch 271/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2113 - val_loss: 0.2013\n",
      "Epoch 272/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2119 - val_loss: 0.2055\n",
      "Epoch 273/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2131 - val_loss: 0.2076\n",
      "Epoch 274/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2081 - val_loss: 0.1986\n",
      "Epoch 275/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2092 - val_loss: 0.2028\n",
      "Epoch 276/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2071 - val_loss: 0.2013\n",
      "Epoch 277/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2114 - val_loss: 0.2018\n",
      "Epoch 278/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2084 - val_loss: 0.1986\n",
      "Epoch 279/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2082 - val_loss: 0.1996\n",
      "Epoch 280/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2114 - val_loss: 0.1983\n",
      "Epoch 281/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2100 - val_loss: 0.2036\n",
      "Epoch 282/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2114 - val_loss: 0.2058\n",
      "Epoch 283/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2096 - val_loss: 0.2170\n",
      "Epoch 284/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2060 - val_loss: 0.1972\n",
      "Epoch 285/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2089 - val_loss: 0.2000\n",
      "Epoch 286/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2084 - val_loss: 0.1979\n",
      "Epoch 287/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2115 - val_loss: 0.1984\n",
      "Epoch 288/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2098 - val_loss: 0.1984\n",
      "Epoch 289/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2084 - val_loss: 0.1987\n",
      "Epoch 290/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2078 - val_loss: 0.2012\n",
      "Epoch 291/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2092 - val_loss: 0.2029\n",
      "Epoch 292/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2076 - val_loss: 0.2047\n",
      "Epoch 293/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2108 - val_loss: 0.2051\n",
      "Epoch 294/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2107 - val_loss: 0.1992\n",
      "Epoch 295/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2112 - val_loss: 0.1982\n",
      "Epoch 296/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2127 - val_loss: 0.2012\n",
      "Epoch 297/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2100 - val_loss: 0.1968\n",
      "Epoch 298/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2082 - val_loss: 0.1994\n",
      "Epoch 299/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2065 - val_loss: 0.1976\n",
      "Epoch 300/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2068 - val_loss: 0.2009\n",
      "Epoch 301/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2087 - val_loss: 0.1990\n",
      "Epoch 302/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2060 - val_loss: 0.1996\n",
      "Epoch 303/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2088 - val_loss: 0.2024\n",
      "Epoch 304/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2096 - val_loss: 0.1971\n",
      "Epoch 305/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2077 - val_loss: 0.2019\n",
      "Epoch 306/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2068 - val_loss: 0.1997\n",
      "Epoch 307/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2095 - val_loss: 0.2008\n",
      "Epoch 308/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2068 - val_loss: 0.2000\n",
      "Epoch 309/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2059 - val_loss: 0.1981\n",
      "Epoch 310/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2076 - val_loss: 0.1996\n",
      "Epoch 311/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2039 - val_loss: 0.1974\n",
      "Epoch 312/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2103 - val_loss: 0.2012\n",
      "Epoch 313/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2082 - val_loss: 0.1969\n",
      "Epoch 314/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2043 - val_loss: 0.1969\n",
      "Epoch 315/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2110 - val_loss: 0.1951\n",
      "Epoch 316/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2063 - val_loss: 0.1973\n",
      "Epoch 317/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2069 - val_loss: 0.2051\n",
      "Epoch 318/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2062 - val_loss: 0.2027\n",
      "Epoch 319/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2055 - val_loss: 0.1953\n",
      "Epoch 320/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2072 - val_loss: 0.2078\n",
      "Epoch 321/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2071 - val_loss: 0.1978\n",
      "Epoch 322/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2077 - val_loss: 0.1953\n",
      "Epoch 323/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2101 - val_loss: 0.2000\n",
      "Epoch 324/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2029 - val_loss: 0.1986\n",
      "Epoch 325/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2060 - val_loss: 0.2008\n",
      "Epoch 326/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2070 - val_loss: 0.1967\n",
      "Epoch 327/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2064 - val_loss: 0.1979\n",
      "Epoch 328/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2066 - val_loss: 0.1969\n",
      "Epoch 329/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2085 - val_loss: 0.1967\n",
      "Epoch 330/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2054 - val_loss: 0.1983\n",
      "Epoch 331/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2091 - val_loss: 0.2020\n",
      "Epoch 332/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2086 - val_loss: 0.2046\n",
      "Epoch 333/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2057 - val_loss: 0.1958\n",
      "Epoch 334/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2057 - val_loss: 0.1967\n",
      "Epoch 335/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2060 - val_loss: 0.2023\n",
      "Epoch 336/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2067 - val_loss: 0.1949\n",
      "Epoch 337/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2049 - val_loss: 0.1975\n",
      "Epoch 338/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2035 - val_loss: 0.2010\n",
      "Epoch 339/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2024 - val_loss: 0.1964\n",
      "Epoch 340/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2077 - val_loss: 0.1965\n",
      "Epoch 341/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2047 - val_loss: 0.1992\n",
      "Epoch 342/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2077 - val_loss: 0.1952\n",
      "Epoch 343/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2056 - val_loss: 0.1974\n",
      "Epoch 344/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2069 - val_loss: 0.1944\n",
      "Epoch 345/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2061 - val_loss: 0.1969\n",
      "Epoch 346/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2039 - val_loss: 0.2018\n",
      "Epoch 347/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2050 - val_loss: 0.2047\n",
      "Epoch 348/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2050 - val_loss: 0.2006\n",
      "Epoch 349/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2025 - val_loss: 0.1961\n",
      "Epoch 350/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2042 - val_loss: 0.1986\n",
      "Epoch 351/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2080 - val_loss: 0.1971\n",
      "Epoch 352/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2062 - val_loss: 0.2035\n",
      "Epoch 353/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2026 - val_loss: 0.2031\n",
      "Epoch 354/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2042 - val_loss: 0.1988\n",
      "Epoch 355/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2068 - val_loss: 0.2057\n",
      "Epoch 356/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2044 - val_loss: 0.1937\n",
      "Epoch 357/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2054 - val_loss: 0.1958\n",
      "Epoch 358/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2030 - val_loss: 0.1978\n",
      "Epoch 359/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2051 - val_loss: 0.1960\n",
      "Epoch 360/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2043 - val_loss: 0.1936\n",
      "Epoch 361/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2056 - val_loss: 0.1989\n",
      "Epoch 362/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2043 - val_loss: 0.1979\n",
      "Epoch 363/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2021 - val_loss: 0.1937\n",
      "Epoch 364/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2048 - val_loss: 0.1930\n",
      "Epoch 365/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2013 - val_loss: 0.1931\n",
      "Epoch 366/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2052 - val_loss: 0.1954\n",
      "Epoch 367/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2064 - val_loss: 0.1945\n",
      "Epoch 368/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2065 - val_loss: 0.1934\n",
      "Epoch 369/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2079 - val_loss: 0.1956\n",
      "Epoch 370/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2032 - val_loss: 0.1958\n",
      "Epoch 371/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2051 - val_loss: 0.2041\n",
      "Epoch 372/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2015 - val_loss: 0.1958\n",
      "Epoch 373/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2051 - val_loss: 0.1943\n",
      "Epoch 374/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2009 - val_loss: 0.1999\n",
      "Epoch 375/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2036 - val_loss: 0.1943\n",
      "Epoch 376/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2029 - val_loss: 0.1952\n",
      "Epoch 377/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2017 - val_loss: 0.1932\n",
      "Epoch 378/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2038 - val_loss: 0.1930\n",
      "Epoch 379/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2058 - val_loss: 0.1960\n",
      "Epoch 380/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2071 - val_loss: 0.2018\n",
      "Epoch 381/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1999 - val_loss: 0.1942\n",
      "Epoch 382/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1996 - val_loss: 0.1935\n",
      "Epoch 383/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2067 - val_loss: 0.1985\n",
      "Epoch 384/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2039 - val_loss: 0.1957\n",
      "Epoch 385/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2034 - val_loss: 0.1978\n",
      "Epoch 386/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2038 - val_loss: 0.1933\n",
      "Epoch 387/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2053 - val_loss: 0.1943\n",
      "Epoch 388/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2084 - val_loss: 0.2009\n",
      "Epoch 389/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2050 - val_loss: 0.1976\n",
      "Epoch 390/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2010 - val_loss: 0.2055\n",
      "Epoch 391/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2041 - val_loss: 0.2019\n",
      "Epoch 392/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2033 - val_loss: 0.1920\n",
      "Epoch 393/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2000 - val_loss: 0.1977\n",
      "Epoch 394/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2024 - val_loss: 0.1928\n",
      "Epoch 395/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2053 - val_loss: 0.1968\n",
      "Epoch 396/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2026 - val_loss: 0.1931\n",
      "Epoch 397/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2025 - val_loss: 0.1939\n",
      "Epoch 398/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2005 - val_loss: 0.1935\n",
      "Epoch 399/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2022 - val_loss: 0.1933\n",
      "Epoch 400/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2002 - val_loss: 0.1937\n",
      "Epoch 401/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2031 - val_loss: 0.1984\n",
      "Epoch 402/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2021 - val_loss: 0.2077\n",
      "Epoch 403/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2063 - val_loss: 0.1924\n",
      "Epoch 404/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1997 - val_loss: 0.1936\n",
      "Epoch 405/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1991 - val_loss: 0.2062\n",
      "Epoch 406/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2005 - val_loss: 0.1942\n",
      "Epoch 407/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2014 - val_loss: 0.1952\n",
      "Epoch 408/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2021 - val_loss: 0.1979\n",
      "Epoch 409/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2022 - val_loss: 0.1971\n",
      "Epoch 410/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2017 - val_loss: 0.2028\n",
      "Epoch 411/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2017 - val_loss: 0.1953\n",
      "Epoch 412/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1995 - val_loss: 0.1944\n",
      "Epoch 413/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2010 - val_loss: 0.1915\n",
      "Epoch 414/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2003 - val_loss: 0.2006\n",
      "Epoch 415/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2022 - val_loss: 0.1958\n",
      "Epoch 416/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2020 - val_loss: 0.1945\n",
      "Epoch 417/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2030 - val_loss: 0.1964\n",
      "Epoch 418/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1997 - val_loss: 0.1971\n",
      "Epoch 419/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2021 - val_loss: 0.1966\n",
      "Epoch 420/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2007 - val_loss: 0.2046\n",
      "Epoch 421/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2001 - val_loss: 0.1923\n",
      "Epoch 422/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2050 - val_loss: 0.2010\n",
      "Epoch 423/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2005 - val_loss: 0.1989\n",
      "Epoch 424/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1996 - val_loss: 0.1924\n",
      "Epoch 425/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2009 - val_loss: 0.1952\n",
      "Epoch 426/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2018 - val_loss: 0.1960\n",
      "Epoch 427/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2036 - val_loss: 0.1968\n",
      "Epoch 428/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2001 - val_loss: 0.1956\n",
      "Epoch 429/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2025 - val_loss: 0.1945\n",
      "Epoch 430/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2028 - val_loss: 0.1946\n",
      "Epoch 431/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2001 - val_loss: 0.1940\n",
      "Epoch 432/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1980 - val_loss: 0.1960\n",
      "Epoch 433/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1990 - val_loss: 0.1905\n",
      "Epoch 434/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2012 - val_loss: 0.1915\n",
      "Epoch 435/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2038 - val_loss: 0.1961\n",
      "Epoch 436/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2019 - val_loss: 0.1926\n",
      "Epoch 437/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2029 - val_loss: 0.1932\n",
      "Epoch 438/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1974 - val_loss: 0.1931\n",
      "Epoch 439/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1982 - val_loss: 0.1948\n",
      "Epoch 440/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1989 - val_loss: 0.1916\n",
      "Epoch 441/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2022 - val_loss: 0.2008\n",
      "Epoch 442/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1979 - val_loss: 0.1921\n",
      "Epoch 443/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1994 - val_loss: 0.1938\n",
      "Epoch 444/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1989 - val_loss: 0.1943\n",
      "Epoch 445/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2005 - val_loss: 0.1959\n",
      "Epoch 446/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2008 - val_loss: 0.1959\n",
      "Epoch 447/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2013 - val_loss: 0.1942\n",
      "Epoch 448/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1982 - val_loss: 0.1934\n",
      "Epoch 449/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1978 - val_loss: 0.1954\n",
      "Epoch 450/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2000 - val_loss: 0.1922\n",
      "Epoch 451/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1990 - val_loss: 0.1954\n",
      "Epoch 452/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2012 - val_loss: 0.2022\n",
      "Epoch 453/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2036 - val_loss: 0.1942\n",
      "Epoch 454/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2015 - val_loss: 0.1948\n",
      "Epoch 455/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2010 - val_loss: 0.1909\n",
      "Epoch 456/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1995 - val_loss: 0.1943\n",
      "Epoch 457/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1962 - val_loss: 0.2045\n",
      "Epoch 458/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2012 - val_loss: 0.1979\n",
      "Epoch 459/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1983 - val_loss: 0.1932\n",
      "Epoch 460/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1994 - val_loss: 0.2006\n",
      "Epoch 461/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2005 - val_loss: 0.2002\n",
      "Epoch 462/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1982 - val_loss: 0.1974\n",
      "Epoch 463/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.2000 - val_loss: 0.1913\n",
      "\n",
      "Epoch 00463: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 464/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1940 - val_loss: 0.1838\n",
      "Epoch 465/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1936 - val_loss: 0.1841\n",
      "Epoch 466/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1922 - val_loss: 0.1863\n",
      "Epoch 467/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1901 - val_loss: 0.1839\n",
      "Epoch 468/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1905 - val_loss: 0.1888\n",
      "Epoch 469/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1895 - val_loss: 0.1855\n",
      "Epoch 470/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1894 - val_loss: 0.1846\n",
      "Epoch 471/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1933 - val_loss: 0.1849\n",
      "Epoch 472/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1903 - val_loss: 0.1836\n",
      "Epoch 473/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1836\n",
      "Epoch 474/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1919 - val_loss: 0.1837\n",
      "Epoch 475/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1838\n",
      "Epoch 476/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1914 - val_loss: 0.1840\n",
      "Epoch 477/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1911 - val_loss: 0.1841\n",
      "Epoch 478/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1900 - val_loss: 0.1837\n",
      "Epoch 479/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1915 - val_loss: 0.1844\n",
      "Epoch 480/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1884 - val_loss: 0.1830\n",
      "Epoch 481/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1909 - val_loss: 0.1836\n",
      "Epoch 482/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1869 - val_loss: 0.1837\n",
      "Epoch 483/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1904 - val_loss: 0.1849\n",
      "Epoch 484/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1921 - val_loss: 0.1843\n",
      "Epoch 485/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1870 - val_loss: 0.1823\n",
      "Epoch 486/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1859 - val_loss: 0.1836\n",
      "Epoch 487/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1875 - val_loss: 0.1841\n",
      "Epoch 488/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1928 - val_loss: 0.1849\n",
      "Epoch 489/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1923 - val_loss: 0.1822\n",
      "Epoch 490/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1921 - val_loss: 0.1829\n",
      "Epoch 491/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1835\n",
      "Epoch 492/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1897 - val_loss: 0.1837\n",
      "Epoch 493/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1890 - val_loss: 0.1835\n",
      "Epoch 494/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1898 - val_loss: 0.1835\n",
      "Epoch 495/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1882 - val_loss: 0.1844\n",
      "Epoch 496/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1882 - val_loss: 0.1832\n",
      "Epoch 497/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1865 - val_loss: 0.1826\n",
      "Epoch 498/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1836\n",
      "Epoch 499/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1869 - val_loss: 0.1837\n",
      "Epoch 500/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1901 - val_loss: 0.1838\n",
      "Epoch 501/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1868 - val_loss: 0.1868\n",
      "Epoch 502/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1892 - val_loss: 0.1838\n",
      "Epoch 503/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1921 - val_loss: 0.1834\n",
      "Epoch 504/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1902 - val_loss: 0.1851\n",
      "Epoch 505/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1826\n",
      "Epoch 506/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1860 - val_loss: 0.1859\n",
      "Epoch 507/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1885\n",
      "Epoch 508/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1847\n",
      "Epoch 509/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1834\n",
      "Epoch 510/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1851 - val_loss: 0.1836\n",
      "Epoch 511/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1894 - val_loss: 0.1825\n",
      "Epoch 512/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1906 - val_loss: 0.1835\n",
      "Epoch 513/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1888 - val_loss: 0.1844\n",
      "Epoch 514/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1868 - val_loss: 0.1820\n",
      "Epoch 515/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1891 - val_loss: 0.1828\n",
      "Epoch 516/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1861 - val_loss: 0.1828\n",
      "Epoch 517/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1879 - val_loss: 0.1852\n",
      "Epoch 518/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1872 - val_loss: 0.1845\n",
      "Epoch 519/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1920 - val_loss: 0.1830\n",
      "Epoch 520/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1912 - val_loss: 0.1844\n",
      "Epoch 521/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1863 - val_loss: 0.1834\n",
      "Epoch 522/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1864 - val_loss: 0.1860\n",
      "Epoch 523/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1875 - val_loss: 0.1828\n",
      "Epoch 524/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1869 - val_loss: 0.1835\n",
      "Epoch 525/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1883 - val_loss: 0.1819\n",
      "Epoch 526/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1860 - val_loss: 0.1826\n",
      "Epoch 527/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1909 - val_loss: 0.1828\n",
      "Epoch 528/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1855 - val_loss: 0.1830\n",
      "Epoch 529/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1901 - val_loss: 0.1830\n",
      "Epoch 530/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1852 - val_loss: 0.1832\n",
      "Epoch 531/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1919 - val_loss: 0.1842\n",
      "Epoch 532/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1943 - val_loss: 0.1847\n",
      "Epoch 533/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1885 - val_loss: 0.1819\n",
      "Epoch 534/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1894 - val_loss: 0.1821\n",
      "Epoch 535/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1869 - val_loss: 0.1827\n",
      "Epoch 536/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1860 - val_loss: 0.1829\n",
      "Epoch 537/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1863 - val_loss: 0.1828\n",
      "Epoch 538/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1880 - val_loss: 0.1816\n",
      "Epoch 539/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1884 - val_loss: 0.1826\n",
      "Epoch 540/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1850 - val_loss: 0.1850\n",
      "Epoch 541/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1855 - val_loss: 0.1827\n",
      "Epoch 542/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1817\n",
      "Epoch 543/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1829\n",
      "Epoch 544/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1879 - val_loss: 0.1828\n",
      "Epoch 545/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1885 - val_loss: 0.1846\n",
      "Epoch 546/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1868 - val_loss: 0.1834\n",
      "Epoch 547/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1878 - val_loss: 0.1841\n",
      "Epoch 548/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1870 - val_loss: 0.1829\n",
      "Epoch 549/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1876 - val_loss: 0.1848\n",
      "Epoch 550/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1885 - val_loss: 0.1839\n",
      "Epoch 551/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1858 - val_loss: 0.1861\n",
      "Epoch 552/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1885 - val_loss: 0.1845\n",
      "Epoch 553/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1877 - val_loss: 0.1824\n",
      "Epoch 554/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1868 - val_loss: 0.1825\n",
      "Epoch 555/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1875 - val_loss: 0.1822\n",
      "Epoch 556/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1931 - val_loss: 0.1853\n",
      "Epoch 557/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1887 - val_loss: 0.1834\n",
      "Epoch 558/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1886 - val_loss: 0.1821\n",
      "Epoch 559/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1834 - val_loss: 0.1813\n",
      "Epoch 560/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1878 - val_loss: 0.1846\n",
      "Epoch 561/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1882 - val_loss: 0.1845\n",
      "Epoch 562/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1859 - val_loss: 0.1845\n",
      "Epoch 563/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1881 - val_loss: 0.1841\n",
      "Epoch 564/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1915 - val_loss: 0.1823\n",
      "Epoch 565/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1895 - val_loss: 0.1834\n",
      "Epoch 566/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1884 - val_loss: 0.1856\n",
      "Epoch 567/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1880 - val_loss: 0.1822\n",
      "Epoch 568/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1848\n",
      "Epoch 569/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1849 - val_loss: 0.1872\n",
      "Epoch 570/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1862 - val_loss: 0.1815\n",
      "Epoch 571/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1871 - val_loss: 0.1822\n",
      "Epoch 572/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1877 - val_loss: 0.1812\n",
      "Epoch 573/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1901 - val_loss: 0.1837\n",
      "Epoch 574/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1905 - val_loss: 0.1825\n",
      "Epoch 575/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1863 - val_loss: 0.1820\n",
      "Epoch 576/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1897 - val_loss: 0.1839\n",
      "Epoch 577/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1889 - val_loss: 0.1820\n",
      "Epoch 578/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1868 - val_loss: 0.1830\n",
      "Epoch 579/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1862 - val_loss: 0.1816\n",
      "Epoch 580/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1893 - val_loss: 0.1819\n",
      "Epoch 581/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1847 - val_loss: 0.1827\n",
      "Epoch 582/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1853 - val_loss: 0.1819\n",
      "Epoch 583/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1878 - val_loss: 0.1835\n",
      "Epoch 584/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1838 - val_loss: 0.1824\n",
      "Epoch 585/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1907 - val_loss: 0.1828\n",
      "Epoch 586/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1886 - val_loss: 0.1836\n",
      "Epoch 587/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1869 - val_loss: 0.1849\n",
      "Epoch 588/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1893 - val_loss: 0.1866\n",
      "Epoch 589/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1841\n",
      "Epoch 590/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1869 - val_loss: 0.1814\n",
      "Epoch 591/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1821\n",
      "Epoch 592/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1844 - val_loss: 0.1825\n",
      "Epoch 593/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1895 - val_loss: 0.1826\n",
      "Epoch 594/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1892 - val_loss: 0.1839\n",
      "Epoch 595/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1892 - val_loss: 0.1827\n",
      "Epoch 596/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1842 - val_loss: 0.1818\n",
      "Epoch 597/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1872 - val_loss: 0.1842\n",
      "Epoch 598/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1844 - val_loss: 0.1815\n",
      "Epoch 599/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1870 - val_loss: 0.1823\n",
      "Epoch 600/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1863 - val_loss: 0.1823\n",
      "Epoch 601/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1874 - val_loss: 0.1838\n",
      "Epoch 602/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1834 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00602: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 603/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1836 - val_loss: 0.1808\n",
      "Epoch 604/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1803 - val_loss: 0.1796\n",
      "Epoch 605/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1786 - val_loss: 0.1793\n",
      "Epoch 606/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1823 - val_loss: 0.1796\n",
      "Epoch 607/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1831 - val_loss: 0.1793\n",
      "Epoch 608/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1823 - val_loss: 0.1787\n",
      "Epoch 609/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1827 - val_loss: 0.1788\n",
      "Epoch 610/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1831 - val_loss: 0.1784\n",
      "Epoch 611/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1837 - val_loss: 0.1791\n",
      "Epoch 612/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1851 - val_loss: 0.1788\n",
      "Epoch 613/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1845 - val_loss: 0.1783\n",
      "Epoch 614/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1845 - val_loss: 0.1785\n",
      "Epoch 615/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1814 - val_loss: 0.1790\n",
      "Epoch 616/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1775 - val_loss: 0.1788\n",
      "Epoch 617/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1803 - val_loss: 0.1778\n",
      "Epoch 618/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1847 - val_loss: 0.1809\n",
      "Epoch 619/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1810 - val_loss: 0.1795\n",
      "Epoch 620/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1812 - val_loss: 0.1783\n",
      "Epoch 621/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1842 - val_loss: 0.1788\n",
      "Epoch 622/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1805 - val_loss: 0.1780\n",
      "Epoch 623/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1827 - val_loss: 0.1789\n",
      "Epoch 624/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1825 - val_loss: 0.1787\n",
      "Epoch 625/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1801 - val_loss: 0.1785\n",
      "Epoch 626/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1841 - val_loss: 0.1792\n",
      "Epoch 627/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1813 - val_loss: 0.1781\n",
      "Epoch 628/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1803 - val_loss: 0.1797\n",
      "Epoch 629/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1814 - val_loss: 0.1799\n",
      "Epoch 630/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1816 - val_loss: 0.1780\n",
      "Epoch 631/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1825 - val_loss: 0.1789\n",
      "Epoch 632/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1823 - val_loss: 0.1781\n",
      "Epoch 633/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1810 - val_loss: 0.1776\n",
      "Epoch 634/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1818 - val_loss: 0.1786\n",
      "Epoch 635/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1827 - val_loss: 0.1783\n",
      "Epoch 636/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1796 - val_loss: 0.1793\n",
      "Epoch 637/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1808 - val_loss: 0.1780\n",
      "Epoch 638/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1827 - val_loss: 0.1781\n",
      "Epoch 639/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1825 - val_loss: 0.1793\n",
      "Epoch 640/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1830 - val_loss: 0.1783\n",
      "Epoch 641/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1803 - val_loss: 0.1789\n",
      "Epoch 642/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1820 - val_loss: 0.1791\n",
      "Epoch 643/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1815 - val_loss: 0.1791\n",
      "Epoch 644/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1826 - val_loss: 0.1778\n",
      "Epoch 645/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1839 - val_loss: 0.1780\n",
      "Epoch 646/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1850 - val_loss: 0.1786\n",
      "Epoch 647/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1788 - val_loss: 0.1783\n",
      "Epoch 648/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1789 - val_loss: 0.1780\n",
      "Epoch 649/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1831 - val_loss: 0.1789\n",
      "Epoch 650/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1804 - val_loss: 0.1780\n",
      "Epoch 651/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1808 - val_loss: 0.1781\n",
      "Epoch 652/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1812 - val_loss: 0.1780\n",
      "Epoch 653/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1787 - val_loss: 0.1783\n",
      "Epoch 654/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1812 - val_loss: 0.1786\n",
      "Epoch 655/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1833 - val_loss: 0.1792\n",
      "Epoch 656/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1835 - val_loss: 0.1781\n",
      "Epoch 657/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1792 - val_loss: 0.1789\n",
      "Epoch 658/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1808 - val_loss: 0.1777\n",
      "Epoch 659/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1797 - val_loss: 0.1797\n",
      "Epoch 660/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1832 - val_loss: 0.1781\n",
      "Epoch 661/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1800 - val_loss: 0.1776\n",
      "Epoch 662/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1802 - val_loss: 0.1779\n",
      "Epoch 663/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1811 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00663: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 664/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1807 - val_loss: 0.1766\n",
      "Epoch 665/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1778 - val_loss: 0.1759\n",
      "Epoch 666/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1788 - val_loss: 0.1763\n",
      "Epoch 667/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1793 - val_loss: 0.1767\n",
      "Epoch 668/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1804 - val_loss: 0.1762\n",
      "Epoch 669/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1814 - val_loss: 0.1766\n",
      "Epoch 670/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1786 - val_loss: 0.1768\n",
      "Epoch 671/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1790 - val_loss: 0.1767\n",
      "Epoch 672/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1779 - val_loss: 0.1763\n",
      "Epoch 673/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1793 - val_loss: 0.1766\n",
      "Epoch 674/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1791 - val_loss: 0.1766\n",
      "Epoch 675/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1818 - val_loss: 0.1767\n",
      "Epoch 676/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1798 - val_loss: 0.1767\n",
      "Epoch 677/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1793 - val_loss: 0.1764\n",
      "Epoch 678/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1812 - val_loss: 0.1765\n",
      "Epoch 679/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1828 - val_loss: 0.1767\n",
      "Epoch 680/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1811 - val_loss: 0.1763\n",
      "Epoch 681/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1811 - val_loss: 0.1760\n",
      "Epoch 682/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1797 - val_loss: 0.1765\n",
      "Epoch 683/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1783 - val_loss: 0.1760\n",
      "Epoch 684/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1763 - val_loss: 0.1765\n",
      "Epoch 685/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1795 - val_loss: 0.1764\n",
      "Epoch 686/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1781 - val_loss: 0.1760\n",
      "Epoch 687/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1793 - val_loss: 0.1765\n",
      "Epoch 688/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1810 - val_loss: 0.1761\n",
      "Epoch 689/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1773 - val_loss: 0.1767\n",
      "Epoch 690/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1826 - val_loss: 0.1763\n",
      "Epoch 691/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1783 - val_loss: 0.1765\n",
      "Epoch 692/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1761 - val_loss: 0.1766\n",
      "Epoch 693/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1808 - val_loss: 0.1767\n",
      "Epoch 694/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1780 - val_loss: 0.1769\n",
      "Epoch 695/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1795 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00695: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 696/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1806 - val_loss: 0.1754\n",
      "Epoch 697/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1794 - val_loss: 0.1761\n",
      "Epoch 698/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1807 - val_loss: 0.1758\n",
      "Epoch 699/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1789 - val_loss: 0.1756\n",
      "Epoch 700/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1777 - val_loss: 0.1755\n",
      "Epoch 701/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1758 - val_loss: 0.1758\n",
      "Epoch 702/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1785 - val_loss: 0.1754\n",
      "Epoch 703/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1755\n",
      "Epoch 704/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1775 - val_loss: 0.1758\n",
      "Epoch 705/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1777 - val_loss: 0.1756\n",
      "Epoch 706/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1755 - val_loss: 0.1756\n",
      "Epoch 707/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1771 - val_loss: 0.1754\n",
      "Epoch 708/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1781 - val_loss: 0.1754\n",
      "Epoch 709/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1770 - val_loss: 0.1753\n",
      "Epoch 710/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1773 - val_loss: 0.1753\n",
      "Epoch 711/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1750\n",
      "Epoch 712/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1763 - val_loss: 0.1753\n",
      "Epoch 713/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1781 - val_loss: 0.1754\n",
      "Epoch 714/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1789 - val_loss: 0.1752\n",
      "Epoch 715/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1767 - val_loss: 0.1755\n",
      "Epoch 716/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1786 - val_loss: 0.1757\n",
      "Epoch 717/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1790 - val_loss: 0.1759\n",
      "Epoch 718/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1809 - val_loss: 0.1751\n",
      "Epoch 719/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1771 - val_loss: 0.1752\n",
      "Epoch 720/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1766 - val_loss: 0.1754\n",
      "Epoch 721/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1744 - val_loss: 0.1752\n",
      "Epoch 722/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1763 - val_loss: 0.1751\n",
      "Epoch 723/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1753 - val_loss: 0.1753\n",
      "Epoch 724/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1774 - val_loss: 0.1755\n",
      "Epoch 725/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1810 - val_loss: 0.1753\n",
      "Epoch 726/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1794 - val_loss: 0.1753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 727/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1763 - val_loss: 0.1754\n",
      "Epoch 728/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1783 - val_loss: 0.1754\n",
      "Epoch 729/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1790 - val_loss: 0.1763\n",
      "Epoch 730/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1805 - val_loss: 0.1752\n",
      "Epoch 731/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1765 - val_loss: 0.1752\n",
      "Epoch 732/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1792 - val_loss: 0.1750\n",
      "Epoch 733/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1809 - val_loss: 0.1751\n",
      "Epoch 734/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1823 - val_loss: 0.1753\n",
      "Epoch 735/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1765 - val_loss: 0.1754\n",
      "Epoch 736/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1778 - val_loss: 0.1755\n",
      "Epoch 737/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1767 - val_loss: 0.1753\n",
      "Epoch 738/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1784 - val_loss: 0.1750\n",
      "Epoch 739/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1773 - val_loss: 0.1753\n",
      "Epoch 740/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1773 - val_loss: 0.1755\n",
      "Epoch 741/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1787 - val_loss: 0.1756\n",
      "\n",
      "Epoch 00741: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 742/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1786 - val_loss: 0.1755\n",
      "Epoch 743/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1787 - val_loss: 0.1750\n",
      "Epoch 744/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1789 - val_loss: 0.1748\n",
      "Epoch 745/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1757 - val_loss: 0.1753\n",
      "Epoch 746/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1796 - val_loss: 0.1747\n",
      "Epoch 747/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1756 - val_loss: 0.1748\n",
      "Epoch 748/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1771 - val_loss: 0.1750\n",
      "Epoch 749/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1755 - val_loss: 0.1749\n",
      "Epoch 750/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1757 - val_loss: 0.1749\n",
      "Epoch 751/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1772 - val_loss: 0.1751\n",
      "Epoch 752/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1778 - val_loss: 0.1748\n",
      "Epoch 753/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1736 - val_loss: 0.1750\n",
      "Epoch 754/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1753 - val_loss: 0.1751\n",
      "Epoch 755/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1785 - val_loss: 0.1749\n",
      "Epoch 756/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1806 - val_loss: 0.1750\n",
      "Epoch 757/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1756 - val_loss: 0.1750\n",
      "Epoch 758/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1770 - val_loss: 0.1750\n",
      "Epoch 759/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1766 - val_loss: 0.1746\n",
      "Epoch 760/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1762 - val_loss: 0.1748\n",
      "Epoch 761/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1793 - val_loss: 0.1749\n",
      "Epoch 762/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1752 - val_loss: 0.1749\n",
      "Epoch 763/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1761 - val_loss: 0.1751\n",
      "Epoch 764/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1776 - val_loss: 0.1749\n",
      "Epoch 765/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1775 - val_loss: 0.1748\n",
      "Epoch 766/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1751 - val_loss: 0.1750\n",
      "Epoch 767/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1760 - val_loss: 0.1750\n",
      "Epoch 768/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1750\n",
      "Epoch 769/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1786 - val_loss: 0.1748\n",
      "Epoch 770/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1780 - val_loss: 0.1749\n",
      "Epoch 771/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1786 - val_loss: 0.1748\n",
      "Epoch 772/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1746 - val_loss: 0.1749\n",
      "Epoch 773/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1767 - val_loss: 0.1749\n",
      "Epoch 774/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1749\n",
      "Epoch 775/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1742 - val_loss: 0.1747\n",
      "Epoch 776/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1770 - val_loss: 0.1749\n",
      "Epoch 777/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1745 - val_loss: 0.1749\n",
      "Epoch 778/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1733 - val_loss: 0.1751\n",
      "Epoch 779/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1799 - val_loss: 0.1748\n",
      "Epoch 780/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1780 - val_loss: 0.1748\n",
      "Epoch 781/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1748\n",
      "Epoch 782/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1751 - val_loss: 0.1748\n",
      "Epoch 783/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1768 - val_loss: 0.1748\n",
      "Epoch 784/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1760 - val_loss: 0.1748\n",
      "Epoch 785/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1752 - val_loss: 0.1748\n",
      "Epoch 786/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1782 - val_loss: 0.1748\n",
      "Epoch 787/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1783 - val_loss: 0.1750\n",
      "Epoch 788/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1749\n",
      "Epoch 789/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1742 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00789: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 790/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1759 - val_loss: 0.1749\n",
      "Epoch 791/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1751 - val_loss: 0.1750\n",
      "Epoch 792/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1738 - val_loss: 0.1747\n",
      "Epoch 793/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1760 - val_loss: 0.1747\n",
      "Epoch 794/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1782 - val_loss: 0.1753\n",
      "Epoch 795/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1791 - val_loss: 0.1746\n",
      "Epoch 796/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1775 - val_loss: 0.1747\n",
      "Epoch 797/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1752 - val_loss: 0.1747\n",
      "Epoch 798/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1785 - val_loss: 0.1747\n",
      "Epoch 799/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1772 - val_loss: 0.1746\n",
      "Epoch 800/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1766 - val_loss: 0.1748\n",
      "Epoch 801/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1790 - val_loss: 0.1747\n",
      "Epoch 802/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1747 - val_loss: 0.1746\n",
      "Epoch 803/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1773 - val_loss: 0.1747\n",
      "Epoch 804/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1775 - val_loss: 0.1747\n",
      "Epoch 805/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1731 - val_loss: 0.1746\n",
      "Epoch 806/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1782 - val_loss: 0.1752\n",
      "Epoch 807/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1750 - val_loss: 0.1746\n",
      "Epoch 808/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1764 - val_loss: 0.1746\n",
      "Epoch 809/2000\n",
      "911004/911004 [==============================] - 10s 11us/step - loss: 0.1785 - val_loss: 0.1747\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00809: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA882Dtyq8kPLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7450731183483659\n",
      "Training 3JHH out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 472508 samples, validate on 118103 samples\n",
      "Epoch 1/2000\n",
      "472508/472508 [==============================] - 6s 12us/step - loss: 1.2565 - val_loss: 0.5356\n",
      "Epoch 2/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.4758 - val_loss: 0.4181\n",
      "Epoch 3/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.4129 - val_loss: 0.3759\n",
      "Epoch 4/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3862 - val_loss: 0.3823\n",
      "Epoch 5/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3630 - val_loss: 0.3309\n",
      "Epoch 6/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3484 - val_loss: 0.3294\n",
      "Epoch 7/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3373 - val_loss: 0.3521\n",
      "Epoch 8/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3301 - val_loss: 0.2984\n",
      "Epoch 9/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3244 - val_loss: 0.3157\n",
      "Epoch 10/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3151 - val_loss: 0.3190\n",
      "Epoch 11/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3102 - val_loss: 0.3002\n",
      "Epoch 12/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.3051 - val_loss: 0.3033\n",
      "Epoch 13/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2944 - val_loss: 0.2577\n",
      "Epoch 14/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2902 - val_loss: 0.2720\n",
      "Epoch 15/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2845 - val_loss: 0.2811\n",
      "Epoch 16/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2817 - val_loss: 0.2857\n",
      "Epoch 17/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2769 - val_loss: 0.2715\n",
      "Epoch 18/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2740 - val_loss: 0.2567\n",
      "Epoch 19/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2725 - val_loss: 0.2418\n",
      "Epoch 20/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2708 - val_loss: 0.2410\n",
      "Epoch 21/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2677 - val_loss: 0.2413\n",
      "Epoch 22/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2604 - val_loss: 0.2400\n",
      "Epoch 23/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2630 - val_loss: 0.2462\n",
      "Epoch 24/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2575 - val_loss: 0.2267\n",
      "Epoch 25/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2552 - val_loss: 0.2323\n",
      "Epoch 26/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2505 - val_loss: 0.2288\n",
      "Epoch 27/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2501 - val_loss: 0.2268\n",
      "Epoch 28/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2469 - val_loss: 0.2283\n",
      "Epoch 29/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2486 - val_loss: 0.2436\n",
      "Epoch 30/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2442 - val_loss: 0.2216\n",
      "Epoch 31/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2439 - val_loss: 0.2213\n",
      "Epoch 32/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2389 - val_loss: 0.2124\n",
      "Epoch 33/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2387 - val_loss: 0.2084\n",
      "Epoch 34/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2363 - val_loss: 0.2390\n",
      "Epoch 35/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2334 - val_loss: 0.2246\n",
      "Epoch 36/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2316 - val_loss: 0.2148\n",
      "Epoch 37/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2316 - val_loss: 0.2077\n",
      "Epoch 38/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2313 - val_loss: 0.2186\n",
      "Epoch 39/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2298 - val_loss: 0.2078\n",
      "Epoch 40/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2289 - val_loss: 0.2016\n",
      "Epoch 41/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2251 - val_loss: 0.2204\n",
      "Epoch 42/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2264 - val_loss: 0.2065\n",
      "Epoch 43/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2248 - val_loss: 0.2083\n",
      "Epoch 44/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2224 - val_loss: 0.2046\n",
      "Epoch 45/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2233 - val_loss: 0.2269\n",
      "Epoch 46/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2201 - val_loss: 0.2150\n",
      "Epoch 47/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2223 - val_loss: 0.2030\n",
      "Epoch 48/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2280 - val_loss: 0.2098\n",
      "Epoch 49/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2171 - val_loss: 0.2122\n",
      "Epoch 50/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2205 - val_loss: 0.2044\n",
      "Epoch 51/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2230 - val_loss: 0.2115\n",
      "Epoch 52/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2167 - val_loss: 0.2210\n",
      "Epoch 53/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2139 - val_loss: 0.1941\n",
      "Epoch 54/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2127 - val_loss: 0.2027\n",
      "Epoch 55/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2110 - val_loss: 0.1967\n",
      "Epoch 56/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2130 - val_loss: 0.2107\n",
      "Epoch 57/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2104 - val_loss: 0.2285\n",
      "Epoch 58/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2068 - val_loss: 0.2149\n",
      "Epoch 59/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2079 - val_loss: 0.2073\n",
      "Epoch 60/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2093 - val_loss: 0.2205\n",
      "Epoch 61/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2119 - val_loss: 0.1955\n",
      "Epoch 62/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2065 - val_loss: 0.1892\n",
      "Epoch 63/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2043 - val_loss: 0.1888\n",
      "Epoch 64/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2059 - val_loss: 0.1900\n",
      "Epoch 65/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2053 - val_loss: 0.2010\n",
      "Epoch 66/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2100 - val_loss: 0.1885\n",
      "Epoch 67/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2050 - val_loss: 0.1839\n",
      "Epoch 68/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2081 - val_loss: 0.1920\n",
      "Epoch 69/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1997 - val_loss: 0.1874\n",
      "Epoch 70/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2027 - val_loss: 0.1946\n",
      "Epoch 71/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1969 - val_loss: 0.1888\n",
      "Epoch 72/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2033 - val_loss: 0.1824\n",
      "Epoch 73/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2035 - val_loss: 0.1825\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1990 - val_loss: 0.1921\n",
      "Epoch 75/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2018 - val_loss: 0.1785\n",
      "Epoch 76/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1992 - val_loss: 0.1739\n",
      "Epoch 77/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.2019 - val_loss: 0.1894\n",
      "Epoch 78/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1969 - val_loss: 0.1808\n",
      "Epoch 79/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1986 - val_loss: 0.1801\n",
      "Epoch 80/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1977 - val_loss: 0.1905\n",
      "Epoch 81/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1960 - val_loss: 0.1749\n",
      "Epoch 82/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1969 - val_loss: 0.1804\n",
      "Epoch 83/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1973 - val_loss: 0.1875\n",
      "Epoch 84/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1947 - val_loss: 0.1788\n",
      "Epoch 85/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1933 - val_loss: 0.1881\n",
      "Epoch 86/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1974 - val_loss: 0.1820\n",
      "Epoch 87/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1921 - val_loss: 0.1895\n",
      "Epoch 88/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1931 - val_loss: 0.1822\n",
      "Epoch 89/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1938 - val_loss: 0.1795\n",
      "Epoch 90/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1940 - val_loss: 0.2160\n",
      "Epoch 91/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1921 - val_loss: 0.1693\n",
      "Epoch 92/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1948 - val_loss: 0.1903\n",
      "Epoch 93/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1921 - val_loss: 0.1706\n",
      "Epoch 94/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1911 - val_loss: 0.1792\n",
      "Epoch 95/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1918 - val_loss: 0.1827\n",
      "Epoch 96/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1922 - val_loss: 0.1735\n",
      "Epoch 97/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1913 - val_loss: 0.1681\n",
      "Epoch 98/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1874 - val_loss: 0.1724\n",
      "Epoch 99/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1904 - val_loss: 0.1743\n",
      "Epoch 100/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1872 - val_loss: 0.1763\n",
      "Epoch 101/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1878 - val_loss: 0.1679\n",
      "Epoch 102/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1891 - val_loss: 0.1996\n",
      "Epoch 103/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1841 - val_loss: 0.1726\n",
      "Epoch 104/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1860 - val_loss: 0.1734\n",
      "Epoch 105/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1871 - val_loss: 0.1659\n",
      "Epoch 106/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1876 - val_loss: 0.1744\n",
      "Epoch 107/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1860 - val_loss: 0.1733\n",
      "Epoch 108/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1857 - val_loss: 0.1723\n",
      "Epoch 109/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1859 - val_loss: 0.1815\n",
      "Epoch 110/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1842 - val_loss: 0.1803\n",
      "Epoch 111/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1857 - val_loss: 0.1675\n",
      "Epoch 112/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1823 - val_loss: 0.1724\n",
      "Epoch 113/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1821 - val_loss: 0.1725\n",
      "Epoch 114/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1844 - val_loss: 0.1680\n",
      "Epoch 115/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1820 - val_loss: 0.1669\n",
      "Epoch 116/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1875 - val_loss: 0.1715\n",
      "Epoch 117/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1831 - val_loss: 0.1692\n",
      "Epoch 118/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1791 - val_loss: 0.1647\n",
      "Epoch 119/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1866 - val_loss: 0.1810\n",
      "Epoch 120/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1856 - val_loss: 0.1601\n",
      "Epoch 121/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1810 - val_loss: 0.1795\n",
      "Epoch 122/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1825 - val_loss: 0.1668\n",
      "Epoch 123/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1812 - val_loss: 0.1889\n",
      "Epoch 124/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1813 - val_loss: 0.1669\n",
      "Epoch 125/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1808 - val_loss: 0.1775\n",
      "Epoch 126/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1813 - val_loss: 0.1746\n",
      "Epoch 127/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1821 - val_loss: 0.1601\n",
      "Epoch 128/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1811 - val_loss: 0.1668\n",
      "Epoch 129/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1776 - val_loss: 0.1682\n",
      "Epoch 130/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1813 - val_loss: 0.1676\n",
      "Epoch 131/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1814 - val_loss: 0.1690\n",
      "Epoch 132/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1783 - val_loss: 0.1616\n",
      "Epoch 133/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1854 - val_loss: 0.1647\n",
      "Epoch 134/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1787 - val_loss: 0.1683\n",
      "Epoch 135/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1768 - val_loss: 0.1644\n",
      "Epoch 136/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1768 - val_loss: 0.1711\n",
      "Epoch 137/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1815 - val_loss: 0.1614\n",
      "Epoch 138/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1790 - val_loss: 0.1638\n",
      "Epoch 139/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1775 - val_loss: 0.1631\n",
      "Epoch 140/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1771 - val_loss: 0.1601\n",
      "Epoch 141/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1783 - val_loss: 0.1693\n",
      "Epoch 142/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1757 - val_loss: 0.1794\n",
      "Epoch 143/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1768 - val_loss: 0.1654\n",
      "Epoch 144/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1777 - val_loss: 0.2081\n",
      "Epoch 145/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1758 - val_loss: 0.1595\n",
      "Epoch 146/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1750 - val_loss: 0.1781\n",
      "Epoch 147/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1778 - val_loss: 0.1591\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1767 - val_loss: 0.1659\n",
      "Epoch 149/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1755 - val_loss: 0.1708\n",
      "Epoch 150/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1812 - val_loss: 0.1613\n",
      "Epoch 151/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1761 - val_loss: 0.1813\n",
      "Epoch 152/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1715 - val_loss: 0.1678\n",
      "Epoch 153/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1822 - val_loss: 0.1570\n",
      "Epoch 154/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1812 - val_loss: 0.1640\n",
      "Epoch 155/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1722 - val_loss: 0.1673\n",
      "Epoch 156/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1781 - val_loss: 0.1870\n",
      "Epoch 157/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1725 - val_loss: 0.1808\n",
      "Epoch 158/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1754 - val_loss: 0.1752\n",
      "Epoch 159/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1710 - val_loss: 0.1521\n",
      "Epoch 160/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1735 - val_loss: 0.1804\n",
      "Epoch 161/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1737 - val_loss: 0.1634\n",
      "Epoch 162/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1749 - val_loss: 0.1644\n",
      "Epoch 163/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1741 - val_loss: 0.1673\n",
      "Epoch 164/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1721 - val_loss: 0.1743\n",
      "Epoch 165/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1732 - val_loss: 0.1764\n",
      "Epoch 166/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1718 - val_loss: 0.1738\n",
      "Epoch 167/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1748 - val_loss: 0.1719\n",
      "Epoch 168/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1745 - val_loss: 0.1644\n",
      "Epoch 169/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1731 - val_loss: 0.1653\n",
      "Epoch 170/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1701 - val_loss: 0.1612\n",
      "Epoch 171/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1755 - val_loss: 0.1729\n",
      "Epoch 172/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1751 - val_loss: 0.1534\n",
      "Epoch 173/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1763 - val_loss: 0.1648\n",
      "Epoch 174/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1741 - val_loss: 0.1541\n",
      "Epoch 175/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1691 - val_loss: 0.1608\n",
      "Epoch 176/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1711 - val_loss: 0.1582\n",
      "Epoch 177/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1711 - val_loss: 0.1515\n",
      "Epoch 178/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1706 - val_loss: 0.1529\n",
      "Epoch 179/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1672 - val_loss: 0.1581\n",
      "Epoch 180/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1696 - val_loss: 0.1626\n",
      "Epoch 181/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1711 - val_loss: 0.1519\n",
      "Epoch 182/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1727 - val_loss: 0.1603\n",
      "Epoch 183/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1716 - val_loss: 0.1636\n",
      "Epoch 184/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1705 - val_loss: 0.1536\n",
      "Epoch 185/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1693 - val_loss: 0.1600\n",
      "Epoch 186/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1668 - val_loss: 0.1683\n",
      "Epoch 187/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1725 - val_loss: 0.1666\n",
      "Epoch 188/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1643 - val_loss: 0.1566\n",
      "Epoch 189/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1643 - val_loss: 0.1559\n",
      "Epoch 190/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1675 - val_loss: 0.1611\n",
      "Epoch 191/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1693 - val_loss: 0.1931\n",
      "Epoch 192/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1688 - val_loss: 0.1761\n",
      "Epoch 193/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1690 - val_loss: 0.1498\n",
      "Epoch 194/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1695 - val_loss: 0.1626\n",
      "Epoch 195/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1637 - val_loss: 0.1575\n",
      "Epoch 196/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1699 - val_loss: 0.1638\n",
      "Epoch 197/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1686 - val_loss: 0.1787\n",
      "Epoch 198/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1725 - val_loss: 0.1512\n",
      "Epoch 199/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1672 - val_loss: 0.1634\n",
      "Epoch 200/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1644 - val_loss: 0.1737\n",
      "Epoch 201/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1670 - val_loss: 0.1556\n",
      "Epoch 202/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1674 - val_loss: 0.1548\n",
      "Epoch 203/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1677 - val_loss: 0.1684\n",
      "Epoch 204/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1713 - val_loss: 0.1666\n",
      "Epoch 205/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1645 - val_loss: 0.1501\n",
      "Epoch 206/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1704 - val_loss: 0.1550\n",
      "Epoch 207/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1693 - val_loss: 0.1598\n",
      "Epoch 208/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1632 - val_loss: 0.1486\n",
      "Epoch 209/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1674 - val_loss: 0.1767\n",
      "Epoch 210/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1651 - val_loss: 0.1512\n",
      "Epoch 211/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1626 - val_loss: 0.1478\n",
      "Epoch 212/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1657 - val_loss: 0.1507\n",
      "Epoch 213/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1675 - val_loss: 0.1501\n",
      "Epoch 214/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1664 - val_loss: 0.1493\n",
      "Epoch 215/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1680 - val_loss: 0.1895\n",
      "Epoch 216/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1639 - val_loss: 0.1522\n",
      "Epoch 217/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1703 - val_loss: 0.2216\n",
      "Epoch 218/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1665 - val_loss: 0.1652\n",
      "Epoch 219/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1667 - val_loss: 0.1533\n",
      "Epoch 220/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1677 - val_loss: 0.1648\n",
      "Epoch 221/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1637 - val_loss: 0.1541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1692 - val_loss: 0.1627\n",
      "Epoch 223/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1647 - val_loss: 0.1512\n",
      "Epoch 224/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1617 - val_loss: 0.1575\n",
      "Epoch 225/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1660 - val_loss: 0.1816\n",
      "Epoch 226/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1692 - val_loss: 0.1481\n",
      "Epoch 227/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1617 - val_loss: 0.1582\n",
      "Epoch 228/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1638 - val_loss: 0.1542\n",
      "Epoch 229/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1619 - val_loss: 0.1674\n",
      "Epoch 230/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1643 - val_loss: 0.1504\n",
      "Epoch 231/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1624 - val_loss: 0.1503\n",
      "Epoch 232/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1611 - val_loss: 0.1687\n",
      "Epoch 233/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1610 - val_loss: 0.1528\n",
      "Epoch 234/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1655 - val_loss: 0.1470\n",
      "Epoch 235/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1630 - val_loss: 0.1477\n",
      "Epoch 236/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1611 - val_loss: 0.1516\n",
      "Epoch 237/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1655 - val_loss: 0.1489\n",
      "Epoch 238/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1628 - val_loss: 0.1455\n",
      "Epoch 239/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1614 - val_loss: 0.1518\n",
      "Epoch 240/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1673 - val_loss: 0.1501\n",
      "Epoch 241/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1661 - val_loss: 0.1564\n",
      "Epoch 242/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1653 - val_loss: 0.1438\n",
      "Epoch 243/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1652 - val_loss: 0.1523\n",
      "Epoch 244/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1609 - val_loss: 0.1581\n",
      "Epoch 245/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1625 - val_loss: 0.1460\n",
      "Epoch 246/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1625 - val_loss: 0.1471\n",
      "Epoch 247/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1601 - val_loss: 0.1539\n",
      "Epoch 248/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1607 - val_loss: 0.1468\n",
      "Epoch 249/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1649 - val_loss: 0.1592\n",
      "Epoch 250/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1657 - val_loss: 0.1671\n",
      "Epoch 251/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1603 - val_loss: 0.1599\n",
      "Epoch 252/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1662 - val_loss: 0.1486\n",
      "Epoch 253/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1646 - val_loss: 0.1476\n",
      "Epoch 254/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1592 - val_loss: 0.1576\n",
      "Epoch 255/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1616 - val_loss: 0.1549\n",
      "Epoch 256/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1583 - val_loss: 0.1561\n",
      "Epoch 257/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1629 - val_loss: 0.1535\n",
      "Epoch 258/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1615 - val_loss: 0.1641\n",
      "Epoch 259/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1601 - val_loss: 0.1521\n",
      "Epoch 260/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1612 - val_loss: 0.1502\n",
      "Epoch 261/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1554 - val_loss: 0.1505\n",
      "Epoch 262/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1591 - val_loss: 0.1450\n",
      "Epoch 263/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1634 - val_loss: 0.1646\n",
      "Epoch 264/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1557 - val_loss: 0.1732\n",
      "Epoch 265/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1574 - val_loss: 0.1639\n",
      "Epoch 266/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1618 - val_loss: 0.1495\n",
      "Epoch 267/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1634 - val_loss: 0.1556\n",
      "Epoch 268/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1574 - val_loss: 0.1642\n",
      "Epoch 269/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1588 - val_loss: 0.1481\n",
      "Epoch 270/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1578 - val_loss: 0.1446\n",
      "Epoch 271/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1605 - val_loss: 0.1727\n",
      "Epoch 272/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1607 - val_loss: 0.1648\n",
      "\n",
      "Epoch 00272: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 273/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1525 - val_loss: 0.1424\n",
      "Epoch 274/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1488 - val_loss: 0.1610\n",
      "Epoch 275/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1478 - val_loss: 0.1400\n",
      "Epoch 276/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1470 - val_loss: 0.1343\n",
      "Epoch 277/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1479 - val_loss: 0.1453\n",
      "Epoch 278/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1499 - val_loss: 0.1344\n",
      "Epoch 279/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1502 - val_loss: 0.1452\n",
      "Epoch 280/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1483 - val_loss: 0.1454\n",
      "Epoch 281/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1416 - val_loss: 0.1569\n",
      "Epoch 282/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1516 - val_loss: 0.1585\n",
      "Epoch 283/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1520 - val_loss: 0.1428\n",
      "Epoch 284/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1474 - val_loss: 0.1477\n",
      "Epoch 285/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1497 - val_loss: 0.1328\n",
      "Epoch 286/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1482 - val_loss: 0.1387\n",
      "Epoch 287/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1471 - val_loss: 0.1460\n",
      "Epoch 288/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1457 - val_loss: 0.1426\n",
      "Epoch 289/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1467 - val_loss: 0.1369\n",
      "Epoch 290/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1487 - val_loss: 0.1606\n",
      "Epoch 291/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1445 - val_loss: 0.1386\n",
      "Epoch 292/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1499 - val_loss: 0.1346\n",
      "Epoch 293/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1471 - val_loss: 0.1371\n",
      "Epoch 294/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1448 - val_loss: 0.1351\n",
      "Epoch 295/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1466 - val_loss: 0.1341\n",
      "Epoch 296/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1488 - val_loss: 0.1361\n",
      "Epoch 297/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1454 - val_loss: 0.1374\n",
      "Epoch 298/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1453 - val_loss: 0.1351\n",
      "Epoch 299/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1490 - val_loss: 0.1427\n",
      "Epoch 300/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1478 - val_loss: 0.1381\n",
      "Epoch 301/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1457 - val_loss: 0.1381\n",
      "Epoch 302/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1454 - val_loss: 0.1351\n",
      "Epoch 303/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1450 - val_loss: 0.1701\n",
      "Epoch 304/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1476 - val_loss: 0.1610\n",
      "Epoch 305/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1493 - val_loss: 0.1387\n",
      "Epoch 306/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1432 - val_loss: 0.1333\n",
      "Epoch 307/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1448 - val_loss: 0.1397\n",
      "Epoch 308/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1494 - val_loss: 0.1596\n",
      "Epoch 309/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1453 - val_loss: 0.1365\n",
      "Epoch 310/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1488 - val_loss: 0.1360\n",
      "Epoch 311/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1461 - val_loss: 0.1349\n",
      "Epoch 312/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1458 - val_loss: 0.1367\n",
      "Epoch 313/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1469 - val_loss: 0.1360\n",
      "Epoch 314/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1443 - val_loss: 0.1359\n",
      "Epoch 315/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1432 - val_loss: 0.1564\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 316/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1427 - val_loss: 0.1286\n",
      "Epoch 317/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1417 - val_loss: 0.1279\n",
      "Epoch 318/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1385 - val_loss: 0.1296\n",
      "Epoch 319/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1395 - val_loss: 0.1300\n",
      "Epoch 320/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1435 - val_loss: 0.1328\n",
      "Epoch 321/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1409 - val_loss: 0.1293\n",
      "Epoch 322/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1401 - val_loss: 0.1277\n",
      "Epoch 323/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1396 - val_loss: 0.1293\n",
      "Epoch 324/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1398 - val_loss: 0.1277\n",
      "Epoch 325/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1368 - val_loss: 0.1284\n",
      "Epoch 326/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1394 - val_loss: 0.1341\n",
      "Epoch 327/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1420 - val_loss: 0.1288\n",
      "Epoch 328/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1390 - val_loss: 0.1362\n",
      "Epoch 329/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1387 - val_loss: 0.1273\n",
      "Epoch 330/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1368 - val_loss: 0.1282\n",
      "Epoch 331/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1390 - val_loss: 0.1278\n",
      "Epoch 332/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1387 - val_loss: 0.1356\n",
      "Epoch 333/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1413 - val_loss: 0.1340\n",
      "Epoch 334/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1422 - val_loss: 0.1310\n",
      "Epoch 335/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1427 - val_loss: 0.1261\n",
      "Epoch 336/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1379 - val_loss: 0.1287\n",
      "Epoch 337/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1375 - val_loss: 0.1328\n",
      "Epoch 338/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1422 - val_loss: 0.1276\n",
      "Epoch 339/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1349 - val_loss: 0.1324\n",
      "Epoch 340/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1403 - val_loss: 0.1386\n",
      "Epoch 341/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1459 - val_loss: 0.1296\n",
      "Epoch 342/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1381 - val_loss: 0.1341\n",
      "Epoch 343/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1381 - val_loss: 0.1349\n",
      "Epoch 344/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1428 - val_loss: 0.1289\n",
      "Epoch 345/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1408 - val_loss: 0.1274\n",
      "Epoch 346/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1401 - val_loss: 0.1286\n",
      "Epoch 347/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1401 - val_loss: 0.1275\n",
      "Epoch 348/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1385 - val_loss: 0.1277\n",
      "Epoch 349/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1371 - val_loss: 0.1271\n",
      "Epoch 350/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1396 - val_loss: 0.1286\n",
      "Epoch 351/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1394 - val_loss: 0.1333\n",
      "Epoch 352/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1355 - val_loss: 0.1304\n",
      "Epoch 353/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1414 - val_loss: 0.1274\n",
      "Epoch 354/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1393 - val_loss: 0.1271\n",
      "Epoch 355/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1429 - val_loss: 0.1286\n",
      "Epoch 356/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1387 - val_loss: 0.1281\n",
      "Epoch 357/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1382 - val_loss: 0.1317\n",
      "Epoch 358/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1422 - val_loss: 0.1274\n",
      "Epoch 359/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1415 - val_loss: 0.1287\n",
      "Epoch 360/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1450 - val_loss: 0.1324\n",
      "Epoch 361/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1423 - val_loss: 0.1270\n",
      "Epoch 362/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1375 - val_loss: 0.1302\n",
      "Epoch 363/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1367 - val_loss: 0.1268\n",
      "Epoch 364/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1435 - val_loss: 0.1320\n",
      "Epoch 365/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1385 - val_loss: 0.1280\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 366/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1342 - val_loss: 0.1245\n",
      "Epoch 367/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1341 - val_loss: 0.1253\n",
      "Epoch 368/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1336 - val_loss: 0.1257\n",
      "Epoch 369/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1335 - val_loss: 0.1247\n",
      "Epoch 370/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1357 - val_loss: 0.1278\n",
      "Epoch 371/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1378 - val_loss: 0.1264\n",
      "Epoch 372/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1348 - val_loss: 0.1248\n",
      "Epoch 373/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1364 - val_loss: 0.1244\n",
      "Epoch 374/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1306 - val_loss: 0.1260\n",
      "Epoch 375/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1352 - val_loss: 0.1246\n",
      "Epoch 376/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1374 - val_loss: 0.1240\n",
      "Epoch 377/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1329 - val_loss: 0.1244\n",
      "Epoch 378/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1342 - val_loss: 0.1262\n",
      "Epoch 379/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1391 - val_loss: 0.1246\n",
      "Epoch 380/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1352 - val_loss: 0.1264\n",
      "Epoch 381/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1346 - val_loss: 0.1249\n",
      "Epoch 382/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1352 - val_loss: 0.1243\n",
      "Epoch 383/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1372 - val_loss: 0.1255\n",
      "Epoch 384/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1356 - val_loss: 0.1252\n",
      "Epoch 385/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1386 - val_loss: 0.1243\n",
      "Epoch 386/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1363 - val_loss: 0.1241\n",
      "Epoch 387/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1345 - val_loss: 0.1258\n",
      "Epoch 388/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1376 - val_loss: 0.1246\n",
      "Epoch 389/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1372 - val_loss: 0.1279\n",
      "Epoch 390/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1391 - val_loss: 0.1266\n",
      "Epoch 391/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1335 - val_loss: 0.1258\n",
      "Epoch 392/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1343 - val_loss: 0.1293\n",
      "Epoch 393/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1380 - val_loss: 0.1275\n",
      "Epoch 394/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1392 - val_loss: 0.1255\n",
      "Epoch 395/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1354 - val_loss: 0.1242\n",
      "Epoch 396/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1377 - val_loss: 0.1249\n",
      "Epoch 397/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1321 - val_loss: 0.1266\n",
      "Epoch 398/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1363 - val_loss: 0.1240\n",
      "Epoch 399/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1379 - val_loss: 0.1265\n",
      "Epoch 400/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1291 - val_loss: 0.1245\n",
      "Epoch 401/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1336 - val_loss: 0.1265\n",
      "Epoch 402/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1399 - val_loss: 0.1277\n",
      "Epoch 403/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1354 - val_loss: 0.1261\n",
      "Epoch 404/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1373 - val_loss: 0.1244\n",
      "Epoch 405/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1358 - val_loss: 0.1333\n",
      "Epoch 406/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1376 - val_loss: 0.1241\n",
      "\n",
      "Epoch 00406: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 407/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1323 - val_loss: 0.1236\n",
      "Epoch 408/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1343 - val_loss: 0.1231\n",
      "Epoch 409/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1232\n",
      "Epoch 410/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1312 - val_loss: 0.1235\n",
      "Epoch 411/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1364 - val_loss: 0.1238\n",
      "Epoch 412/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1330 - val_loss: 0.1232\n",
      "Epoch 413/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1232\n",
      "Epoch 414/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1356 - val_loss: 0.1231\n",
      "Epoch 415/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1321 - val_loss: 0.1234\n",
      "Epoch 416/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1355 - val_loss: 0.1240\n",
      "Epoch 417/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1371 - val_loss: 0.1229\n",
      "Epoch 418/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1370 - val_loss: 0.1231\n",
      "Epoch 419/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1367 - val_loss: 0.1238\n",
      "Epoch 420/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1341 - val_loss: 0.1230\n",
      "Epoch 421/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1330 - val_loss: 0.1237\n",
      "Epoch 422/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1365 - val_loss: 0.1231\n",
      "Epoch 423/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1322 - val_loss: 0.1237\n",
      "Epoch 424/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1382 - val_loss: 0.1234\n",
      "Epoch 425/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1360 - val_loss: 0.1234\n",
      "Epoch 426/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1294 - val_loss: 0.1234\n",
      "Epoch 427/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1357 - val_loss: 0.1231\n",
      "Epoch 428/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1366 - val_loss: 0.1239\n",
      "Epoch 429/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1395 - val_loss: 0.1228\n",
      "Epoch 430/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1342 - val_loss: 0.1235\n",
      "Epoch 431/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1302 - val_loss: 0.1232\n",
      "Epoch 432/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1302 - val_loss: 0.1233\n",
      "Epoch 433/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1300 - val_loss: 0.1242\n",
      "Epoch 434/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1352 - val_loss: 0.1230\n",
      "Epoch 435/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1347 - val_loss: 0.1233\n",
      "Epoch 436/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1239\n",
      "Epoch 437/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1230\n",
      "Epoch 438/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1354 - val_loss: 0.1231\n",
      "Epoch 439/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1299 - val_loss: 0.1231\n",
      "Epoch 440/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1336 - val_loss: 0.1228\n",
      "Epoch 441/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1311 - val_loss: 0.1235\n",
      "Epoch 442/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1317 - val_loss: 0.1228\n",
      "Epoch 443/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1339 - val_loss: 0.1228\n",
      "Epoch 444/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1373 - val_loss: 0.1227\n",
      "Epoch 445/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1346 - val_loss: 0.1230\n",
      "Epoch 446/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1356 - val_loss: 0.1229\n",
      "Epoch 447/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1313 - val_loss: 0.1227\n",
      "Epoch 448/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1346 - val_loss: 0.1241\n",
      "Epoch 449/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1325 - val_loss: 0.1227\n",
      "Epoch 450/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1333 - val_loss: 0.1232\n",
      "Epoch 451/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1323 - val_loss: 0.1228\n",
      "Epoch 452/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1309 - val_loss: 0.1233\n",
      "Epoch 453/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1233\n",
      "Epoch 454/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1381 - val_loss: 0.1231\n",
      "Epoch 455/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1326 - val_loss: 0.1230\n",
      "Epoch 456/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1321 - val_loss: 0.1226\n",
      "Epoch 457/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1335 - val_loss: 0.1244\n",
      "Epoch 458/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1335 - val_loss: 0.1228\n",
      "Epoch 459/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1353 - val_loss: 0.1228\n",
      "Epoch 460/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1293 - val_loss: 0.1234\n",
      "Epoch 461/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1337 - val_loss: 0.1243\n",
      "Epoch 462/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1343 - val_loss: 0.1231\n",
      "Epoch 463/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1322 - val_loss: 0.1228\n",
      "Epoch 464/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1277 - val_loss: 0.1227\n",
      "Epoch 465/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1324 - val_loss: 0.1227\n",
      "Epoch 466/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1327 - val_loss: 0.1234\n",
      "Epoch 467/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1343 - val_loss: 0.1226\n",
      "Epoch 468/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1348 - val_loss: 0.1235\n",
      "Epoch 469/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1341 - val_loss: 0.1231\n",
      "Epoch 470/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1325 - val_loss: 0.1234\n",
      "Epoch 471/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1364 - val_loss: 0.1230\n",
      "Epoch 472/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1320 - val_loss: 0.1236\n",
      "Epoch 473/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1372 - val_loss: 0.1232\n",
      "Epoch 474/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1351 - val_loss: 0.1229\n",
      "Epoch 475/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1231\n",
      "Epoch 476/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1303 - val_loss: 0.1230\n",
      "Epoch 477/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1381 - val_loss: 0.1231\n",
      "Epoch 478/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1345 - val_loss: 0.1226\n",
      "Epoch 479/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1303 - val_loss: 0.1234\n",
      "Epoch 480/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1337 - val_loss: 0.1232\n",
      "Epoch 481/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1228\n",
      "Epoch 482/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1305 - val_loss: 0.1229\n",
      "Epoch 483/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1368 - val_loss: 0.1231\n",
      "Epoch 484/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1313 - val_loss: 0.1236\n",
      "Epoch 485/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1314 - val_loss: 0.1233\n",
      "Epoch 486/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1322 - val_loss: 0.1234\n",
      "\n",
      "Epoch 00486: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 487/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1225\n",
      "Epoch 488/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1347 - val_loss: 0.1222\n",
      "Epoch 489/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1316 - val_loss: 0.1223\n",
      "Epoch 490/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1330 - val_loss: 0.1225\n",
      "Epoch 491/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1353 - val_loss: 0.1223\n",
      "Epoch 492/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1304 - val_loss: 0.1226\n",
      "Epoch 493/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1319 - val_loss: 0.1222\n",
      "Epoch 494/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1319 - val_loss: 0.1229\n",
      "Epoch 495/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1222\n",
      "Epoch 496/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1326 - val_loss: 0.1222\n",
      "Epoch 497/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1303 - val_loss: 0.1222\n",
      "Epoch 498/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1284 - val_loss: 0.1223\n",
      "Epoch 499/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1326 - val_loss: 0.1222\n",
      "Epoch 500/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1320 - val_loss: 0.1223\n",
      "Epoch 501/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1291 - val_loss: 0.1236\n",
      "Epoch 502/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1223\n",
      "Epoch 503/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1366 - val_loss: 0.1224\n",
      "Epoch 504/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1317 - val_loss: 0.1223\n",
      "Epoch 505/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1324 - val_loss: 0.1221\n",
      "Epoch 506/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1298 - val_loss: 0.1223\n",
      "Epoch 507/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1363 - val_loss: 0.1223\n",
      "Epoch 508/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1233\n",
      "Epoch 509/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1319 - val_loss: 0.1224\n",
      "Epoch 510/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1292 - val_loss: 0.1222\n",
      "Epoch 511/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1338 - val_loss: 0.1223\n",
      "Epoch 512/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1309 - val_loss: 0.1223\n",
      "Epoch 513/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1333 - val_loss: 0.1223\n",
      "Epoch 514/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1315 - val_loss: 0.1224\n",
      "Epoch 515/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1349 - val_loss: 0.1226\n",
      "Epoch 516/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1309 - val_loss: 0.1229\n",
      "Epoch 517/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1329 - val_loss: 0.1224\n",
      "Epoch 518/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1304 - val_loss: 0.1221\n",
      "Epoch 519/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1223\n",
      "Epoch 520/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1340 - val_loss: 0.1225\n",
      "Epoch 521/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1305 - val_loss: 0.1222\n",
      "Epoch 522/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1359 - val_loss: 0.1221\n",
      "Epoch 523/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1369 - val_loss: 0.1221\n",
      "Epoch 524/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1344 - val_loss: 0.1223\n",
      "Epoch 525/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1305 - val_loss: 0.1223\n",
      "Epoch 526/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1286 - val_loss: 0.1222\n",
      "Epoch 527/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1295 - val_loss: 0.1223\n",
      "Epoch 528/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1320 - val_loss: 0.1223\n",
      "Epoch 529/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1226\n",
      "Epoch 530/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1306 - val_loss: 0.1223\n",
      "Epoch 531/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1222\n",
      "Epoch 532/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1322 - val_loss: 0.1222\n",
      "Epoch 533/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1223\n",
      "Epoch 534/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1301 - val_loss: 0.1223\n",
      "Epoch 535/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1281 - val_loss: 0.1223\n",
      "\n",
      "Epoch 00535: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 536/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1222\n",
      "Epoch 537/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1372 - val_loss: 0.1221\n",
      "Epoch 538/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1309 - val_loss: 0.1220\n",
      "Epoch 539/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1220\n",
      "Epoch 540/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1341 - val_loss: 0.1220\n",
      "Epoch 541/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1306 - val_loss: 0.1219\n",
      "Epoch 542/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1309 - val_loss: 0.1218\n",
      "Epoch 543/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1363 - val_loss: 0.1221\n",
      "Epoch 544/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1329 - val_loss: 0.1220\n",
      "Epoch 545/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1364 - val_loss: 0.1219\n",
      "Epoch 546/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1343 - val_loss: 0.1219\n",
      "Epoch 547/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1347 - val_loss: 0.1219\n",
      "Epoch 548/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1291 - val_loss: 0.1218\n",
      "Epoch 549/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1328 - val_loss: 0.1220\n",
      "Epoch 550/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1345 - val_loss: 0.1218\n",
      "Epoch 551/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1338 - val_loss: 0.1221\n",
      "Epoch 552/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1285 - val_loss: 0.1219\n",
      "Epoch 553/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1337 - val_loss: 0.1221\n",
      "Epoch 554/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1296 - val_loss: 0.1220\n",
      "Epoch 555/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1218\n",
      "Epoch 556/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1287 - val_loss: 0.1220\n",
      "Epoch 557/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1331 - val_loss: 0.1219\n",
      "Epoch 558/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1365 - val_loss: 0.1219\n",
      "Epoch 559/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1349 - val_loss: 0.1220\n",
      "Epoch 560/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1305 - val_loss: 0.1219\n",
      "Epoch 561/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1352 - val_loss: 0.1219\n",
      "Epoch 562/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1303 - val_loss: 0.1219\n",
      "Epoch 563/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1330 - val_loss: 0.1221\n",
      "Epoch 564/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1351 - val_loss: 0.1220\n",
      "Epoch 565/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1333 - val_loss: 0.1221\n",
      "Epoch 566/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1272 - val_loss: 0.1219\n",
      "Epoch 567/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1332 - val_loss: 0.1219\n",
      "Epoch 568/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1376 - val_loss: 0.1219\n",
      "Epoch 569/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1318 - val_loss: 0.1218\n",
      "Epoch 570/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1355 - val_loss: 0.1218\n",
      "Epoch 571/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1326 - val_loss: 0.1219\n",
      "\n",
      "Epoch 00571: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 572/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1252 - val_loss: 0.1218\n",
      "Epoch 573/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1371 - val_loss: 0.1218\n",
      "Epoch 574/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1325 - val_loss: 0.1217\n",
      "Epoch 575/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1289 - val_loss: 0.1218\n",
      "Epoch 576/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1340 - val_loss: 0.1217\n",
      "Epoch 577/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1351 - val_loss: 0.1218\n",
      "Epoch 578/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1292 - val_loss: 0.1221\n",
      "Epoch 579/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1340 - val_loss: 0.1218\n",
      "Epoch 580/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1342 - val_loss: 0.1219\n",
      "Epoch 581/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1286 - val_loss: 0.1217\n",
      "Epoch 582/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1346 - val_loss: 0.1218\n",
      "Epoch 583/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1291 - val_loss: 0.1221\n",
      "Epoch 584/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1321 - val_loss: 0.1224\n",
      "Epoch 585/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1348 - val_loss: 0.1218\n",
      "Epoch 586/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1364 - val_loss: 0.1218\n",
      "Epoch 587/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1380 - val_loss: 0.1217\n",
      "Epoch 588/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1338 - val_loss: 0.1218\n",
      "Epoch 589/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1287 - val_loss: 0.1219\n",
      "Epoch 590/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1322 - val_loss: 0.1218\n",
      "Epoch 591/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1319 - val_loss: 0.1218\n",
      "Epoch 592/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1219\n",
      "Epoch 593/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1277 - val_loss: 0.1220\n",
      "Epoch 594/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1319 - val_loss: 0.1217\n",
      "Epoch 595/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1315 - val_loss: 0.1220\n",
      "Epoch 596/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1282 - val_loss: 0.1218\n",
      "Epoch 597/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1304 - val_loss: 0.1217\n",
      "Epoch 598/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1329 - val_loss: 0.1218\n",
      "Epoch 599/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1289 - val_loss: 0.1217\n",
      "Epoch 600/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1322 - val_loss: 0.1220\n",
      "Epoch 601/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1323 - val_loss: 0.1217\n",
      "Epoch 602/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1336 - val_loss: 0.1221\n",
      "Epoch 603/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1307 - val_loss: 0.1221\n",
      "Epoch 604/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1336 - val_loss: 0.1217\n",
      "\n",
      "Epoch 00604: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 605/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1286 - val_loss: 0.1217\n",
      "Epoch 606/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1336 - val_loss: 0.1217\n",
      "Epoch 607/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1282 - val_loss: 0.1217\n",
      "Epoch 608/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1286 - val_loss: 0.1217\n",
      "Epoch 609/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1344 - val_loss: 0.1217\n",
      "Epoch 610/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1288 - val_loss: 0.1220\n",
      "Epoch 611/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1310 - val_loss: 0.1222\n",
      "Epoch 612/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1312 - val_loss: 0.1217\n",
      "Epoch 613/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1369 - val_loss: 0.1218\n",
      "Epoch 614/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1313 - val_loss: 0.1216\n",
      "Epoch 615/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1277 - val_loss: 0.1218\n",
      "Epoch 616/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1307 - val_loss: 0.1217\n",
      "Epoch 617/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1300 - val_loss: 0.1217\n",
      "Epoch 618/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1316 - val_loss: 0.1217\n",
      "Epoch 619/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1293 - val_loss: 0.1224\n",
      "Epoch 620/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1301 - val_loss: 0.1221\n",
      "Epoch 621/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1308 - val_loss: 0.1220\n",
      "Epoch 622/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1329 - val_loss: 0.1217\n",
      "Epoch 623/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1350 - val_loss: 0.1217\n",
      "Epoch 624/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 0.1348 - val_loss: 0.1217\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00624: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1060814107301375\n",
      "Training 3JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 1207321 samples, validate on 303058 samples\n",
      "Epoch 1/2000\n",
      "1207321/1207321 [==============================] - 14s 11us/step - loss: 0.9409 - val_loss: 0.6632\n",
      "Epoch 2/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.5924 - val_loss: 0.5488\n",
      "Epoch 3/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.5325 - val_loss: 0.5194\n",
      "Epoch 4/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4940 - val_loss: 0.4758\n",
      "Epoch 5/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4656 - val_loss: 0.4417\n",
      "Epoch 6/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4438 - val_loss: 0.5025\n",
      "Epoch 7/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4288 - val_loss: 0.4091\n",
      "Epoch 8/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4143 - val_loss: 0.4166\n",
      "Epoch 9/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4053 - val_loss: 0.3887\n",
      "Epoch 10/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.4007 - val_loss: 0.3766\n",
      "Epoch 11/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3805 - val_loss: 0.3836\n",
      "Epoch 12/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3801 - val_loss: 0.3816\n",
      "Epoch 13/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3722 - val_loss: 0.3608\n",
      "Epoch 14/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3668 - val_loss: 0.3605\n",
      "Epoch 15/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3606 - val_loss: 0.3420\n",
      "Epoch 16/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3486 - val_loss: 0.3406\n",
      "Epoch 17/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3452 - val_loss: 0.3468\n",
      "Epoch 18/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3472 - val_loss: 0.3435\n",
      "Epoch 19/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3446 - val_loss: 0.3355\n",
      "Epoch 20/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3374 - val_loss: 0.3174\n",
      "Epoch 21/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3316 - val_loss: 0.3257\n",
      "Epoch 22/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3295 - val_loss: 0.3281\n",
      "Epoch 23/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3322 - val_loss: 0.3066\n",
      "Epoch 24/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3180 - val_loss: 0.3284\n",
      "Epoch 25/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3223 - val_loss: 0.3144\n",
      "Epoch 26/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3148 - val_loss: 0.2981\n",
      "Epoch 27/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3167 - val_loss: 0.3023\n",
      "Epoch 28/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3097 - val_loss: 0.3014\n",
      "Epoch 29/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3066 - val_loss: 0.2985\n",
      "Epoch 30/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3067 - val_loss: 0.2911\n",
      "Epoch 31/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3021 - val_loss: 0.2878\n",
      "Epoch 32/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3004 - val_loss: 0.2932\n",
      "Epoch 33/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3010 - val_loss: 0.2924\n",
      "Epoch 34/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2951 - val_loss: 0.2878\n",
      "Epoch 35/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.3005 - val_loss: 0.2912\n",
      "Epoch 36/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2978 - val_loss: 0.2958\n",
      "Epoch 37/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2926 - val_loss: 0.2755\n",
      "Epoch 38/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2899 - val_loss: 0.2783\n",
      "Epoch 39/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2895 - val_loss: 0.2773\n",
      "Epoch 40/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2858 - val_loss: 0.2815\n",
      "Epoch 41/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2857 - val_loss: 0.2778\n",
      "Epoch 42/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2849 - val_loss: 0.2808\n",
      "Epoch 43/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2877 - val_loss: 0.2696\n",
      "Epoch 44/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2839 - val_loss: 0.2748\n",
      "Epoch 45/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2785 - val_loss: 0.2657\n",
      "Epoch 46/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2843 - val_loss: 0.2676\n",
      "Epoch 47/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2797 - val_loss: 0.2719\n",
      "Epoch 48/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2788 - val_loss: 0.2624\n",
      "Epoch 49/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2733 - val_loss: 0.2720\n",
      "Epoch 50/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2736 - val_loss: 0.2668\n",
      "Epoch 51/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2847 - val_loss: 0.2706\n",
      "Epoch 52/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2721 - val_loss: 0.2587\n",
      "Epoch 53/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2686 - val_loss: 0.2585\n",
      "Epoch 54/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2718 - val_loss: 0.2605\n",
      "Epoch 55/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2716 - val_loss: 0.2573\n",
      "Epoch 56/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2671 - val_loss: 0.2600\n",
      "Epoch 57/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2667 - val_loss: 0.2549\n",
      "Epoch 58/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2680 - val_loss: 0.2607\n",
      "Epoch 59/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2665 - val_loss: 0.2553\n",
      "Epoch 60/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2631 - val_loss: 0.2573\n",
      "Epoch 61/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2662 - val_loss: 0.2501\n",
      "Epoch 62/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2623 - val_loss: 0.2482\n",
      "Epoch 63/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2592 - val_loss: 0.2538\n",
      "Epoch 64/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2613 - val_loss: 0.2592\n",
      "Epoch 65/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2600 - val_loss: 0.2467\n",
      "Epoch 66/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2581 - val_loss: 0.2534\n",
      "Epoch 67/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2619 - val_loss: 0.2566\n",
      "Epoch 68/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2571 - val_loss: 0.2543\n",
      "Epoch 69/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2574 - val_loss: 0.2456\n",
      "Epoch 70/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2579 - val_loss: 0.2488\n",
      "Epoch 71/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2594 - val_loss: 0.2456\n",
      "Epoch 72/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2529 - val_loss: 0.2488\n",
      "Epoch 73/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2563 - val_loss: 0.2418\n",
      "Epoch 74/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2560 - val_loss: 0.2461\n",
      "Epoch 75/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2538 - val_loss: 0.2514\n",
      "Epoch 76/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2531 - val_loss: 0.2432\n",
      "Epoch 77/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2524 - val_loss: 0.2471\n",
      "Epoch 78/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2527 - val_loss: 0.2415\n",
      "Epoch 79/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2500 - val_loss: 0.2410\n",
      "Epoch 80/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2511 - val_loss: 0.2442\n",
      "Epoch 81/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2477 - val_loss: 0.2428\n",
      "Epoch 82/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2497 - val_loss: 0.2365\n",
      "Epoch 83/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2470 - val_loss: 0.2508\n",
      "Epoch 84/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2512 - val_loss: 0.2529\n",
      "Epoch 85/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2477 - val_loss: 0.2416\n",
      "Epoch 86/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2460 - val_loss: 0.2346\n",
      "Epoch 87/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2471 - val_loss: 0.2494\n",
      "Epoch 88/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2443 - val_loss: 0.2528\n",
      "Epoch 89/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2456 - val_loss: 0.2430\n",
      "Epoch 90/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2431 - val_loss: 0.2415\n",
      "Epoch 91/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2433 - val_loss: 0.2506\n",
      "Epoch 92/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2438 - val_loss: 0.2387\n",
      "Epoch 93/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2420 - val_loss: 0.2366\n",
      "Epoch 94/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2424 - val_loss: 0.2436\n",
      "Epoch 95/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2412 - val_loss: 0.2433\n",
      "Epoch 96/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2412 - val_loss: 0.2338\n",
      "Epoch 97/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2426 - val_loss: 0.2333\n",
      "Epoch 98/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2403 - val_loss: 0.2346\n",
      "Epoch 99/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2410 - val_loss: 0.2519\n",
      "Epoch 100/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2400 - val_loss: 0.2343\n",
      "Epoch 101/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2381 - val_loss: 0.2364\n",
      "Epoch 102/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2399 - val_loss: 0.2346\n",
      "Epoch 103/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2389 - val_loss: 0.2337\n",
      "Epoch 104/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2388 - val_loss: 0.2304\n",
      "Epoch 105/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2369 - val_loss: 0.2344\n",
      "Epoch 106/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2381 - val_loss: 0.2345\n",
      "Epoch 107/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2381 - val_loss: 0.2363\n",
      "Epoch 108/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2354 - val_loss: 0.2362\n",
      "Epoch 109/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2368 - val_loss: 0.2288\n",
      "Epoch 110/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2356 - val_loss: 0.2316\n",
      "Epoch 111/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2393 - val_loss: 0.2297\n",
      "Epoch 112/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2343 - val_loss: 0.2421\n",
      "Epoch 113/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2354 - val_loss: 0.2480\n",
      "Epoch 114/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2348 - val_loss: 0.2409\n",
      "Epoch 115/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2359 - val_loss: 0.2267\n",
      "Epoch 116/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2331 - val_loss: 0.2349\n",
      "Epoch 117/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2336 - val_loss: 0.2267\n",
      "Epoch 118/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2344 - val_loss: 0.2343\n",
      "Epoch 119/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2337 - val_loss: 0.2392\n",
      "Epoch 120/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2308 - val_loss: 0.2265\n",
      "Epoch 121/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2309 - val_loss: 0.2227\n",
      "Epoch 122/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2313 - val_loss: 0.2241\n",
      "Epoch 123/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2301 - val_loss: 0.2321\n",
      "Epoch 124/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2311 - val_loss: 0.2403\n",
      "Epoch 125/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2296 - val_loss: 0.2335\n",
      "Epoch 126/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2373 - val_loss: 0.2353\n",
      "Epoch 127/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2305 - val_loss: 0.2304\n",
      "Epoch 128/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2304 - val_loss: 0.2238\n",
      "Epoch 129/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2284 - val_loss: 0.2244\n",
      "Epoch 130/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2298 - val_loss: 0.2223\n",
      "Epoch 131/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2282 - val_loss: 0.2238\n",
      "Epoch 132/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2309 - val_loss: 0.2286\n",
      "Epoch 133/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2295 - val_loss: 0.2339\n",
      "Epoch 134/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2293 - val_loss: 0.2274\n",
      "Epoch 135/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2339 - val_loss: 0.2336\n",
      "Epoch 136/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2268 - val_loss: 0.2291\n",
      "Epoch 137/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2263 - val_loss: 0.2270\n",
      "Epoch 138/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2259 - val_loss: 0.2260\n",
      "Epoch 139/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2290 - val_loss: 0.2239\n",
      "Epoch 140/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2278 - val_loss: 0.2252\n",
      "Epoch 141/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2274 - val_loss: 0.2300\n",
      "Epoch 142/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2271 - val_loss: 0.2269\n",
      "Epoch 143/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2255 - val_loss: 0.2331\n",
      "Epoch 144/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2258 - val_loss: 0.2184\n",
      "Epoch 145/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2250 - val_loss: 0.2203\n",
      "Epoch 146/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2240 - val_loss: 0.2177\n",
      "Epoch 147/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2251 - val_loss: 0.2197\n",
      "Epoch 148/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2239 - val_loss: 0.2241\n",
      "Epoch 149/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2262 - val_loss: 0.2236\n",
      "Epoch 150/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2240 - val_loss: 0.2205\n",
      "Epoch 151/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2227 - val_loss: 0.2270\n",
      "Epoch 152/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2241 - val_loss: 0.2224\n",
      "Epoch 153/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2260 - val_loss: 0.2193\n",
      "Epoch 154/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2234 - val_loss: 0.2184\n",
      "Epoch 155/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2220 - val_loss: 0.2212\n",
      "Epoch 156/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2223 - val_loss: 0.2236\n",
      "Epoch 157/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2230 - val_loss: 0.2219\n",
      "Epoch 158/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2249 - val_loss: 0.2198\n",
      "Epoch 159/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2207 - val_loss: 0.2221\n",
      "Epoch 160/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2220 - val_loss: 0.2195\n",
      "Epoch 161/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2206 - val_loss: 0.2168\n",
      "Epoch 162/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2221 - val_loss: 0.2282\n",
      "Epoch 163/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2200 - val_loss: 0.2185\n",
      "Epoch 164/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2221 - val_loss: 0.2168\n",
      "Epoch 165/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2226 - val_loss: 0.2158\n",
      "Epoch 166/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2208 - val_loss: 0.2155\n",
      "Epoch 167/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2214 - val_loss: 0.2235\n",
      "Epoch 168/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2190 - val_loss: 0.2184\n",
      "Epoch 169/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2204 - val_loss: 0.2169\n",
      "Epoch 170/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2197 - val_loss: 0.2197\n",
      "Epoch 171/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2194 - val_loss: 0.2210\n",
      "Epoch 172/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2197 - val_loss: 0.2156\n",
      "Epoch 173/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2219 - val_loss: 0.2226\n",
      "Epoch 174/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2186 - val_loss: 0.2138\n",
      "Epoch 175/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2190 - val_loss: 0.2155\n",
      "Epoch 176/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2184 - val_loss: 0.2156\n",
      "Epoch 177/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2188 - val_loss: 0.2168\n",
      "Epoch 178/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2187 - val_loss: 0.2172\n",
      "Epoch 179/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2193 - val_loss: 0.2122\n",
      "Epoch 180/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2183 - val_loss: 0.2177\n",
      "Epoch 181/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2181 - val_loss: 0.2144\n",
      "Epoch 182/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2200 - val_loss: 0.2126\n",
      "Epoch 183/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2179 - val_loss: 0.2177\n",
      "Epoch 184/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2190 - val_loss: 0.2144\n",
      "Epoch 185/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2180 - val_loss: 0.2142\n",
      "Epoch 186/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2172 - val_loss: 0.2227\n",
      "Epoch 187/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2158 - val_loss: 0.2127\n",
      "Epoch 188/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2173 - val_loss: 0.2237\n",
      "Epoch 189/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2149 - val_loss: 0.2126\n",
      "Epoch 190/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2176 - val_loss: 0.2155\n",
      "Epoch 191/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2153 - val_loss: 0.2175\n",
      "Epoch 192/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2202 - val_loss: 0.2117\n",
      "Epoch 193/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2199 - val_loss: 0.2114\n",
      "Epoch 194/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2154 - val_loss: 0.2130\n",
      "Epoch 195/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2140 - val_loss: 0.2166\n",
      "Epoch 196/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2133 - val_loss: 0.2189\n",
      "Epoch 197/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2163 - val_loss: 0.2150\n",
      "Epoch 198/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2167 - val_loss: 0.2129\n",
      "Epoch 199/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2172 - val_loss: 0.2232\n",
      "Epoch 200/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2161 - val_loss: 0.2122\n",
      "Epoch 201/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2160 - val_loss: 0.2120\n",
      "Epoch 202/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2130 - val_loss: 0.2173\n",
      "Epoch 203/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2147 - val_loss: 0.2140\n",
      "Epoch 204/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2144 - val_loss: 0.2155\n",
      "Epoch 205/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2135 - val_loss: 0.2134\n",
      "Epoch 206/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2132 - val_loss: 0.2155\n",
      "Epoch 207/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2156 - val_loss: 0.2146\n",
      "Epoch 208/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2138 - val_loss: 0.2174\n",
      "Epoch 209/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2146 - val_loss: 0.2199\n",
      "Epoch 210/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2150 - val_loss: 0.2109\n",
      "Epoch 211/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2134 - val_loss: 0.2106\n",
      "Epoch 212/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2129 - val_loss: 0.2109\n",
      "Epoch 213/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2128 - val_loss: 0.2157\n",
      "Epoch 214/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2133 - val_loss: 0.2072\n",
      "Epoch 215/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2124 - val_loss: 0.2103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2121 - val_loss: 0.2133\n",
      "Epoch 217/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2126 - val_loss: 0.2224\n",
      "Epoch 218/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2115 - val_loss: 0.2242\n",
      "Epoch 219/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2111 - val_loss: 0.2097\n",
      "Epoch 220/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2126 - val_loss: 0.2124\n",
      "Epoch 221/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2115 - val_loss: 0.2161\n",
      "Epoch 222/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2109 - val_loss: 0.2092\n",
      "Epoch 223/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2108 - val_loss: 0.2082\n",
      "Epoch 224/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2111 - val_loss: 0.2155\n",
      "Epoch 225/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2134 - val_loss: 0.2099\n",
      "Epoch 226/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2103 - val_loss: 0.2097\n",
      "Epoch 227/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2127 - val_loss: 0.2161\n",
      "Epoch 228/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2108 - val_loss: 0.2081\n",
      "Epoch 229/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2119 - val_loss: 0.2129\n",
      "Epoch 230/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2112 - val_loss: 0.2198\n",
      "Epoch 231/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2124 - val_loss: 0.2083\n",
      "Epoch 232/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2090 - val_loss: 0.2162\n",
      "Epoch 233/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2098 - val_loss: 0.2197\n",
      "Epoch 234/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2121 - val_loss: 0.2156\n",
      "Epoch 235/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2096 - val_loss: 0.2123\n",
      "Epoch 236/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2097 - val_loss: 0.2084\n",
      "Epoch 237/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2096 - val_loss: 0.2057\n",
      "Epoch 238/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2089 - val_loss: 0.2090\n",
      "Epoch 239/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2097 - val_loss: 0.2075\n",
      "Epoch 240/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2083 - val_loss: 0.2125\n",
      "Epoch 241/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2099 - val_loss: 0.2130\n",
      "Epoch 242/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2131 - val_loss: 0.2074\n",
      "Epoch 243/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2086 - val_loss: 0.2166\n",
      "Epoch 244/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2101 - val_loss: 0.2178\n",
      "Epoch 245/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2094 - val_loss: 0.2079\n",
      "Epoch 246/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2092 - val_loss: 0.2074\n",
      "Epoch 247/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2096 - val_loss: 0.2092\n",
      "Epoch 248/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2078 - val_loss: 0.2351\n",
      "Epoch 249/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2114 - val_loss: 0.2128\n",
      "Epoch 250/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2067 - val_loss: 0.2091\n",
      "Epoch 251/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2142 - val_loss: 0.2119\n",
      "Epoch 252/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2086 - val_loss: 0.2118\n",
      "Epoch 253/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2077 - val_loss: 0.2068\n",
      "Epoch 254/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2080 - val_loss: 0.2120\n",
      "Epoch 255/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2099 - val_loss: 0.2065\n",
      "Epoch 256/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2081 - val_loss: 0.2107\n",
      "Epoch 257/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2071 - val_loss: 0.2114\n",
      "Epoch 258/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2070 - val_loss: 0.2085\n",
      "Epoch 259/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2076 - val_loss: 0.2054\n",
      "Epoch 260/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2077 - val_loss: 0.2073\n",
      "Epoch 261/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2087 - val_loss: 0.2177\n",
      "Epoch 262/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2075 - val_loss: 0.2088\n",
      "Epoch 263/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2083 - val_loss: 0.2094\n",
      "Epoch 264/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2071 - val_loss: 0.2201\n",
      "Epoch 265/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2069 - val_loss: 0.2289\n",
      "Epoch 266/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2073 - val_loss: 0.2098\n",
      "Epoch 267/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2062 - val_loss: 0.2065\n",
      "Epoch 268/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2062 - val_loss: 0.2056\n",
      "Epoch 269/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2078 - val_loss: 0.2052\n",
      "Epoch 270/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2065 - val_loss: 0.2052\n",
      "Epoch 271/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2065 - val_loss: 0.2057\n",
      "Epoch 272/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2050 - val_loss: 0.2084\n",
      "Epoch 273/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2058 - val_loss: 0.2061\n",
      "Epoch 274/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2049 - val_loss: 0.2047\n",
      "Epoch 275/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2066 - val_loss: 0.2109\n",
      "Epoch 276/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2056 - val_loss: 0.2148\n",
      "Epoch 277/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2055 - val_loss: 0.2154\n",
      "Epoch 278/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2057 - val_loss: 0.2053\n",
      "Epoch 279/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2062 - val_loss: 0.2093\n",
      "Epoch 280/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2050 - val_loss: 0.2054\n",
      "Epoch 281/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2074 - val_loss: 0.2050\n",
      "Epoch 282/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2048 - val_loss: 0.2149\n",
      "Epoch 283/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2043 - val_loss: 0.2062\n",
      "Epoch 284/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2046 - val_loss: 0.2067\n",
      "Epoch 285/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2069 - val_loss: 0.2027\n",
      "Epoch 286/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2049 - val_loss: 0.2083\n",
      "Epoch 287/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2057 - val_loss: 0.2066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2055 - val_loss: 0.2068\n",
      "Epoch 289/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2051 - val_loss: 0.2057\n",
      "Epoch 290/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2103 - val_loss: 0.2051\n",
      "Epoch 291/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2040 - val_loss: 0.2053\n",
      "Epoch 292/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2035 - val_loss: 0.2036\n",
      "Epoch 293/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2048 - val_loss: 0.2089\n",
      "Epoch 294/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2042 - val_loss: 0.2061\n",
      "Epoch 295/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2045 - val_loss: 0.2112\n",
      "Epoch 296/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2037 - val_loss: 0.2056\n",
      "Epoch 297/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2035 - val_loss: 0.2020\n",
      "Epoch 298/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2042 - val_loss: 0.2038\n",
      "Epoch 299/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2019 - val_loss: 0.2035\n",
      "Epoch 300/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2042 - val_loss: 0.2112\n",
      "Epoch 301/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2030 - val_loss: 0.2023\n",
      "Epoch 302/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2022 - val_loss: 0.2096\n",
      "Epoch 303/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2032 - val_loss: 0.2081\n",
      "Epoch 304/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2026 - val_loss: 0.2038\n",
      "Epoch 305/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2033 - val_loss: 0.2045\n",
      "Epoch 306/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2034 - val_loss: 0.2044\n",
      "Epoch 307/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2038 - val_loss: 0.2040\n",
      "Epoch 308/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2042 - val_loss: 0.2073\n",
      "Epoch 309/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2015 - val_loss: 0.2050\n",
      "Epoch 310/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2028 - val_loss: 0.2065\n",
      "Epoch 311/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2025 - val_loss: 0.2058\n",
      "Epoch 312/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2042 - val_loss: 0.2053\n",
      "Epoch 313/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2026 - val_loss: 0.2028\n",
      "Epoch 314/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2017 - val_loss: 0.2035\n",
      "Epoch 315/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2014 - val_loss: 0.2029\n",
      "Epoch 316/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2033 - val_loss: 0.2074\n",
      "Epoch 317/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2037 - val_loss: 0.2020\n",
      "Epoch 318/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2031 - val_loss: 0.2048\n",
      "Epoch 319/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2018 - val_loss: 0.2070\n",
      "Epoch 320/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2027 - val_loss: 0.2031\n",
      "Epoch 321/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2010 - val_loss: 0.2014\n",
      "Epoch 322/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2021 - val_loss: 0.2044\n",
      "Epoch 323/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2024 - val_loss: 0.2072\n",
      "Epoch 324/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2022 - val_loss: 0.2044\n",
      "Epoch 325/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2026 - val_loss: 0.2039\n",
      "Epoch 326/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2005 - val_loss: 0.2038\n",
      "Epoch 327/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2023 - val_loss: 0.2013\n",
      "Epoch 328/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2016 - val_loss: 0.2053\n",
      "Epoch 329/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2006 - val_loss: 0.2047\n",
      "Epoch 330/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2058 - val_loss: 0.2049\n",
      "Epoch 331/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2026 - val_loss: 0.2011\n",
      "Epoch 332/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2017 - val_loss: 0.2029\n",
      "Epoch 333/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2021 - val_loss: 0.2040\n",
      "Epoch 334/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2019 - val_loss: 0.1996\n",
      "Epoch 335/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2010 - val_loss: 0.2019\n",
      "Epoch 336/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2031 - val_loss: 0.2050\n",
      "Epoch 337/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2008 - val_loss: 0.2108\n",
      "Epoch 338/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2009 - val_loss: 0.2015\n",
      "Epoch 339/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2001 - val_loss: 0.2072\n",
      "Epoch 340/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2010 - val_loss: 0.2027\n",
      "Epoch 341/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2038 - val_loss: 0.2184\n",
      "Epoch 342/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2036 - val_loss: 0.2154\n",
      "Epoch 343/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2007 - val_loss: 0.2062\n",
      "Epoch 344/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2012 - val_loss: 0.2160\n",
      "Epoch 345/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2007 - val_loss: 0.2034\n",
      "Epoch 346/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1997 - val_loss: 0.2038\n",
      "Epoch 347/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2083 - val_loss: 0.2025\n",
      "Epoch 348/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2001 - val_loss: 0.2018\n",
      "Epoch 349/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1998 - val_loss: 0.2073\n",
      "Epoch 350/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2014 - val_loss: 0.2108\n",
      "Epoch 351/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2026 - val_loss: 0.1985\n",
      "Epoch 352/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1990 - val_loss: 0.2151\n",
      "Epoch 353/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2006 - val_loss: 0.1989\n",
      "Epoch 354/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2000 - val_loss: 0.2007\n",
      "Epoch 355/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1985 - val_loss: 0.2067\n",
      "Epoch 356/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1989 - val_loss: 0.2027\n",
      "Epoch 357/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1998 - val_loss: 0.2028\n",
      "Epoch 358/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2024 - val_loss: 0.2034\n",
      "Epoch 359/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1988 - val_loss: 0.2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1995 - val_loss: 0.2040\n",
      "Epoch 361/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1990 - val_loss: 0.2006\n",
      "Epoch 362/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1977 - val_loss: 0.2033\n",
      "Epoch 363/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2006 - val_loss: 0.1997\n",
      "Epoch 364/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1999 - val_loss: 0.1985\n",
      "Epoch 365/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1986 - val_loss: 0.2021\n",
      "Epoch 366/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1983 - val_loss: 0.1990\n",
      "Epoch 367/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2000 - val_loss: 0.1993\n",
      "Epoch 368/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1997 - val_loss: 0.2021\n",
      "Epoch 369/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2008 - val_loss: 0.2055\n",
      "Epoch 370/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1973 - val_loss: 0.1984\n",
      "Epoch 371/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1982 - val_loss: 0.1987\n",
      "Epoch 372/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2079 - val_loss: 0.2018\n",
      "Epoch 373/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1985 - val_loss: 0.2036\n",
      "Epoch 374/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1990 - val_loss: 0.2074\n",
      "Epoch 375/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1987 - val_loss: 0.1982\n",
      "Epoch 376/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1975 - val_loss: 0.1976\n",
      "Epoch 377/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1981 - val_loss: 0.2108\n",
      "Epoch 378/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1991 - val_loss: 0.2008\n",
      "Epoch 379/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1970 - val_loss: 0.1992\n",
      "Epoch 380/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1991 - val_loss: 0.2002\n",
      "Epoch 381/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1981 - val_loss: 0.2006\n",
      "Epoch 382/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1976 - val_loss: 0.1984\n",
      "Epoch 383/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1981 - val_loss: 0.1988\n",
      "Epoch 384/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1986 - val_loss: 0.1977\n",
      "Epoch 385/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1995 - val_loss: 0.1982\n",
      "Epoch 386/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1975 - val_loss: 0.2006\n",
      "Epoch 387/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1968 - val_loss: 0.1989\n",
      "Epoch 388/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1977 - val_loss: 0.1992\n",
      "Epoch 389/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1979 - val_loss: 0.1972\n",
      "Epoch 390/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1975 - val_loss: 0.2014\n",
      "Epoch 391/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1989 - val_loss: 0.1999\n",
      "Epoch 392/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1987 - val_loss: 0.2035\n",
      "Epoch 393/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1984 - val_loss: 0.1987\n",
      "Epoch 394/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1974 - val_loss: 0.2006\n",
      "Epoch 395/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1968 - val_loss: 0.2006\n",
      "Epoch 396/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1988 - val_loss: 0.2025\n",
      "Epoch 397/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1974 - val_loss: 0.2066\n",
      "Epoch 398/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1964 - val_loss: 0.2029\n",
      "Epoch 399/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1988 - val_loss: 0.2012\n",
      "Epoch 400/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1968 - val_loss: 0.1980\n",
      "Epoch 401/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1968 - val_loss: 0.2008\n",
      "Epoch 402/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1984 - val_loss: 0.2024\n",
      "Epoch 403/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1961 - val_loss: 0.2019\n",
      "Epoch 404/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1978 - val_loss: 0.2023\n",
      "Epoch 405/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1969 - val_loss: 0.2042\n",
      "Epoch 406/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1973 - val_loss: 0.2003\n",
      "Epoch 407/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1962 - val_loss: 0.2016\n",
      "Epoch 408/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1971 - val_loss: 0.2022\n",
      "Epoch 409/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1965 - val_loss: 0.2255\n",
      "Epoch 410/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1951 - val_loss: 0.2035\n",
      "Epoch 411/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2021 - val_loss: 0.1979\n",
      "Epoch 412/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1955 - val_loss: 0.1980\n",
      "Epoch 413/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1954 - val_loss: 0.1973\n",
      "Epoch 414/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1966 - val_loss: 0.1989\n",
      "Epoch 415/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1954 - val_loss: 0.1977\n",
      "Epoch 416/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1975 - val_loss: 0.1975\n",
      "Epoch 417/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1951 - val_loss: 0.1986\n",
      "Epoch 418/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1963 - val_loss: 0.2002\n",
      "Epoch 419/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1945 - val_loss: 0.1955\n",
      "Epoch 420/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1957 - val_loss: 0.1970\n",
      "Epoch 421/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1971 - val_loss: 0.2011\n",
      "Epoch 422/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1964 - val_loss: 0.1975\n",
      "Epoch 423/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1940 - val_loss: 0.1992\n",
      "Epoch 424/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.2004 - val_loss: 0.1950\n",
      "Epoch 425/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1976 - val_loss: 0.1972\n",
      "Epoch 426/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1963 - val_loss: 0.1956\n",
      "Epoch 427/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1953 - val_loss: 0.2114\n",
      "Epoch 428/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1969 - val_loss: 0.2035\n",
      "Epoch 429/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1953 - val_loss: 0.1960\n",
      "Epoch 430/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1963 - val_loss: 0.1997\n",
      "Epoch 431/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1958 - val_loss: 0.1988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 432/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1952 - val_loss: 0.1968\n",
      "Epoch 433/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1953 - val_loss: 0.1949\n",
      "Epoch 434/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1939 - val_loss: 0.1997\n",
      "Epoch 435/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1946 - val_loss: 0.1984\n",
      "Epoch 436/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1947 - val_loss: 0.1980\n",
      "Epoch 437/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1944 - val_loss: 0.1964\n",
      "Epoch 438/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1935 - val_loss: 0.2102\n",
      "Epoch 439/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1946 - val_loss: 0.2017\n",
      "Epoch 440/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1955 - val_loss: 0.2009\n",
      "Epoch 441/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1960 - val_loss: 0.1979\n",
      "Epoch 442/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1944 - val_loss: 0.1965\n",
      "Epoch 443/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1949 - val_loss: 0.1993\n",
      "Epoch 444/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1948 - val_loss: 0.1960\n",
      "Epoch 445/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1935 - val_loss: 0.1945\n",
      "Epoch 446/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1954 - val_loss: 0.1971\n",
      "Epoch 447/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1957 - val_loss: 0.1973\n",
      "Epoch 448/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1932 - val_loss: 0.1998\n",
      "Epoch 449/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1958 - val_loss: 0.1978\n",
      "Epoch 450/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1938 - val_loss: 0.2055\n",
      "Epoch 451/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1948 - val_loss: 0.1979\n",
      "Epoch 452/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1931 - val_loss: 0.2016\n",
      "Epoch 453/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1937 - val_loss: 0.2031\n",
      "Epoch 454/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1943 - val_loss: 0.1946\n",
      "Epoch 455/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1939 - val_loss: 0.1967\n",
      "Epoch 456/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1941 - val_loss: 0.2060\n",
      "Epoch 457/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1971 - val_loss: 0.2012\n",
      "Epoch 458/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1944 - val_loss: 0.1979\n",
      "Epoch 459/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1931 - val_loss: 0.1998\n",
      "Epoch 460/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1949 - val_loss: 0.1954\n",
      "Epoch 461/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1940 - val_loss: 0.1963\n",
      "Epoch 462/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1953 - val_loss: 0.2127\n",
      "Epoch 463/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1958 - val_loss: 0.1951\n",
      "Epoch 464/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1949 - val_loss: 0.1993\n",
      "Epoch 465/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1932 - val_loss: 0.1997\n",
      "Epoch 466/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1931 - val_loss: 0.1995\n",
      "Epoch 467/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1931 - val_loss: 0.1948\n",
      "Epoch 468/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1944 - val_loss: 0.2065\n",
      "Epoch 469/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1935 - val_loss: 0.1968\n",
      "Epoch 470/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1940 - val_loss: 0.1938\n",
      "Epoch 471/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1931 - val_loss: 0.1964\n",
      "Epoch 472/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1929 - val_loss: 0.2046\n",
      "Epoch 473/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1953 - val_loss: 0.2004\n",
      "Epoch 474/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1930 - val_loss: 0.1993\n",
      "Epoch 475/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1940 - val_loss: 0.1952\n",
      "Epoch 476/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1917 - val_loss: 0.1989\n",
      "Epoch 477/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1963 - val_loss: 0.1945\n",
      "Epoch 478/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1932 - val_loss: 0.1973\n",
      "Epoch 479/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1927 - val_loss: 0.2011\n",
      "Epoch 480/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1937 - val_loss: 0.1983\n",
      "Epoch 481/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1940 - val_loss: 0.1947\n",
      "Epoch 482/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1933 - val_loss: 0.1956\n",
      "Epoch 483/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1941 - val_loss: 0.1946\n",
      "Epoch 484/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1931 - val_loss: 0.2053\n",
      "Epoch 485/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1928 - val_loss: 0.1938\n",
      "Epoch 486/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1923 - val_loss: 0.1941\n",
      "Epoch 487/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1926 - val_loss: 0.1978\n",
      "Epoch 488/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1927 - val_loss: 0.1983\n",
      "Epoch 489/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1936 - val_loss: 0.2070\n",
      "Epoch 490/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1933 - val_loss: 0.1975\n",
      "Epoch 491/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1958 - val_loss: 0.1965\n",
      "Epoch 492/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1910 - val_loss: 0.1948\n",
      "Epoch 493/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1919 - val_loss: 0.2035\n",
      "Epoch 494/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1916 - val_loss: 0.1954\n",
      "Epoch 495/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1928 - val_loss: 0.1955\n",
      "Epoch 496/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1930 - val_loss: 0.1954\n",
      "Epoch 497/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1928 - val_loss: 0.1949\n",
      "Epoch 498/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1921 - val_loss: 0.1966\n",
      "Epoch 499/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1918 - val_loss: 0.1963\n",
      "Epoch 500/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1927 - val_loss: 0.1948\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 501/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1848 - val_loss: 0.1893\n",
      "Epoch 502/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1836 - val_loss: 0.1894\n",
      "Epoch 503/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1832 - val_loss: 0.1895\n",
      "Epoch 504/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1833 - val_loss: 0.1878\n",
      "Epoch 505/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1810 - val_loss: 0.1876\n",
      "Epoch 506/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1824 - val_loss: 0.1876\n",
      "Epoch 507/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1823 - val_loss: 0.1893\n",
      "Epoch 508/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1813 - val_loss: 0.1886\n",
      "Epoch 509/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1831 - val_loss: 0.1860\n",
      "Epoch 510/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1813 - val_loss: 0.1859\n",
      "Epoch 511/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1808 - val_loss: 0.1916\n",
      "Epoch 512/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1815 - val_loss: 0.1877\n",
      "Epoch 513/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1834 - val_loss: 0.1877\n",
      "Epoch 514/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1818 - val_loss: 0.1878\n",
      "Epoch 515/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1815 - val_loss: 0.1885\n",
      "Epoch 516/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1810 - val_loss: 0.1871\n",
      "Epoch 517/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1804 - val_loss: 0.1864\n",
      "Epoch 518/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1823 - val_loss: 0.1869\n",
      "Epoch 519/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1870\n",
      "Epoch 520/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1815 - val_loss: 0.1891\n",
      "Epoch 521/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1816 - val_loss: 0.1861\n",
      "Epoch 522/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1817 - val_loss: 0.1870\n",
      "Epoch 523/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1828 - val_loss: 0.1863\n",
      "Epoch 524/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1808 - val_loss: 0.1917\n",
      "Epoch 525/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1822 - val_loss: 0.1863\n",
      "Epoch 526/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1815 - val_loss: 0.1875\n",
      "Epoch 527/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1820 - val_loss: 0.1869\n",
      "Epoch 528/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1857\n",
      "Epoch 529/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1869\n",
      "Epoch 530/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1827 - val_loss: 0.1866\n",
      "Epoch 531/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1809 - val_loss: 0.1868\n",
      "Epoch 532/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1816 - val_loss: 0.1900\n",
      "Epoch 533/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1819 - val_loss: 0.1905\n",
      "Epoch 534/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1814 - val_loss: 0.1876\n",
      "Epoch 535/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1813 - val_loss: 0.1861\n",
      "Epoch 536/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1808 - val_loss: 0.1996\n",
      "Epoch 537/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1883\n",
      "Epoch 538/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1820 - val_loss: 0.1856\n",
      "Epoch 539/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1829 - val_loss: 0.1861\n",
      "Epoch 540/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1805 - val_loss: 0.1878\n",
      "Epoch 541/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1814 - val_loss: 0.1868\n",
      "Epoch 542/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1822 - val_loss: 0.1893\n",
      "Epoch 543/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1803 - val_loss: 0.1860\n",
      "Epoch 544/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1816 - val_loss: 0.1908\n",
      "Epoch 545/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1810 - val_loss: 0.1990\n",
      "Epoch 546/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1805 - val_loss: 0.1871\n",
      "Epoch 547/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1863\n",
      "Epoch 548/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1810 - val_loss: 0.1872\n",
      "Epoch 549/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1810 - val_loss: 0.1878\n",
      "Epoch 550/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1830 - val_loss: 0.1867\n",
      "Epoch 551/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1801 - val_loss: 0.1918\n",
      "Epoch 552/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1795 - val_loss: 0.1852\n",
      "Epoch 553/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1807 - val_loss: 0.1871\n",
      "Epoch 554/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1801 - val_loss: 0.1874\n",
      "Epoch 555/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1800 - val_loss: 0.1854\n",
      "Epoch 556/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1804 - val_loss: 0.1859\n",
      "Epoch 557/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1808 - val_loss: 0.1904\n",
      "Epoch 558/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1802 - val_loss: 0.1869\n",
      "Epoch 559/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1807 - val_loss: 0.1856\n",
      "Epoch 560/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1816 - val_loss: 0.1910\n",
      "Epoch 561/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1798 - val_loss: 0.1862\n",
      "Epoch 562/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1868\n",
      "Epoch 563/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1811 - val_loss: 0.1866\n",
      "Epoch 564/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1803 - val_loss: 0.1866\n",
      "Epoch 565/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1806 - val_loss: 0.1874\n",
      "Epoch 566/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1804 - val_loss: 0.1868\n",
      "Epoch 567/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1801 - val_loss: 0.1856\n",
      "Epoch 568/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1796 - val_loss: 0.1860\n",
      "Epoch 569/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1800 - val_loss: 0.1879\n",
      "Epoch 570/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1804 - val_loss: 0.1867\n",
      "Epoch 571/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1808 - val_loss: 0.1876\n",
      "Epoch 572/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1804 - val_loss: 0.1866\n",
      "Epoch 573/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1793 - val_loss: 0.1935\n",
      "Epoch 574/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1800 - val_loss: 0.1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 575/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1798 - val_loss: 0.1871\n",
      "Epoch 576/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1805 - val_loss: 0.1861\n",
      "Epoch 577/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1799 - val_loss: 0.1855\n",
      "Epoch 578/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1800 - val_loss: 0.1854\n",
      "Epoch 579/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1795 - val_loss: 0.1857\n",
      "Epoch 580/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1821 - val_loss: 0.1861\n",
      "Epoch 581/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1807 - val_loss: 0.1906\n",
      "Epoch 582/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1812 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00582: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 583/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1780 - val_loss: 0.1848\n",
      "Epoch 584/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1754 - val_loss: 0.1843\n",
      "Epoch 585/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1758 - val_loss: 0.1828\n",
      "Epoch 586/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1757 - val_loss: 0.1825\n",
      "Epoch 587/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1738 - val_loss: 0.1829\n",
      "Epoch 588/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1751 - val_loss: 0.1821\n",
      "Epoch 589/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1748 - val_loss: 0.1828\n",
      "Epoch 590/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1741 - val_loss: 0.1823\n",
      "Epoch 591/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1746 - val_loss: 0.1828\n",
      "Epoch 592/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1747 - val_loss: 0.1824\n",
      "Epoch 593/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1746 - val_loss: 0.1852\n",
      "Epoch 594/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1749 - val_loss: 0.1832\n",
      "Epoch 595/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1761 - val_loss: 0.1836\n",
      "Epoch 596/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1751 - val_loss: 0.1827\n",
      "Epoch 597/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1751 - val_loss: 0.1823\n",
      "Epoch 598/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1749 - val_loss: 0.1828\n",
      "Epoch 599/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1739 - val_loss: 0.1834\n",
      "Epoch 600/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1747 - val_loss: 0.1828\n",
      "Epoch 601/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1751 - val_loss: 0.1820\n",
      "Epoch 602/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1739 - val_loss: 0.1824\n",
      "Epoch 603/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1751 - val_loss: 0.1822\n",
      "Epoch 604/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1745 - val_loss: 0.1832\n",
      "Epoch 605/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1763 - val_loss: 0.1825\n",
      "Epoch 606/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1753 - val_loss: 0.1830\n",
      "Epoch 607/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1748 - val_loss: 0.1870\n",
      "Epoch 608/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1755 - val_loss: 0.1827\n",
      "Epoch 609/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1741 - val_loss: 0.1818\n",
      "Epoch 610/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1750 - val_loss: 0.1832\n",
      "Epoch 611/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1739 - val_loss: 0.1821\n",
      "Epoch 612/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1749 - val_loss: 0.1826\n",
      "Epoch 613/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1727 - val_loss: 0.1832\n",
      "Epoch 614/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1755 - val_loss: 0.1830\n",
      "Epoch 615/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1746 - val_loss: 0.1826\n",
      "Epoch 616/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1753 - val_loss: 0.1830\n",
      "Epoch 617/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1748 - val_loss: 0.1816\n",
      "Epoch 618/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1745 - val_loss: 0.1832\n",
      "Epoch 619/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1753 - val_loss: 0.1828\n",
      "Epoch 620/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1749 - val_loss: 0.1828\n",
      "Epoch 621/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1743 - val_loss: 0.1829\n",
      "Epoch 622/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1745 - val_loss: 0.1823\n",
      "Epoch 623/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1742 - val_loss: 0.1832\n",
      "Epoch 624/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1739 - val_loss: 0.1824\n",
      "Epoch 625/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1760 - val_loss: 0.1860\n",
      "Epoch 626/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1739 - val_loss: 0.1829\n",
      "Epoch 627/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1744 - val_loss: 0.1840\n",
      "Epoch 628/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1740 - val_loss: 0.1838\n",
      "Epoch 629/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1745 - val_loss: 0.1829\n",
      "Epoch 630/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1749 - val_loss: 0.1829\n",
      "Epoch 631/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1731 - val_loss: 0.1836\n",
      "Epoch 632/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1753 - val_loss: 0.1844\n",
      "Epoch 633/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1743 - val_loss: 0.1821\n",
      "Epoch 634/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1742 - val_loss: 0.1823\n",
      "Epoch 635/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1739 - val_loss: 0.1820\n",
      "Epoch 636/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1738 - val_loss: 0.1821\n",
      "Epoch 637/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1747 - val_loss: 0.1833\n",
      "Epoch 638/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1748 - val_loss: 0.1820\n",
      "Epoch 639/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1752 - val_loss: 0.1843\n",
      "Epoch 640/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1754 - val_loss: 0.1824\n",
      "Epoch 641/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1735 - val_loss: 0.1815\n",
      "Epoch 642/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1745 - val_loss: 0.1815\n",
      "Epoch 643/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1748 - val_loss: 0.1830\n",
      "Epoch 644/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1742 - val_loss: 0.1820\n",
      "Epoch 645/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1740 - val_loss: 0.1816\n",
      "Epoch 646/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1722 - val_loss: 0.1822\n",
      "Epoch 647/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1745 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00647: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 648/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1738 - val_loss: 0.1810\n",
      "Epoch 649/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1720 - val_loss: 0.1806\n",
      "Epoch 650/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1728 - val_loss: 0.1809\n",
      "Epoch 651/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1717 - val_loss: 0.1804\n",
      "Epoch 652/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1726 - val_loss: 0.1811\n",
      "Epoch 653/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1718 - val_loss: 0.1802\n",
      "Epoch 654/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1720 - val_loss: 0.1814\n",
      "Epoch 655/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1722 - val_loss: 0.1802\n",
      "Epoch 656/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1712 - val_loss: 0.1807\n",
      "Epoch 657/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1734 - val_loss: 0.1806\n",
      "Epoch 658/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1731 - val_loss: 0.1804\n",
      "Epoch 659/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1722 - val_loss: 0.1804\n",
      "Epoch 660/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1722 - val_loss: 0.1809\n",
      "Epoch 661/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1724 - val_loss: 0.1808\n",
      "Epoch 662/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1718 - val_loss: 0.1806\n",
      "Epoch 663/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1710 - val_loss: 0.1802\n",
      "Epoch 664/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1718 - val_loss: 0.1802\n",
      "Epoch 665/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1722 - val_loss: 0.1812\n",
      "Epoch 666/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1719 - val_loss: 0.1805\n",
      "Epoch 667/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1712 - val_loss: 0.1802\n",
      "Epoch 668/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1720 - val_loss: 0.1801\n",
      "Epoch 669/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1723 - val_loss: 0.1807\n",
      "Epoch 670/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1802\n",
      "Epoch 671/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1717 - val_loss: 0.1807\n",
      "Epoch 672/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1711 - val_loss: 0.1806\n",
      "Epoch 673/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1720 - val_loss: 0.1802\n",
      "Epoch 674/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1713 - val_loss: 0.1801\n",
      "Epoch 675/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1725 - val_loss: 0.1817\n",
      "Epoch 676/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1701 - val_loss: 0.1801\n",
      "Epoch 677/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1731 - val_loss: 0.1812\n",
      "Epoch 678/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1715 - val_loss: 0.1803\n",
      "Epoch 679/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1701 - val_loss: 0.1801\n",
      "Epoch 680/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1718 - val_loss: 0.1806\n",
      "Epoch 681/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1719 - val_loss: 0.1802\n",
      "Epoch 682/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1723 - val_loss: 0.1800\n",
      "Epoch 683/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1715 - val_loss: 0.1807\n",
      "Epoch 684/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1708 - val_loss: 0.1801\n",
      "Epoch 685/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1722 - val_loss: 0.1802\n",
      "Epoch 686/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1706 - val_loss: 0.1805\n",
      "Epoch 687/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1703 - val_loss: 0.1808\n",
      "Epoch 688/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1709 - val_loss: 0.1800\n",
      "Epoch 689/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1718 - val_loss: 0.1805\n",
      "Epoch 690/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1707 - val_loss: 0.1803\n",
      "Epoch 691/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1726 - val_loss: 0.1804\n",
      "Epoch 692/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1709 - val_loss: 0.1801\n",
      "Epoch 693/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1718 - val_loss: 0.1809\n",
      "Epoch 694/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1807\n",
      "Epoch 695/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1705 - val_loss: 0.1804\n",
      "Epoch 696/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1799\n",
      "Epoch 697/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1711 - val_loss: 0.1807\n",
      "Epoch 698/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1716 - val_loss: 0.1804\n",
      "Epoch 699/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1713 - val_loss: 0.1802\n",
      "Epoch 700/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1700 - val_loss: 0.1806\n",
      "Epoch 701/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1716 - val_loss: 0.1803\n",
      "Epoch 702/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1711 - val_loss: 0.1801\n",
      "Epoch 703/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1731 - val_loss: 0.1803\n",
      "Epoch 704/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1709 - val_loss: 0.1807\n",
      "Epoch 705/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1717 - val_loss: 0.1805\n",
      "Epoch 706/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1702 - val_loss: 0.1799\n",
      "Epoch 707/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1717 - val_loss: 0.1816\n",
      "Epoch 708/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1708 - val_loss: 0.1805\n",
      "Epoch 709/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1724 - val_loss: 0.1805\n",
      "Epoch 710/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1719 - val_loss: 0.1808\n",
      "Epoch 711/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1711 - val_loss: 0.1813\n",
      "Epoch 712/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1717 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00712: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 713/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1795\n",
      "Epoch 714/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1701 - val_loss: 0.1799\n",
      "Epoch 715/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1706 - val_loss: 0.1794\n",
      "Epoch 716/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1705 - val_loss: 0.1794\n",
      "Epoch 717/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1709 - val_loss: 0.1795\n",
      "Epoch 718/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1690 - val_loss: 0.1794\n",
      "Epoch 719/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1696 - val_loss: 0.1794\n",
      "Epoch 720/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1705 - val_loss: 0.1795\n",
      "Epoch 721/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1690 - val_loss: 0.1792\n",
      "Epoch 722/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1704 - val_loss: 0.1796\n",
      "Epoch 723/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1691 - val_loss: 0.1794\n",
      "Epoch 724/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1691 - val_loss: 0.1793\n",
      "Epoch 725/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1700 - val_loss: 0.1795\n",
      "Epoch 726/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1703 - val_loss: 0.1803\n",
      "Epoch 727/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1696 - val_loss: 0.1795\n",
      "Epoch 728/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1794\n",
      "Epoch 729/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1691 - val_loss: 0.1794\n",
      "Epoch 730/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1703 - val_loss: 0.1792\n",
      "Epoch 731/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1717 - val_loss: 0.1793\n",
      "Epoch 732/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1796\n",
      "Epoch 733/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1707 - val_loss: 0.1796\n",
      "Epoch 734/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1709 - val_loss: 0.1794\n",
      "Epoch 735/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1793\n",
      "Epoch 736/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1797\n",
      "Epoch 737/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1795\n",
      "Epoch 738/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1703 - val_loss: 0.1795\n",
      "Epoch 739/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1690 - val_loss: 0.1798\n",
      "Epoch 740/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1791\n",
      "Epoch 741/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1701 - val_loss: 0.1795\n",
      "Epoch 742/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1794\n",
      "Epoch 743/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1799\n",
      "Epoch 744/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1699 - val_loss: 0.1796\n",
      "Epoch 745/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1794\n",
      "Epoch 746/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1704 - val_loss: 0.1792\n",
      "Epoch 747/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1795\n",
      "Epoch 748/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1703 - val_loss: 0.1797\n",
      "Epoch 749/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1794\n",
      "Epoch 750/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1705 - val_loss: 0.1798\n",
      "Epoch 751/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1691 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00751: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 752/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1789\n",
      "Epoch 753/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1712 - val_loss: 0.1791\n",
      "Epoch 754/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1680 - val_loss: 0.1790\n",
      "Epoch 755/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1680 - val_loss: 0.1791\n",
      "Epoch 756/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1686 - val_loss: 0.1790\n",
      "Epoch 757/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1790\n",
      "Epoch 758/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1687 - val_loss: 0.1791\n",
      "Epoch 759/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1789\n",
      "Epoch 760/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1789\n",
      "Epoch 761/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1690 - val_loss: 0.1790\n",
      "Epoch 762/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1696 - val_loss: 0.1792\n",
      "Epoch 763/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1700 - val_loss: 0.1790\n",
      "Epoch 764/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1699 - val_loss: 0.1791\n",
      "Epoch 765/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1680 - val_loss: 0.1789\n",
      "Epoch 766/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1683 - val_loss: 0.1792\n",
      "Epoch 767/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1789\n",
      "Epoch 768/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1788\n",
      "Epoch 769/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1790\n",
      "Epoch 770/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1789\n",
      "Epoch 771/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1788\n",
      "Epoch 772/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1790\n",
      "Epoch 773/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1684 - val_loss: 0.1790\n",
      "Epoch 774/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1705 - val_loss: 0.1790\n",
      "Epoch 775/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1790\n",
      "Epoch 776/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1680 - val_loss: 0.1791\n",
      "Epoch 777/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1690 - val_loss: 0.1788\n",
      "Epoch 778/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1685 - val_loss: 0.1788\n",
      "Epoch 779/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1696 - val_loss: 0.1789\n",
      "Epoch 780/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1694 - val_loss: 0.1790\n",
      "Epoch 781/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1685 - val_loss: 0.1791\n",
      "Epoch 782/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1682 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00782: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 783/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1794\n",
      "Epoch 784/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1695 - val_loss: 0.1791\n",
      "Epoch 785/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1787\n",
      "Epoch 786/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1680 - val_loss: 0.1789\n",
      "Epoch 787/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1674 - val_loss: 0.1792\n",
      "Epoch 788/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1687 - val_loss: 0.1790\n",
      "Epoch 789/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1696 - val_loss: 0.1787\n",
      "Epoch 790/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1683 - val_loss: 0.1788\n",
      "Epoch 791/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1679 - val_loss: 0.1786\n",
      "Epoch 792/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1788\n",
      "Epoch 793/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1789\n",
      "Epoch 794/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1788\n",
      "Epoch 795/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1696 - val_loss: 0.1787\n",
      "Epoch 796/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1670 - val_loss: 0.1787\n",
      "Epoch 797/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1686 - val_loss: 0.1787\n",
      "Epoch 798/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1788\n",
      "Epoch 799/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1684 - val_loss: 0.1788\n",
      "Epoch 800/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1682 - val_loss: 0.1788\n",
      "Epoch 801/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1789\n",
      "Epoch 802/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1705 - val_loss: 0.1787\n",
      "Epoch 803/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1675 - val_loss: 0.1789\n",
      "Epoch 804/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1798\n",
      "Epoch 805/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1687 - val_loss: 0.1788\n",
      "Epoch 806/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1699 - val_loss: 0.1788\n",
      "Epoch 807/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1692 - val_loss: 0.1788\n",
      "Epoch 808/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1790\n",
      "Epoch 809/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1787\n",
      "Epoch 810/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1698 - val_loss: 0.1789\n",
      "Epoch 811/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1693 - val_loss: 0.1789\n",
      "Epoch 812/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1681 - val_loss: 0.1788\n",
      "Epoch 813/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1673 - val_loss: 0.1789\n",
      "Epoch 814/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1698 - val_loss: 0.1787\n",
      "Epoch 815/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1704 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00815: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 816/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1703 - val_loss: 0.1790\n",
      "Epoch 817/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1701 - val_loss: 0.1787\n",
      "Epoch 818/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1787\n",
      "Epoch 819/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1675 - val_loss: 0.1787\n",
      "Epoch 820/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1691 - val_loss: 0.1789\n",
      "Epoch 821/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1676 - val_loss: 0.1786\n",
      "Epoch 822/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1700 - val_loss: 0.1789\n",
      "Epoch 823/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1682 - val_loss: 0.1800\n",
      "Epoch 824/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1676 - val_loss: 0.1794\n",
      "Epoch 825/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1697 - val_loss: 0.1788\n",
      "Epoch 826/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1680 - val_loss: 0.1786\n",
      "Epoch 827/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1688 - val_loss: 0.1788\n",
      "Epoch 828/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1691 - val_loss: 0.1787\n",
      "Epoch 829/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1686 - val_loss: 0.1790\n",
      "Epoch 830/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1674 - val_loss: 0.1786\n",
      "Epoch 831/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1788\n",
      "Epoch 832/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1684 - val_loss: 0.1793\n",
      "Epoch 833/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1686 - val_loss: 0.1787\n",
      "Epoch 834/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1689 - val_loss: 0.1787\n",
      "Epoch 835/2000\n",
      "1207321/1207321 [==============================] - 13s 11us/step - loss: 0.1686 - val_loss: 0.1787\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00835: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7222734542428406\n",
      "Training 3JHN out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 133228 samples, validate on 33187 samples\n",
      "Epoch 1/2000\n",
      "133228/133228 [==============================] - 2s 16us/step - loss: 0.6172 - val_loss: 0.4016\n",
      "Epoch 2/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.3654 - val_loss: 0.3250\n",
      "Epoch 3/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.3107 - val_loss: 0.2820\n",
      "Epoch 4/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.2763 - val_loss: 0.2764\n",
      "Epoch 5/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.2495 - val_loss: 0.2419\n",
      "Epoch 6/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.2357 - val_loss: 0.2256\n",
      "Epoch 7/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.2225 - val_loss: 0.2166\n",
      "Epoch 8/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.2134 - val_loss: 0.2027\n",
      "Epoch 9/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.2040 - val_loss: 0.2126\n",
      "Epoch 10/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.2018 - val_loss: 0.2294\n",
      "Epoch 11/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1943 - val_loss: 0.2074\n",
      "Epoch 12/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1863 - val_loss: 0.1982\n",
      "Epoch 13/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.1864 - val_loss: 0.2206\n",
      "Epoch 14/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.1778 - val_loss: 0.2163\n",
      "Epoch 15/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1742 - val_loss: 0.1781\n",
      "Epoch 16/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1696 - val_loss: 0.1948\n",
      "Epoch 17/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1722 - val_loss: 0.1740\n",
      "Epoch 18/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1632 - val_loss: 0.1660\n",
      "Epoch 19/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1583 - val_loss: 0.1738\n",
      "Epoch 20/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.1593 - val_loss: 0.1667\n",
      "Epoch 21/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1549 - val_loss: 0.1604\n",
      "Epoch 22/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1548 - val_loss: 0.1633\n",
      "Epoch 23/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1517 - val_loss: 0.1590\n",
      "Epoch 24/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1470 - val_loss: 0.1622\n",
      "Epoch 25/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1451 - val_loss: 0.1515\n",
      "Epoch 26/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1478 - val_loss: 0.1628\n",
      "Epoch 27/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1428 - val_loss: 0.1534\n",
      "Epoch 28/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1438 - val_loss: 0.1564\n",
      "Epoch 29/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1454 - val_loss: 0.1529\n",
      "Epoch 30/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1398 - val_loss: 0.1588\n",
      "Epoch 31/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.1380 - val_loss: 0.1576\n",
      "Epoch 32/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1356 - val_loss: 0.1476\n",
      "Epoch 33/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1355 - val_loss: 0.1501\n",
      "Epoch 34/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1344 - val_loss: 0.1468\n",
      "Epoch 35/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1337 - val_loss: 0.1421\n",
      "Epoch 36/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1279 - val_loss: 0.1426\n",
      "Epoch 37/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1299 - val_loss: 0.1442\n",
      "Epoch 38/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1280 - val_loss: 0.1374\n",
      "Epoch 39/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1290 - val_loss: 0.1514\n",
      "Epoch 40/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1257 - val_loss: 0.1388\n",
      "Epoch 41/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1238 - val_loss: 0.1485\n",
      "Epoch 42/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1254 - val_loss: 0.1484\n",
      "Epoch 43/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1304 - val_loss: 0.1568\n",
      "Epoch 44/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1283 - val_loss: 0.1458\n",
      "Epoch 45/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1255 - val_loss: 0.1462\n",
      "Epoch 46/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1199 - val_loss: 0.1351\n",
      "Epoch 47/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1195 - val_loss: 0.1414\n",
      "Epoch 48/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1221 - val_loss: 0.1413\n",
      "Epoch 49/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1218 - val_loss: 0.1363\n",
      "Epoch 50/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1195 - val_loss: 0.1374\n",
      "Epoch 51/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1160 - val_loss: 0.1366\n",
      "Epoch 52/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1169 - val_loss: 0.1404\n",
      "Epoch 53/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1197 - val_loss: 0.1349\n",
      "Epoch 54/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1207 - val_loss: 0.1330\n",
      "Epoch 55/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1164 - val_loss: 0.1320\n",
      "Epoch 56/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1152 - val_loss: 0.1321\n",
      "Epoch 57/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1121 - val_loss: 0.1310\n",
      "Epoch 58/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1103 - val_loss: 0.1278\n",
      "Epoch 59/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1138 - val_loss: 0.1318\n",
      "Epoch 60/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1112 - val_loss: 0.1366\n",
      "Epoch 61/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1150 - val_loss: 0.1305\n",
      "Epoch 62/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1098 - val_loss: 0.1248\n",
      "Epoch 63/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1098 - val_loss: 0.1285\n",
      "Epoch 64/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1078 - val_loss: 0.1222\n",
      "Epoch 65/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1077 - val_loss: 0.1248\n",
      "Epoch 66/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1067 - val_loss: 0.1224\n",
      "Epoch 67/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1080 - val_loss: 0.1307\n",
      "Epoch 68/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1074 - val_loss: 0.1242\n",
      "Epoch 69/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1071 - val_loss: 0.1242\n",
      "Epoch 70/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1074 - val_loss: 0.1336\n",
      "Epoch 71/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1048 - val_loss: 0.1201\n",
      "Epoch 72/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1064 - val_loss: 0.1251\n",
      "Epoch 73/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1047 - val_loss: 0.1245\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1065 - val_loss: 0.1358\n",
      "Epoch 75/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1085 - val_loss: 0.1289\n",
      "Epoch 76/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1043 - val_loss: 0.1273\n",
      "Epoch 77/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1040 - val_loss: 0.1244\n",
      "Epoch 78/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1042 - val_loss: 0.1284\n",
      "Epoch 79/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1027 - val_loss: 0.1200\n",
      "Epoch 80/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1004 - val_loss: 0.1228\n",
      "Epoch 81/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1009 - val_loss: 0.1240\n",
      "Epoch 82/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0991 - val_loss: 0.1242\n",
      "Epoch 83/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.1010 - val_loss: 0.1225\n",
      "Epoch 84/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0989 - val_loss: 0.1205\n",
      "Epoch 85/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0972 - val_loss: 0.1271\n",
      "Epoch 86/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1002 - val_loss: 0.1179\n",
      "Epoch 87/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.1018 - val_loss: 0.1236\n",
      "Epoch 88/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0972 - val_loss: 0.1178\n",
      "Epoch 89/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0960 - val_loss: 0.1230\n",
      "Epoch 90/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0996 - val_loss: 0.1190\n",
      "Epoch 91/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0961 - val_loss: 0.1226\n",
      "Epoch 92/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0953 - val_loss: 0.1179\n",
      "Epoch 93/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0956 - val_loss: 0.1193\n",
      "Epoch 94/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0964 - val_loss: 0.1181\n",
      "Epoch 95/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0919 - val_loss: 0.1172\n",
      "Epoch 96/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0931 - val_loss: 0.1240\n",
      "Epoch 97/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0984 - val_loss: 0.1198\n",
      "Epoch 98/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0956 - val_loss: 0.1246\n",
      "Epoch 99/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0951 - val_loss: 0.1141\n",
      "Epoch 100/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0948 - val_loss: 0.1167\n",
      "Epoch 101/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0927 - val_loss: 0.1215\n",
      "Epoch 102/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0928 - val_loss: 0.1209\n",
      "Epoch 103/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0953 - val_loss: 0.1198\n",
      "Epoch 104/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0958 - val_loss: 0.1216\n",
      "Epoch 105/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0909 - val_loss: 0.1205\n",
      "Epoch 106/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0919 - val_loss: 0.1121\n",
      "Epoch 107/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0928 - val_loss: 0.1188\n",
      "Epoch 108/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0921 - val_loss: 0.1186\n",
      "Epoch 109/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0961 - val_loss: 0.1150\n",
      "Epoch 110/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0978 - val_loss: 0.1164\n",
      "Epoch 111/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0911 - val_loss: 0.1126\n",
      "Epoch 112/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0890 - val_loss: 0.1148\n",
      "Epoch 113/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0957 - val_loss: 0.1127\n",
      "Epoch 114/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0901 - val_loss: 0.1183\n",
      "Epoch 115/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0901 - val_loss: 0.1170\n",
      "Epoch 116/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0907 - val_loss: 0.1145\n",
      "Epoch 117/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0915 - val_loss: 0.1129\n",
      "Epoch 118/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0889 - val_loss: 0.1144\n",
      "Epoch 119/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0868 - val_loss: 0.1104\n",
      "Epoch 120/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0890 - val_loss: 0.1111\n",
      "Epoch 121/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0866 - val_loss: 0.1124\n",
      "Epoch 122/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0877 - val_loss: 0.1136\n",
      "Epoch 123/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0891 - val_loss: 0.1112\n",
      "Epoch 124/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0859 - val_loss: 0.1158\n",
      "Epoch 125/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0912 - val_loss: 0.1120\n",
      "Epoch 126/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0868 - val_loss: 0.1131\n",
      "Epoch 127/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0860 - val_loss: 0.1117\n",
      "Epoch 128/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0850 - val_loss: 0.1099\n",
      "Epoch 129/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0855 - val_loss: 0.1112\n",
      "Epoch 130/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0874 - val_loss: 0.1110\n",
      "Epoch 131/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0857 - val_loss: 0.1158\n",
      "Epoch 132/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0932 - val_loss: 0.1150\n",
      "Epoch 133/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0890 - val_loss: 0.1123\n",
      "Epoch 134/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0879 - val_loss: 0.1140\n",
      "Epoch 135/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0857 - val_loss: 0.1095\n",
      "Epoch 136/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0860 - val_loss: 0.1080\n",
      "Epoch 137/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0882 - val_loss: 0.1137\n",
      "Epoch 138/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0884 - val_loss: 0.1162\n",
      "Epoch 139/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0874 - val_loss: 0.1294\n",
      "Epoch 140/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0853 - val_loss: 0.1081\n",
      "Epoch 141/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0863 - val_loss: 0.1119\n",
      "Epoch 142/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0862 - val_loss: 0.1112\n",
      "Epoch 143/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0853 - val_loss: 0.1070\n",
      "Epoch 144/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0857 - val_loss: 0.1139\n",
      "Epoch 145/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0869 - val_loss: 0.1089\n",
      "Epoch 146/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0858 - val_loss: 0.1075\n",
      "Epoch 147/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0816 - val_loss: 0.1070\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0855 - val_loss: 0.1125\n",
      "Epoch 149/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0852 - val_loss: 0.1078\n",
      "Epoch 150/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0818 - val_loss: 0.1118\n",
      "Epoch 151/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0854 - val_loss: 0.1092\n",
      "Epoch 152/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0836 - val_loss: 0.1083\n",
      "Epoch 153/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0834 - val_loss: 0.1059\n",
      "Epoch 154/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0805 - val_loss: 0.1091\n",
      "Epoch 155/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0846 - val_loss: 0.1127\n",
      "Epoch 156/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0836 - val_loss: 0.1120\n",
      "Epoch 157/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0817 - val_loss: 0.1085\n",
      "Epoch 158/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0810 - val_loss: 0.1086\n",
      "Epoch 159/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0822 - val_loss: 0.1152\n",
      "Epoch 160/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0804 - val_loss: 0.1068\n",
      "Epoch 161/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0806 - val_loss: 0.1049\n",
      "Epoch 162/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0807 - val_loss: 0.1077\n",
      "Epoch 163/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0834 - val_loss: 0.1071\n",
      "Epoch 164/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0815 - val_loss: 0.1112\n",
      "Epoch 165/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0805 - val_loss: 0.1085\n",
      "Epoch 166/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0807 - val_loss: 0.1078\n",
      "Epoch 167/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0804 - val_loss: 0.1042\n",
      "Epoch 168/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0810 - val_loss: 0.1095\n",
      "Epoch 169/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0812 - val_loss: 0.1078\n",
      "Epoch 170/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0797 - val_loss: 0.1101\n",
      "Epoch 171/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0829 - val_loss: 0.1113\n",
      "Epoch 172/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0817 - val_loss: 0.1076\n",
      "Epoch 173/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0817 - val_loss: 0.1072\n",
      "Epoch 174/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0803 - val_loss: 0.1068\n",
      "Epoch 175/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0806 - val_loss: 0.1040\n",
      "Epoch 176/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0820 - val_loss: 0.1100\n",
      "Epoch 177/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0792 - val_loss: 0.1052\n",
      "Epoch 178/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0790 - val_loss: 0.1052\n",
      "Epoch 179/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0797 - val_loss: 0.1045\n",
      "Epoch 180/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0780 - val_loss: 0.1043\n",
      "Epoch 181/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0795 - val_loss: 0.1045\n",
      "Epoch 182/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0767 - val_loss: 0.1039\n",
      "Epoch 183/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0775 - val_loss: 0.1089\n",
      "Epoch 184/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0813 - val_loss: 0.1058\n",
      "Epoch 185/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0782 - val_loss: 0.1105\n",
      "Epoch 186/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0769 - val_loss: 0.1064\n",
      "Epoch 187/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0806 - val_loss: 0.1050\n",
      "Epoch 188/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0779 - val_loss: 0.1042\n",
      "Epoch 189/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0761 - val_loss: 0.1077\n",
      "Epoch 190/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0760 - val_loss: 0.1030\n",
      "Epoch 191/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0768 - val_loss: 0.1054\n",
      "Epoch 192/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0757 - val_loss: 0.1043\n",
      "Epoch 193/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0819 - val_loss: 0.1078\n",
      "Epoch 194/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0777 - val_loss: 0.1056\n",
      "Epoch 195/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0772 - val_loss: 0.1040\n",
      "Epoch 196/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0813 - val_loss: 0.1059\n",
      "Epoch 197/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0755 - val_loss: 0.1044\n",
      "Epoch 198/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0774 - val_loss: 0.1039\n",
      "Epoch 199/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0754 - val_loss: 0.1061\n",
      "Epoch 200/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0761 - val_loss: 0.1034\n",
      "Epoch 201/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0776 - val_loss: 0.1028\n",
      "Epoch 202/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0769 - val_loss: 0.1022\n",
      "Epoch 203/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0752 - val_loss: 0.1039\n",
      "Epoch 204/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0776 - val_loss: 0.1050\n",
      "Epoch 205/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0801 - val_loss: 0.1038\n",
      "Epoch 206/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0757 - val_loss: 0.1064\n",
      "Epoch 207/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0775 - val_loss: 0.1071\n",
      "Epoch 208/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0790 - val_loss: 0.1084\n",
      "Epoch 209/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0773 - val_loss: 0.1028\n",
      "Epoch 210/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0738 - val_loss: 0.1037\n",
      "Epoch 211/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0758 - val_loss: 0.1030\n",
      "Epoch 212/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0758 - val_loss: 0.1048\n",
      "Epoch 213/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0763 - val_loss: 0.1017\n",
      "Epoch 214/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0764 - val_loss: 0.1014\n",
      "Epoch 215/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0754 - val_loss: 0.1052\n",
      "Epoch 216/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0747 - val_loss: 0.1046\n",
      "Epoch 217/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0746 - val_loss: 0.1126\n",
      "Epoch 218/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0754 - val_loss: 0.1049\n",
      "Epoch 219/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0755 - val_loss: 0.1035\n",
      "Epoch 220/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0773 - val_loss: 0.1030\n",
      "Epoch 221/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0747 - val_loss: 0.1056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0738 - val_loss: 0.1070\n",
      "Epoch 223/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0760 - val_loss: 0.1018\n",
      "Epoch 224/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0724 - val_loss: 0.1012\n",
      "Epoch 225/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0783 - val_loss: 0.1040\n",
      "Epoch 226/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0740 - val_loss: 0.1023\n",
      "Epoch 227/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0744 - val_loss: 0.1040\n",
      "Epoch 228/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0787 - val_loss: 0.1029\n",
      "Epoch 229/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0754 - val_loss: 0.1010\n",
      "Epoch 230/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0731 - val_loss: 0.1088\n",
      "Epoch 231/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0751 - val_loss: 0.1095\n",
      "Epoch 232/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0724 - val_loss: 0.1014\n",
      "Epoch 233/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0736 - val_loss: 0.1020\n",
      "Epoch 234/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0745 - val_loss: 0.1038\n",
      "Epoch 235/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0754 - val_loss: 0.1049\n",
      "Epoch 236/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0745 - val_loss: 0.1031\n",
      "Epoch 237/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0760 - val_loss: 0.1038\n",
      "Epoch 238/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0739 - val_loss: 0.1029\n",
      "Epoch 239/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0723 - val_loss: 0.1085\n",
      "Epoch 240/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0753 - val_loss: 0.1034\n",
      "Epoch 241/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0716 - val_loss: 0.1027\n",
      "Epoch 242/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0720 - val_loss: 0.1033\n",
      "Epoch 243/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0768 - val_loss: 0.1059\n",
      "Epoch 244/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0742 - val_loss: 0.1005\n",
      "Epoch 245/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0725 - val_loss: 0.1040\n",
      "Epoch 246/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0731 - val_loss: 0.1041\n",
      "Epoch 247/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0732 - val_loss: 0.1025\n",
      "Epoch 248/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0724 - val_loss: 0.1066\n",
      "Epoch 249/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0746 - val_loss: 0.1030\n",
      "Epoch 250/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0743 - val_loss: 0.1006\n",
      "Epoch 251/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0718 - val_loss: 0.1068\n",
      "Epoch 252/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0738 - val_loss: 0.1030\n",
      "Epoch 253/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0722 - val_loss: 0.1003\n",
      "Epoch 254/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0736 - val_loss: 0.1026\n",
      "Epoch 255/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0721 - val_loss: 0.1019\n",
      "Epoch 256/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0717 - val_loss: 0.1022\n",
      "Epoch 257/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0749 - val_loss: 0.1041\n",
      "Epoch 258/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0732 - val_loss: 0.1020\n",
      "Epoch 259/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0762 - val_loss: 0.1036\n",
      "Epoch 260/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0731 - val_loss: 0.1106\n",
      "Epoch 261/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0707 - val_loss: 0.1008\n",
      "Epoch 262/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0709 - val_loss: 0.1114\n",
      "Epoch 263/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0734 - val_loss: 0.1022\n",
      "Epoch 264/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0721 - val_loss: 0.0994\n",
      "Epoch 265/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0729 - val_loss: 0.0995\n",
      "Epoch 266/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0718 - val_loss: 0.1006\n",
      "Epoch 267/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0710 - val_loss: 0.1024\n",
      "Epoch 268/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0708 - val_loss: 0.1022\n",
      "Epoch 269/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0710 - val_loss: 0.1004\n",
      "Epoch 270/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0712 - val_loss: 0.1001\n",
      "Epoch 271/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0715 - val_loss: 0.1015\n",
      "Epoch 272/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0695 - val_loss: 0.1005\n",
      "Epoch 273/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0749 - val_loss: 0.1001\n",
      "Epoch 274/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0732 - val_loss: 0.1006\n",
      "Epoch 275/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0745 - val_loss: 0.1039\n",
      "Epoch 276/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0722 - val_loss: 0.1005\n",
      "Epoch 277/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0735 - val_loss: 0.1013\n",
      "Epoch 278/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0751 - val_loss: 0.1011\n",
      "Epoch 279/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0735 - val_loss: 0.1047\n",
      "Epoch 280/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0697 - val_loss: 0.1044\n",
      "Epoch 281/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0695 - val_loss: 0.1006\n",
      "Epoch 282/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0705 - val_loss: 0.1026\n",
      "Epoch 283/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0706 - val_loss: 0.0982\n",
      "Epoch 284/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0715 - val_loss: 0.1038\n",
      "Epoch 285/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0723 - val_loss: 0.1050\n",
      "Epoch 286/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0700 - val_loss: 0.1008\n",
      "Epoch 287/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0701 - val_loss: 0.1037\n",
      "Epoch 288/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0693 - val_loss: 0.0992\n",
      "Epoch 289/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0677 - val_loss: 0.0991\n",
      "Epoch 290/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0695 - val_loss: 0.1019\n",
      "Epoch 291/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0723 - val_loss: 0.1011\n",
      "Epoch 292/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0705 - val_loss: 0.0984\n",
      "Epoch 293/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0691 - val_loss: 0.1015\n",
      "Epoch 294/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0695 - val_loss: 0.0975\n",
      "Epoch 295/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0663 - val_loss: 0.0983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0711 - val_loss: 0.0989\n",
      "Epoch 297/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0702 - val_loss: 0.1099\n",
      "Epoch 298/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0713 - val_loss: 0.0977\n",
      "Epoch 299/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0693 - val_loss: 0.0988\n",
      "Epoch 300/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0687 - val_loss: 0.0985\n",
      "Epoch 301/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0729 - val_loss: 0.1013\n",
      "Epoch 302/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0707 - val_loss: 0.0988\n",
      "Epoch 303/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0682 - val_loss: 0.1004\n",
      "Epoch 304/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0690 - val_loss: 0.0994\n",
      "Epoch 305/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0705 - val_loss: 0.1004\n",
      "Epoch 306/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0717 - val_loss: 0.0995\n",
      "Epoch 307/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0688 - val_loss: 0.1024\n",
      "Epoch 308/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0682 - val_loss: 0.0988\n",
      "Epoch 309/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0693 - val_loss: 0.1018\n",
      "Epoch 310/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0677 - val_loss: 0.1002\n",
      "Epoch 311/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0680 - val_loss: 0.1007\n",
      "Epoch 312/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0684 - val_loss: 0.1025\n",
      "Epoch 313/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0684 - val_loss: 0.1010\n",
      "Epoch 314/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0681 - val_loss: 0.0996\n",
      "Epoch 315/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0681 - val_loss: 0.0998\n",
      "Epoch 316/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0687 - val_loss: 0.0981\n",
      "Epoch 317/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0676 - val_loss: 0.1027\n",
      "Epoch 318/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0692 - val_loss: 0.1008\n",
      "Epoch 319/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0686 - val_loss: 0.1005\n",
      "Epoch 320/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0705 - val_loss: 0.1010\n",
      "Epoch 321/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0675 - val_loss: 0.0996\n",
      "Epoch 322/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0693 - val_loss: 0.0965\n",
      "Epoch 323/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0704 - val_loss: 0.0971\n",
      "Epoch 324/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0677 - val_loss: 0.0974\n",
      "Epoch 325/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0663 - val_loss: 0.1001\n",
      "Epoch 326/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0709 - val_loss: 0.1003\n",
      "Epoch 327/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0702 - val_loss: 0.0996\n",
      "Epoch 328/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0641 - val_loss: 0.0985\n",
      "Epoch 329/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0663 - val_loss: 0.0977\n",
      "Epoch 330/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0692 - val_loss: 0.0987\n",
      "Epoch 331/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0655 - val_loss: 0.0986\n",
      "Epoch 332/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0703 - val_loss: 0.1002\n",
      "Epoch 333/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0676 - val_loss: 0.1025\n",
      "Epoch 334/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0684 - val_loss: 0.0968\n",
      "Epoch 335/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0662 - val_loss: 0.0975\n",
      "Epoch 336/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0681 - val_loss: 0.1032\n",
      "Epoch 337/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0672 - val_loss: 0.0989\n",
      "Epoch 338/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0660 - val_loss: 0.0963\n",
      "Epoch 339/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0654 - val_loss: 0.0976\n",
      "Epoch 340/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0688 - val_loss: 0.0980\n",
      "Epoch 341/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0658 - val_loss: 0.0997\n",
      "Epoch 342/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0707 - val_loss: 0.0979\n",
      "Epoch 343/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0685 - val_loss: 0.1015\n",
      "Epoch 344/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0693 - val_loss: 0.0982\n",
      "Epoch 345/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0678 - val_loss: 0.0987\n",
      "Epoch 346/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0664 - val_loss: 0.1018\n",
      "Epoch 347/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0690 - val_loss: 0.0978\n",
      "Epoch 348/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0682 - val_loss: 0.1050\n",
      "Epoch 349/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0682 - val_loss: 0.1022\n",
      "Epoch 350/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0652 - val_loss: 0.1003\n",
      "Epoch 351/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0677 - val_loss: 0.0983\n",
      "Epoch 352/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0685 - val_loss: 0.1006\n",
      "Epoch 353/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0687 - val_loss: 0.0965\n",
      "Epoch 354/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0675 - val_loss: 0.0978\n",
      "Epoch 355/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0668 - val_loss: 0.0985\n",
      "Epoch 356/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0662 - val_loss: 0.0982\n",
      "Epoch 357/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0683 - val_loss: 0.0991\n",
      "Epoch 358/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0648 - val_loss: 0.0990\n",
      "Epoch 359/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0684 - val_loss: 0.1056\n",
      "Epoch 360/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0666 - val_loss: 0.1012\n",
      "Epoch 361/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0676 - val_loss: 0.1005\n",
      "Epoch 362/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0676 - val_loss: 0.0987\n",
      "Epoch 363/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0662 - val_loss: 0.1006\n",
      "Epoch 364/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0681 - val_loss: 0.0999\n",
      "Epoch 365/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0679 - val_loss: 0.0971\n",
      "Epoch 366/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0679 - val_loss: 0.1004\n",
      "Epoch 367/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0653 - val_loss: 0.1003\n",
      "Epoch 368/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0651 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00368: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 369/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0602 - val_loss: 0.0948\n",
      "Epoch 370/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0621 - val_loss: 0.0943\n",
      "Epoch 371/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0608 - val_loss: 0.0959\n",
      "Epoch 372/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0592 - val_loss: 0.0950\n",
      "Epoch 373/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0592 - val_loss: 0.0943\n",
      "Epoch 374/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0585 - val_loss: 0.0940\n",
      "Epoch 375/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0601 - val_loss: 0.0939\n",
      "Epoch 376/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0579 - val_loss: 0.0927\n",
      "Epoch 377/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0586 - val_loss: 0.0939\n",
      "Epoch 378/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0595 - val_loss: 0.0958\n",
      "Epoch 379/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0615 - val_loss: 0.0940\n",
      "Epoch 380/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0575 - val_loss: 0.0924\n",
      "Epoch 381/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.0934\n",
      "Epoch 382/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0571 - val_loss: 0.0930\n",
      "Epoch 383/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.0943\n",
      "Epoch 384/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0604 - val_loss: 0.0944\n",
      "Epoch 385/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0611 - val_loss: 0.0932\n",
      "Epoch 386/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0580 - val_loss: 0.0922\n",
      "Epoch 387/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0575 - val_loss: 0.0926\n",
      "Epoch 388/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0576 - val_loss: 0.0929\n",
      "Epoch 389/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0590 - val_loss: 0.0934\n",
      "Epoch 390/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0596 - val_loss: 0.0933\n",
      "Epoch 391/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0572 - val_loss: 0.0941\n",
      "Epoch 392/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0600 - val_loss: 0.0933\n",
      "Epoch 393/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0590 - val_loss: 0.0955\n",
      "Epoch 394/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0579 - val_loss: 0.0943\n",
      "Epoch 395/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0589 - val_loss: 0.0932\n",
      "Epoch 396/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0596 - val_loss: 0.0921\n",
      "Epoch 397/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0574 - val_loss: 0.0943\n",
      "Epoch 398/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0576 - val_loss: 0.0949\n",
      "Epoch 399/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0576 - val_loss: 0.0931\n",
      "Epoch 400/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0567 - val_loss: 0.0931\n",
      "Epoch 401/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0576 - val_loss: 0.0922\n",
      "Epoch 402/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0587 - val_loss: 0.0933\n",
      "Epoch 403/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0598 - val_loss: 0.0946\n",
      "Epoch 404/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0608 - val_loss: 0.0939\n",
      "Epoch 405/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0602 - val_loss: 0.0936\n",
      "Epoch 406/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0593 - val_loss: 0.0931\n",
      "Epoch 407/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0584 - val_loss: 0.0969\n",
      "Epoch 408/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.0921\n",
      "Epoch 409/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.0928\n",
      "Epoch 410/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0599 - val_loss: 0.0922\n",
      "Epoch 411/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0578 - val_loss: 0.0938\n",
      "Epoch 412/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.0933\n",
      "Epoch 413/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0596 - val_loss: 0.0941\n",
      "Epoch 414/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0560 - val_loss: 0.0955\n",
      "Epoch 415/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0589 - val_loss: 0.0936\n",
      "Epoch 416/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0592 - val_loss: 0.0955\n",
      "Epoch 417/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0577 - val_loss: 0.0921\n",
      "Epoch 418/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.0923\n",
      "Epoch 419/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0584 - val_loss: 0.1024\n",
      "Epoch 420/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0570 - val_loss: 0.0935\n",
      "Epoch 421/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0577 - val_loss: 0.0932\n",
      "Epoch 422/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0594 - val_loss: 0.0924\n",
      "Epoch 423/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0583 - val_loss: 0.0919\n",
      "Epoch 424/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0568 - val_loss: 0.0930\n",
      "Epoch 425/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0606 - val_loss: 0.0932\n",
      "Epoch 426/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0569 - val_loss: 0.0917\n",
      "Epoch 427/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0569 - val_loss: 0.0945\n",
      "Epoch 428/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0563 - val_loss: 0.0927\n",
      "Epoch 429/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0611 - val_loss: 0.0927\n",
      "Epoch 430/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0601 - val_loss: 0.0920\n",
      "Epoch 431/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0588 - val_loss: 0.1035\n",
      "Epoch 432/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0562 - val_loss: 0.0922\n",
      "Epoch 433/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0579 - val_loss: 0.0936\n",
      "Epoch 434/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0567 - val_loss: 0.0928\n",
      "Epoch 435/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0582 - val_loss: 0.0943\n",
      "Epoch 436/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0583 - val_loss: 0.0934\n",
      "Epoch 437/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0566 - val_loss: 0.0917\n",
      "Epoch 438/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0562 - val_loss: 0.0927\n",
      "Epoch 439/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0572 - val_loss: 0.0933\n",
      "Epoch 440/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0589 - val_loss: 0.0921\n",
      "Epoch 441/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0566 - val_loss: 0.0941\n",
      "Epoch 442/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0582 - val_loss: 0.0935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0572 - val_loss: 0.0917\n",
      "Epoch 444/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0570 - val_loss: 0.0930\n",
      "Epoch 445/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0587 - val_loss: 0.0929\n",
      "Epoch 446/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0580 - val_loss: 0.0934\n",
      "Epoch 447/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0556 - val_loss: 0.0919\n",
      "Epoch 448/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0581 - val_loss: 0.0923\n",
      "Epoch 449/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0587 - val_loss: 0.0918\n",
      "Epoch 450/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0568 - val_loss: 0.0975\n",
      "Epoch 451/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0589 - val_loss: 0.0937\n",
      "Epoch 452/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0548 - val_loss: 0.0930\n",
      "Epoch 453/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0579 - val_loss: 0.0946\n",
      "Epoch 454/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0574 - val_loss: 0.0944\n",
      "Epoch 455/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0585 - val_loss: 0.0919\n",
      "Epoch 456/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0603 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00456: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 457/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0540 - val_loss: 0.0913\n",
      "Epoch 458/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0535 - val_loss: 0.0904\n",
      "Epoch 459/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0555 - val_loss: 0.0915\n",
      "Epoch 460/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0531 - val_loss: 0.0902\n",
      "Epoch 461/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0561 - val_loss: 0.0909\n",
      "Epoch 462/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0545 - val_loss: 0.0902\n",
      "Epoch 463/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0905\n",
      "Epoch 464/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0552 - val_loss: 0.0909\n",
      "Epoch 465/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0903\n",
      "Epoch 466/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0542 - val_loss: 0.0902\n",
      "Epoch 467/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0569 - val_loss: 0.0909\n",
      "Epoch 468/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0536 - val_loss: 0.0922\n",
      "Epoch 469/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0547 - val_loss: 0.0913\n",
      "Epoch 470/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0553 - val_loss: 0.0901\n",
      "Epoch 471/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0532 - val_loss: 0.0903\n",
      "Epoch 472/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0542 - val_loss: 0.0917\n",
      "Epoch 473/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0527 - val_loss: 0.0903\n",
      "Epoch 474/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0900\n",
      "Epoch 475/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0904\n",
      "Epoch 476/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0905\n",
      "Epoch 477/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0536 - val_loss: 0.0908\n",
      "Epoch 478/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0898\n",
      "Epoch 479/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0541 - val_loss: 0.0909\n",
      "Epoch 480/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0522 - val_loss: 0.0900\n",
      "Epoch 481/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0903\n",
      "Epoch 482/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0907\n",
      "Epoch 483/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0528 - val_loss: 0.0900\n",
      "Epoch 484/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0543 - val_loss: 0.0913\n",
      "Epoch 485/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0896\n",
      "Epoch 486/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0533 - val_loss: 0.0903\n",
      "Epoch 487/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0549 - val_loss: 0.0901\n",
      "Epoch 488/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0540 - val_loss: 0.0905\n",
      "Epoch 489/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0547 - val_loss: 0.0904\n",
      "Epoch 490/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0530 - val_loss: 0.0904\n",
      "Epoch 491/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0900\n",
      "Epoch 492/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0547 - val_loss: 0.0899\n",
      "Epoch 493/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0545 - val_loss: 0.0899\n",
      "Epoch 494/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0539 - val_loss: 0.0906\n",
      "Epoch 495/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0900\n",
      "Epoch 496/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0539 - val_loss: 0.0905\n",
      "Epoch 497/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0550 - val_loss: 0.0909\n",
      "Epoch 498/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0530 - val_loss: 0.0906\n",
      "Epoch 499/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0537 - val_loss: 0.0915\n",
      "Epoch 500/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0541 - val_loss: 0.0895\n",
      "Epoch 501/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0902\n",
      "Epoch 502/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0505 - val_loss: 0.0900\n",
      "Epoch 503/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0553 - val_loss: 0.0904\n",
      "Epoch 504/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0535 - val_loss: 0.0897\n",
      "Epoch 505/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0536 - val_loss: 0.0916\n",
      "Epoch 506/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0534 - val_loss: 0.0903\n",
      "Epoch 507/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0564 - val_loss: 0.0905\n",
      "Epoch 508/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0897\n",
      "Epoch 509/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0526 - val_loss: 0.0909\n",
      "Epoch 510/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0536 - val_loss: 0.0900\n",
      "Epoch 511/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0533 - val_loss: 0.0899\n",
      "Epoch 512/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0535 - val_loss: 0.0901\n",
      "Epoch 513/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0553 - val_loss: 0.0901\n",
      "Epoch 514/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0907\n",
      "Epoch 515/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0897\n",
      "Epoch 516/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0554 - val_loss: 0.0907\n",
      "Epoch 517/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0903\n",
      "Epoch 518/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0516 - val_loss: 0.0897\n",
      "Epoch 519/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0905\n",
      "Epoch 520/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0542 - val_loss: 0.0904\n",
      "Epoch 521/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0526 - val_loss: 0.0908\n",
      "Epoch 522/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0505 - val_loss: 0.0906\n",
      "Epoch 523/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0527 - val_loss: 0.0902\n",
      "Epoch 524/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0545 - val_loss: 0.0898\n",
      "Epoch 525/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0892\n",
      "Epoch 526/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0534 - val_loss: 0.0904\n",
      "Epoch 527/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0526 - val_loss: 0.0894\n",
      "Epoch 528/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0902\n",
      "Epoch 529/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0533 - val_loss: 0.0896\n",
      "Epoch 530/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0515 - val_loss: 0.0902\n",
      "Epoch 531/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0555 - val_loss: 0.0898\n",
      "Epoch 532/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0548 - val_loss: 0.0906\n",
      "Epoch 533/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0533 - val_loss: 0.0900\n",
      "Epoch 534/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0531 - val_loss: 0.0909\n",
      "Epoch 535/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0539 - val_loss: 0.0900\n",
      "Epoch 536/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0905\n",
      "Epoch 537/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0896\n",
      "Epoch 538/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0526 - val_loss: 0.0903\n",
      "Epoch 539/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0530 - val_loss: 0.0949\n",
      "Epoch 540/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0539 - val_loss: 0.0908\n",
      "Epoch 541/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0898\n",
      "Epoch 542/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0538 - val_loss: 0.0909\n",
      "Epoch 543/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0541 - val_loss: 0.0908\n",
      "Epoch 544/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0900\n",
      "Epoch 545/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0521 - val_loss: 0.0897\n",
      "Epoch 546/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0523 - val_loss: 0.0899\n",
      "Epoch 547/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0552 - val_loss: 0.0896\n",
      "Epoch 548/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0540 - val_loss: 0.0903\n",
      "Epoch 549/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0507 - val_loss: 0.0926\n",
      "Epoch 550/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0532 - val_loss: 0.0903\n",
      "Epoch 551/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0517 - val_loss: 0.0902\n",
      "Epoch 552/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0541 - val_loss: 0.0907\n",
      "Epoch 553/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0532 - val_loss: 0.0898\n",
      "Epoch 554/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0540 - val_loss: 0.0906\n",
      "Epoch 555/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0541 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00555: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 556/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0543 - val_loss: 0.0891\n",
      "Epoch 557/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0504 - val_loss: 0.0890\n",
      "Epoch 558/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0515 - val_loss: 0.0889\n",
      "Epoch 559/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0542 - val_loss: 0.0894\n",
      "Epoch 560/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0890\n",
      "Epoch 561/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0513 - val_loss: 0.0889\n",
      "Epoch 562/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0523 - val_loss: 0.0893\n",
      "Epoch 563/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0538 - val_loss: 0.0905\n",
      "Epoch 564/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0522 - val_loss: 0.0892\n",
      "Epoch 565/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0510 - val_loss: 0.0893\n",
      "Epoch 566/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0895\n",
      "Epoch 567/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0890\n",
      "Epoch 568/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0526 - val_loss: 0.0890\n",
      "Epoch 569/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0520 - val_loss: 0.0892\n",
      "Epoch 570/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0511 - val_loss: 0.0889\n",
      "Epoch 571/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0889\n",
      "Epoch 572/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0515 - val_loss: 0.0887\n",
      "Epoch 573/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0486 - val_loss: 0.0890\n",
      "Epoch 574/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0516 - val_loss: 0.0888\n",
      "Epoch 575/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0493 - val_loss: 0.0892\n",
      "Epoch 576/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0527 - val_loss: 0.0889\n",
      "Epoch 577/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0489 - val_loss: 0.0892\n",
      "Epoch 578/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0521 - val_loss: 0.0892\n",
      "Epoch 579/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0892\n",
      "Epoch 580/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0514 - val_loss: 0.0890\n",
      "Epoch 581/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0542 - val_loss: 0.0893\n",
      "Epoch 582/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0524 - val_loss: 0.0892\n",
      "Epoch 583/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0524 - val_loss: 0.0887\n",
      "Epoch 584/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0503 - val_loss: 0.0890\n",
      "Epoch 585/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0509 - val_loss: 0.0897\n",
      "Epoch 586/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0540 - val_loss: 0.0891\n",
      "Epoch 587/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0503 - val_loss: 0.0888\n",
      "Epoch 588/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0529 - val_loss: 0.0889\n",
      "Epoch 589/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0494 - val_loss: 0.0888\n",
      "Epoch 590/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0550 - val_loss: 0.0889\n",
      "Epoch 591/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0505 - val_loss: 0.0887\n",
      "Epoch 592/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0495 - val_loss: 0.0886\n",
      "Epoch 593/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0514 - val_loss: 0.0890\n",
      "Epoch 594/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0529 - val_loss: 0.0890\n",
      "Epoch 595/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0510 - val_loss: 0.0889\n",
      "Epoch 596/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0499 - val_loss: 0.0889\n",
      "Epoch 597/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0537 - val_loss: 0.0895\n",
      "Epoch 598/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0527 - val_loss: 0.0892\n",
      "Epoch 599/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0533 - val_loss: 0.0888\n",
      "Epoch 600/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0514 - val_loss: 0.0894\n",
      "Epoch 601/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0887\n",
      "Epoch 602/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0495 - val_loss: 0.0887\n",
      "Epoch 603/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0531 - val_loss: 0.0888\n",
      "Epoch 604/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0510 - val_loss: 0.0888\n",
      "Epoch 605/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0492 - val_loss: 0.0891\n",
      "Epoch 606/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0510 - val_loss: 0.0890\n",
      "Epoch 607/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0543 - val_loss: 0.0890\n",
      "Epoch 608/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0548 - val_loss: 0.0887\n",
      "Epoch 609/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0885\n",
      "Epoch 610/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0517 - val_loss: 0.0889\n",
      "Epoch 611/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0528 - val_loss: 0.0891\n",
      "Epoch 612/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0501 - val_loss: 0.0886\n",
      "Epoch 613/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0886\n",
      "Epoch 614/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0532 - val_loss: 0.0889\n",
      "Epoch 615/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0506 - val_loss: 0.0888\n",
      "Epoch 616/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0516 - val_loss: 0.0887\n",
      "Epoch 617/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0888\n",
      "Epoch 618/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0530 - val_loss: 0.0886\n",
      "Epoch 619/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0500 - val_loss: 0.0891\n",
      "Epoch 620/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0888\n",
      "Epoch 621/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0504 - val_loss: 0.0888\n",
      "Epoch 622/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0519 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00622: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 623/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0502 - val_loss: 0.0884\n",
      "Epoch 624/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0883\n",
      "Epoch 625/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0528 - val_loss: 0.0884\n",
      "Epoch 626/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0482 - val_loss: 0.0884\n",
      "Epoch 627/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0514 - val_loss: 0.0883\n",
      "Epoch 628/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0884\n",
      "Epoch 629/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0511 - val_loss: 0.0884\n",
      "Epoch 630/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0485 - val_loss: 0.0885\n",
      "Epoch 631/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0491 - val_loss: 0.0884\n",
      "Epoch 632/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0507 - val_loss: 0.0884\n",
      "Epoch 633/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0501 - val_loss: 0.0885\n",
      "Epoch 634/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0883\n",
      "Epoch 635/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0514 - val_loss: 0.0883\n",
      "Epoch 636/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0502 - val_loss: 0.0884\n",
      "Epoch 637/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0498 - val_loss: 0.0885\n",
      "Epoch 638/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0528 - val_loss: 0.0884\n",
      "Epoch 639/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0504 - val_loss: 0.0884\n",
      "Epoch 640/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0517 - val_loss: 0.0882\n",
      "Epoch 641/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0505 - val_loss: 0.0883\n",
      "Epoch 642/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0496 - val_loss: 0.0883\n",
      "Epoch 643/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0499 - val_loss: 0.0883\n",
      "Epoch 644/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0525 - val_loss: 0.0888\n",
      "Epoch 645/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0488 - val_loss: 0.0883\n",
      "Epoch 646/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0492 - val_loss: 0.0884\n",
      "Epoch 647/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0520 - val_loss: 0.0884\n",
      "Epoch 648/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0485 - val_loss: 0.0883\n",
      "Epoch 649/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0882\n",
      "Epoch 650/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0474 - val_loss: 0.0883\n",
      "Epoch 651/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0503 - val_loss: 0.0884\n",
      "Epoch 652/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0481 - val_loss: 0.0887\n",
      "Epoch 653/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0503 - val_loss: 0.0886\n",
      "Epoch 654/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0481 - val_loss: 0.0884\n",
      "Epoch 655/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0500 - val_loss: 0.0885\n",
      "Epoch 656/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0487 - val_loss: 0.0883\n",
      "Epoch 657/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0500 - val_loss: 0.0887\n",
      "Epoch 658/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0494 - val_loss: 0.0884\n",
      "Epoch 659/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0885\n",
      "Epoch 660/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0491 - val_loss: 0.0884\n",
      "Epoch 661/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0498 - val_loss: 0.0883\n",
      "Epoch 662/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0517 - val_loss: 0.0887\n",
      "Epoch 663/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0490 - val_loss: 0.0884\n",
      "Epoch 664/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0493 - val_loss: 0.0883\n",
      "Epoch 665/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0515 - val_loss: 0.0884\n",
      "Epoch 666/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0493 - val_loss: 0.0884\n",
      "Epoch 667/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0492 - val_loss: 0.0885\n",
      "Epoch 668/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0884\n",
      "Epoch 669/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0488 - val_loss: 0.0883\n",
      "Epoch 670/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0515 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00670: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 671/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0500 - val_loss: 0.0882\n",
      "Epoch 672/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0494 - val_loss: 0.0881\n",
      "Epoch 673/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0501 - val_loss: 0.0881\n",
      "Epoch 674/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0489 - val_loss: 0.0882\n",
      "Epoch 675/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0497 - val_loss: 0.0881\n",
      "Epoch 676/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0484 - val_loss: 0.0881\n",
      "Epoch 677/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0483 - val_loss: 0.0881\n",
      "Epoch 678/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0505 - val_loss: 0.0881\n",
      "Epoch 679/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0880\n",
      "Epoch 680/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0880\n",
      "Epoch 681/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0498 - val_loss: 0.0881\n",
      "Epoch 682/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0469 - val_loss: 0.0881\n",
      "Epoch 683/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0479 - val_loss: 0.0881\n",
      "Epoch 684/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0505 - val_loss: 0.0881\n",
      "Epoch 685/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0494 - val_loss: 0.0880\n",
      "Epoch 686/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0484 - val_loss: 0.0880\n",
      "Epoch 687/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0491 - val_loss: 0.0881\n",
      "Epoch 688/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0509 - val_loss: 0.0880\n",
      "Epoch 689/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0499 - val_loss: 0.0880\n",
      "Epoch 690/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0530 - val_loss: 0.0881\n",
      "Epoch 691/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0490 - val_loss: 0.0882\n",
      "Epoch 692/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0506 - val_loss: 0.0882\n",
      "Epoch 693/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0493 - val_loss: 0.0881\n",
      "Epoch 694/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0495 - val_loss: 0.0880\n",
      "Epoch 695/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0480 - val_loss: 0.0881\n",
      "Epoch 696/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0507 - val_loss: 0.0881\n",
      "Epoch 697/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0472 - val_loss: 0.0881\n",
      "Epoch 698/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0881\n",
      "Epoch 699/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0477 - val_loss: 0.0882\n",
      "Epoch 700/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0489 - val_loss: 0.0881\n",
      "Epoch 701/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0881\n",
      "Epoch 702/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0483 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00702: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 703/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0479 - val_loss: 0.0880\n",
      "Epoch 704/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0496 - val_loss: 0.0880\n",
      "Epoch 705/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0491 - val_loss: 0.0881\n",
      "Epoch 706/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0495 - val_loss: 0.0881\n",
      "Epoch 707/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0496 - val_loss: 0.0880\n",
      "Epoch 708/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0482 - val_loss: 0.0880\n",
      "Epoch 709/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0468 - val_loss: 0.0880\n",
      "Epoch 710/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0482 - val_loss: 0.0881\n",
      "Epoch 711/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0488 - val_loss: 0.0880\n",
      "Epoch 712/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0485 - val_loss: 0.0880\n",
      "Epoch 713/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0500 - val_loss: 0.0880\n",
      "Epoch 714/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0477 - val_loss: 0.0880\n",
      "Epoch 715/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0484 - val_loss: 0.0880\n",
      "Epoch 716/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0520 - val_loss: 0.0880\n",
      "Epoch 717/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0479 - val_loss: 0.0880\n",
      "Epoch 718/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0491 - val_loss: 0.0879\n",
      "Epoch 719/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0507 - val_loss: 0.0880\n",
      "Epoch 720/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0490 - val_loss: 0.0879\n",
      "Epoch 721/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0504 - val_loss: 0.0880\n",
      "Epoch 722/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0501 - val_loss: 0.0880\n",
      "Epoch 723/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0478 - val_loss: 0.0880\n",
      "Epoch 724/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0507 - val_loss: 0.0880\n",
      "Epoch 725/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0475 - val_loss: 0.0880\n",
      "Epoch 726/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0468 - val_loss: 0.0880\n",
      "Epoch 727/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0481 - val_loss: 0.0880\n",
      "Epoch 728/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0483 - val_loss: 0.0880\n",
      "Epoch 729/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0492 - val_loss: 0.0880\n",
      "Epoch 730/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0479 - val_loss: 0.0879\n",
      "Epoch 731/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0478 - val_loss: 0.0880\n",
      "Epoch 732/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0492 - val_loss: 0.0880\n",
      "Epoch 733/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0508 - val_loss: 0.0879\n",
      "Epoch 734/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0489 - val_loss: 0.0880\n",
      "Epoch 735/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0491 - val_loss: 0.0880\n",
      "Epoch 736/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0478 - val_loss: 0.0880\n",
      "Epoch 737/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0490 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00737: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 738/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0479 - val_loss: 0.0879\n",
      "Epoch 739/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0499 - val_loss: 0.0880\n",
      "Epoch 740/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0490 - val_loss: 0.0879\n",
      "Epoch 741/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0498 - val_loss: 0.0879\n",
      "Epoch 742/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0518 - val_loss: 0.0879\n",
      "Epoch 743/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0504 - val_loss: 0.0879\n",
      "Epoch 744/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0495 - val_loss: 0.0879\n",
      "Epoch 745/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0473 - val_loss: 0.0879\n",
      "Epoch 746/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0493 - val_loss: 0.0879\n",
      "Epoch 747/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0507 - val_loss: 0.0879\n",
      "Epoch 748/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0511 - val_loss: 0.0879\n",
      "Epoch 749/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0509 - val_loss: 0.0879\n",
      "Epoch 750/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0473 - val_loss: 0.0879\n",
      "Epoch 751/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0493 - val_loss: 0.0879\n",
      "Epoch 752/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0512 - val_loss: 0.0879\n",
      "Epoch 753/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0492 - val_loss: 0.0880\n",
      "Epoch 754/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0469 - val_loss: 0.0879\n",
      "Epoch 755/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0466 - val_loss: 0.0880\n",
      "Epoch 756/2000\n",
      "133228/133228 [==============================] - 2s 11us/step - loss: 0.0480 - val_loss: 0.0879\n",
      "Epoch 757/2000\n",
      "133228/133228 [==============================] - 1s 11us/step - loss: 0.0504 - val_loss: 0.0879\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00757: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njM9ZIjDGGJ/7f1HvOxPR33q+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.430813846693948\n"
     ]
    }
   ],
   "source": [
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "\n",
    "    #model_name_wrt = ('/kaggle/working/molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    #X_train = X_train.fillna(0)\n",
    "    #X_test = X_test.fillna(0)\n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data = FeatureTransformer().transform(\n",
    "        dataset=pd.concat([X_train, X_test]),\n",
    "        ohe_features=oh_cols,\n",
    "        continuous_features=continuous_cols\n",
    "    )\n",
    "    \n",
    "    #target_data = target\n",
    "    \n",
    "    train = input_data[:len(X_train)]\n",
    "    test_input = input_data[len(X_train):]\n",
    "    \n",
    "    cur_type_idx_train = (train_type == mol_type)\n",
    "    cur_type_idx_test = (test_type == mol_type)\n",
    "\n",
    "    cur_type_train = train[cur_type_idx_train]\n",
    "    cur_type_target = target[cur_type_idx_train]\n",
    "    cur_type_mols = molecule_name[cur_type_idx_train]\n",
    "\n",
    "    train_idx, val_idx = next(GroupShuffleSplit(random_state=42, n_splits=1, test_size=0.2).split(cur_type_train, cur_type_target, cur_type_mols))\n",
    "\n",
    "    x_train, y_train = cur_type_train[train_idx], cur_type_target[train_idx]\n",
    "    x_val, y_val = cur_type_train[val_idx], cur_type_target[val_idx]\n",
    "\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(x_train.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam())\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=30, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    #sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "    history = nn_model.fit(x_train, [y_train], \n",
    "            validation_data=(x_val, [y_val]), \n",
    "            callbacks=[es, rlr], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(x_val)\n",
    "    plot_history(history, mol_type)\n",
    "    accuracy=np.mean(np.abs(y_val-cv_predict[:,0]))\n",
    "    print(np.log(accuracy))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[test[\"type\"]==mol_type]=test_predict[:,0][test[\"type\"]==mol_type]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "sub = feather.read_dataframe('../data/input/sample_submission.feather')\n",
    "sub['scalar_coupling_constant'] = test_prediction\n",
    "sub.to_csv('../data/nn_sub_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7561472606284803"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
