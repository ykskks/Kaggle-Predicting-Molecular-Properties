{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/kohei3/anaconda3/envs/tensorflow/lib/python3.6/site-packages/\")\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8986161502963420462\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7896396596\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15074122847688718939\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/input/train.csv\")\n",
    "test = pd.read_csv(\"../data/input/test.csv\")\n",
    "target = train[\"scalar_coupling_constant\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "        \"Atom\",\n",
    "        #\"AtomPosition\",\n",
    "        #\"AtomDistance\",\n",
    "        #\"CouplingType\",\n",
    "        #\"AtomEnvironment\",\n",
    "        #\"AtomNeighbors\",\n",
    "        #\"BruteForce\",\n",
    "        #\"DistanceFromClosest\",\n",
    "        #\"ElectroNegFromClosest\",\n",
    "        #\"ACSF\",\n",
    "        #\"MullikenChargePred\",\n",
    "        #\"OpenBabelBasic\",\n",
    "        #\"CosineDistance\",\n",
    "        #\"PotentialPred\", \n",
    "        \"DisIsAllYouNeed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([feather.read_dataframe(\"../features/\" + feature + \"_train.feather\") for feature in features], axis=1)\n",
    "X_test = pd.concat([feather.read_dataframe(\"../features/\" + feature + \"_test.feather\") for feature in features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4658147, 40), (2505542, 40))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_2</th>\n",
       "      <th>atom_3</th>\n",
       "      <th>atom_4</th>\n",
       "      <th>atom_5</th>\n",
       "      <th>atom_6</th>\n",
       "      <th>atom_7</th>\n",
       "      <th>atom_8</th>\n",
       "      <th>atom_9</th>\n",
       "      <th>...</th>\n",
       "      <th>d_7_2</th>\n",
       "      <th>d_7_3</th>\n",
       "      <th>d_8_0</th>\n",
       "      <th>d_8_1</th>\n",
       "      <th>d_8_2</th>\n",
       "      <th>d_8_3</th>\n",
       "      <th>d_9_0</th>\n",
       "      <th>d_9_1</th>\n",
       "      <th>d_9_2</th>\n",
       "      <th>d_9_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  atom_0 atom_1  atom_2  atom_3  atom_4  atom_5  atom_6  atom_7  atom_8  \\\n",
       "0      H      C       1       1       1       0       0       0       0   \n",
       "1      H      H       6       1       1       0       0       0       0   \n",
       "2      H      H       6       1       1       0       0       0       0   \n",
       "3      H      H       6       1       1       0       0       0       0   \n",
       "4      H      C       1       1       1       0       0       0       0   \n",
       "\n",
       "   atom_9  ...  d_7_2  d_7_3  d_8_0  d_8_1  d_8_2  d_8_3  d_9_0  d_9_1  d_9_2  \\\n",
       "0       0  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1       0  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2       0  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3       0  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4       0  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "   d_9_3  \n",
       "0    NaN  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3    NaN  \n",
       "4    NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting atom_0\n",
      "Starting atom_1\n"
     ]
    }
   ],
   "source": [
    "# keep nan as is\n",
    "#coupling_types = X_train['type']\n",
    "categorical_cols = list(X_train.columns[X_train.dtypes == object])\n",
    "\n",
    "\n",
    "for col in categorical_cols:    \n",
    "    print(f'Starting {col}')\n",
    "    uniques = list(X_train[col].unique())\n",
    "    if None in uniques:\n",
    "        uniques.remove(None)\n",
    "    mapping = dict(zip(uniques, range(1, len(uniques)+1)))\n",
    "    X_train[col] = X_train[col].map(mapping)\n",
    "    X_test[col] = X_test[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 639.70 MB\n",
      "Memory usage after optimization is: 577.51 MB\n",
      "Decreased by 9.7%\n",
      "Memory usage of dataframe is 344.08 MB\n",
      "Memory usage after optimization is: 310.63 MB\n",
      "Decreased by 9.7%\n"
     ]
    }
   ],
   "source": [
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['atom_0', 'atom_1', 'atom_2', 'atom_3', 'atom_4', 'atom_5',\n",
       "       'atom_6', 'atom_7', 'atom_8', 'atom_9', 'd_1_0', 'd_2_0', 'd_2_1',\n",
       "       'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3',\n",
       "       'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2',\n",
       "       'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
       "       'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up GPU preferences\n",
    "#config = tf.ConfigProto( device_count = {'GPU': 2 , 'CPU': 1} ) \n",
    "#config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "#sess = tf.Session(config=config) \n",
    "#K.set_session(sess)\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "\n",
    "K.clear_session()\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(1024, activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"linear\")(x)  \n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    return model\n",
    "\n",
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "mol_types=train[\"type\"].unique()\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 2000\n",
    "verbose = 1\n",
    "batch_size = 1024\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain =True\n",
    "\n",
    "start_time=datetime.now()\n",
    "test_prediction=np.zeros(len(test))\n",
    "\n",
    "#del train, test\n",
    "#gc.collect()\n",
    "\n",
    "cols = list(X_train.columns.values)\n",
    "\n",
    "oh_cols = ['atom_0', 'atom_1', 'atom_2', 'atom_3', 'atom_4', 'atom_5', \n",
    "           'atom_6', 'atom_7', 'atom_8','atom_9']\n",
    "           #'tertiary_atom_0', 'tertiary_atom_1', 'tertiary_atom_2', 'tertiary_atom_3',\n",
    "         # 'tertiary_atom_4', 'tertiary_atom_5', 'tertiary_atom_6', 'tertiary_atom_7',\n",
    "         # 'tertiary_atom_8', 'tertiary_atom_9', 'bond_atom']\n",
    "\n",
    "continuous_cols = [col for col in cols if col not in oh_cols]\n",
    "\n",
    "\n",
    "class FeatureTransformer:\n",
    "    def transform(self, dataset, ohe_features=[], continuous_features=[]):\n",
    "        ohe_df = OneHotEncoder().fit_transform(dataset.loc[:, ohe_features]).toarray()\n",
    "        skews = dataset.loc[:, continuous_features].skew().to_dict()\n",
    "        for column, skew in skews.items():\n",
    "            if skew > 1:\n",
    "                dataset[column] = np.log1p(dataset[column])\n",
    "        return np.concatenate([StandardScaler().fit_transform(dataset.loc[:, continuous_features]), ohe_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for col in X_train.columns:\n",
    "    try:\n",
    "        X_train[col].fillna(0, inplace=True)\n",
    "    except:\n",
    "        X_train[col] = X_train[col].cat.add_categories([0])\n",
    "        X_train[col].fillna(0, inplace=True)\n",
    "    \n",
    "for col in X_test.columns:\n",
    "    try:\n",
    "        X_test[col].fillna(0, inplace=True)\n",
    "    except:\n",
    "        X_test[col] = X_test[col].cat.add_categories([0])\n",
    "        X_test[col].fillna(0, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_type = train['type'].values\n",
    "test_type = test['type'].values\n",
    "molecule_name = train['molecule_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 567440 samples, validate on 141976 samples\n",
      "Epoch 1/2000\n",
      "567440/567440 [==============================] - 6s 11us/step - loss: 44.4052 - val_loss: 4.3560\n",
      "Epoch 2/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 2.0254 - val_loss: 1.8921\n",
      "Epoch 3/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.8461 - val_loss: 1.6626\n",
      "Epoch 4/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.7334 - val_loss: 1.6458\n",
      "Epoch 5/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.6905 - val_loss: 1.6284\n",
      "Epoch 6/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.6587 - val_loss: 1.5360\n",
      "Epoch 7/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.6082 - val_loss: 1.5464\n",
      "Epoch 8/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.5889 - val_loss: 1.4665\n",
      "Epoch 9/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.5619 - val_loss: 1.4350\n",
      "Epoch 10/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.5412 - val_loss: 1.4747\n",
      "Epoch 11/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.5206 - val_loss: 1.7503\n",
      "Epoch 12/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.5129 - val_loss: 1.5992\n",
      "Epoch 13/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.4843 - val_loss: 1.5636\n",
      "Epoch 14/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.4689 - val_loss: 1.8946\n",
      "Epoch 15/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.4518 - val_loss: 2.6618\n",
      "Epoch 16/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.4382 - val_loss: 3.3891\n",
      "Epoch 17/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.4177 - val_loss: 2.5629\n",
      "Epoch 18/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.3909 - val_loss: 1.9608\n",
      "Epoch 19/2000\n",
      "567440/567440 [==============================] - 6s 10us/step - loss: 1.3718 - val_loss: 1.3739\n",
      "Epoch 20/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.3554 - val_loss: 2.1527\n",
      "Epoch 21/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.3469 - val_loss: 2.5413\n",
      "Epoch 22/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.3228 - val_loss: 1.8140\n",
      "Epoch 23/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.3217 - val_loss: 1.8967\n",
      "Epoch 24/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.3022 - val_loss: 3.7110\n",
      "Epoch 25/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.3034 - val_loss: 2.8870\n",
      "Epoch 26/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2886 - val_loss: 1.9023\n",
      "Epoch 27/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2852 - val_loss: 1.3854\n",
      "Epoch 28/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.2784 - val_loss: 1.9998\n",
      "Epoch 29/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2777 - val_loss: 4.0803\n",
      "Epoch 30/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2711 - val_loss: 1.2855\n",
      "Epoch 31/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2570 - val_loss: 2.2474\n",
      "Epoch 32/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2641 - val_loss: 1.1774\n",
      "Epoch 33/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2531 - val_loss: 3.1149\n",
      "Epoch 34/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.2274 - val_loss: 1.7768\n",
      "Epoch 35/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2367 - val_loss: 2.1120\n",
      "Epoch 36/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2192 - val_loss: 1.1394\n",
      "Epoch 37/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.2205 - val_loss: 1.6655\n",
      "Epoch 38/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2052 - val_loss: 1.4180\n",
      "Epoch 39/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.2100 - val_loss: 1.4011\n",
      "Epoch 40/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1987 - val_loss: 2.4196\n",
      "Epoch 41/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.2134 - val_loss: 1.6568\n",
      "Epoch 42/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1941 - val_loss: 2.8868\n",
      "Epoch 43/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1870 - val_loss: 1.1249\n",
      "Epoch 44/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1816 - val_loss: 1.8676\n",
      "Epoch 45/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.1828 - val_loss: 1.7403\n",
      "Epoch 46/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1898 - val_loss: 1.3149\n",
      "Epoch 47/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1838 - val_loss: 1.3458\n",
      "Epoch 48/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.1803 - val_loss: 1.7879\n",
      "Epoch 49/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1643 - val_loss: 2.2607\n",
      "Epoch 50/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1670 - val_loss: 1.3630\n",
      "Epoch 51/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1535 - val_loss: 1.7293\n",
      "Epoch 52/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1495 - val_loss: 1.6044\n",
      "Epoch 53/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1418 - val_loss: 1.6684\n",
      "Epoch 54/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1478 - val_loss: 1.7388\n",
      "Epoch 55/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1316 - val_loss: 1.1464\n",
      "Epoch 56/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1276 - val_loss: 1.1146\n",
      "Epoch 57/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1353 - val_loss: 2.3052\n",
      "Epoch 58/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1495 - val_loss: 1.6149\n",
      "Epoch 59/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.1266 - val_loss: 1.7013\n",
      "Epoch 60/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1225 - val_loss: 1.2116\n",
      "Epoch 61/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1149 - val_loss: 1.6627\n",
      "Epoch 62/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1333 - val_loss: 1.5452\n",
      "Epoch 63/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1175 - val_loss: 1.2128\n",
      "Epoch 64/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1160 - val_loss: 2.0881\n",
      "Epoch 65/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.1170 - val_loss: 1.8621\n",
      "Epoch 66/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1076 - val_loss: 1.2642\n",
      "Epoch 67/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0968 - val_loss: 2.0764\n",
      "Epoch 68/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0979 - val_loss: 1.1454\n",
      "Epoch 69/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0984 - val_loss: 1.4480\n",
      "Epoch 70/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.1139 - val_loss: 1.7869\n",
      "Epoch 71/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0883 - val_loss: 1.2660\n",
      "Epoch 72/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0967 - val_loss: 1.2785\n",
      "Epoch 73/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.1055 - val_loss: 1.4287\n",
      "Epoch 74/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0926 - val_loss: 1.7322\n",
      "Epoch 75/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0910 - val_loss: 1.0825\n",
      "Epoch 76/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0852 - val_loss: 1.0300\n",
      "Epoch 77/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0861 - val_loss: 1.1793\n",
      "Epoch 78/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0868 - val_loss: 1.2920\n",
      "Epoch 79/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0868 - val_loss: 1.5241\n",
      "Epoch 80/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.1002 - val_loss: 1.5594\n",
      "Epoch 81/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0820 - val_loss: 1.2108\n",
      "Epoch 82/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0779 - val_loss: 1.1289\n",
      "Epoch 83/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0792 - val_loss: 1.1106\n",
      "Epoch 84/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0791 - val_loss: 1.1778\n",
      "Epoch 85/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0746 - val_loss: 1.5364\n",
      "Epoch 86/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0700 - val_loss: 1.1463\n",
      "Epoch 87/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0645 - val_loss: 1.9868\n",
      "Epoch 88/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0610 - val_loss: 1.1538\n",
      "Epoch 89/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0704 - val_loss: 1.3734\n",
      "Epoch 90/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0570 - val_loss: 1.2110\n",
      "Epoch 91/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0712 - val_loss: 1.2032\n",
      "Epoch 92/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0628 - val_loss: 1.5360\n",
      "Epoch 93/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0687 - val_loss: 1.2688\n",
      "Epoch 94/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0644 - val_loss: 1.1653\n",
      "Epoch 95/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0582 - val_loss: 1.0525\n",
      "Epoch 96/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0613 - val_loss: 1.0273\n",
      "Epoch 97/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0588 - val_loss: 1.4718\n",
      "Epoch 98/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0694 - val_loss: 1.4192\n",
      "Epoch 99/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0748 - val_loss: 1.3271\n",
      "Epoch 100/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0574 - val_loss: 1.1677\n",
      "Epoch 101/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0538 - val_loss: 1.1256\n",
      "Epoch 102/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0514 - val_loss: 1.0313\n",
      "Epoch 103/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0576 - val_loss: 0.9790\n",
      "Epoch 104/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0484 - val_loss: 1.2257\n",
      "Epoch 105/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0470 - val_loss: 1.7424\n",
      "Epoch 106/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0239 - val_loss: 0.9501\n",
      "Epoch 107/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0362 - val_loss: 1.6864\n",
      "Epoch 108/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0457 - val_loss: 0.9606\n",
      "Epoch 109/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0402 - val_loss: 1.0103\n",
      "Epoch 110/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0492 - val_loss: 1.1669\n",
      "Epoch 111/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0427 - val_loss: 1.3352\n",
      "Epoch 112/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0484 - val_loss: 1.0346\n",
      "Epoch 113/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0348 - val_loss: 0.9635\n",
      "Epoch 114/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0414 - val_loss: 1.0440\n",
      "Epoch 115/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0402 - val_loss: 1.1353\n",
      "Epoch 116/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0380 - val_loss: 1.0826\n",
      "Epoch 117/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0445 - val_loss: 1.2180\n",
      "Epoch 118/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0336 - val_loss: 0.9551\n",
      "Epoch 119/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0358 - val_loss: 1.0334\n",
      "Epoch 120/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0231 - val_loss: 1.2823\n",
      "Epoch 121/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0315 - val_loss: 1.1233\n",
      "Epoch 122/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0404 - val_loss: 0.9704\n",
      "Epoch 123/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0195 - val_loss: 0.9872\n",
      "Epoch 124/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0180 - val_loss: 1.2036\n",
      "Epoch 125/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0272 - val_loss: 1.2554\n",
      "Epoch 126/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0304 - val_loss: 1.0297\n",
      "Epoch 127/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0209 - val_loss: 1.1476\n",
      "Epoch 128/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0198 - val_loss: 1.3912\n",
      "Epoch 129/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0232 - val_loss: 0.9976\n",
      "Epoch 130/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0154 - val_loss: 1.2690\n",
      "Epoch 131/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0297 - val_loss: 1.0686\n",
      "Epoch 132/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0261 - val_loss: 0.9821\n",
      "Epoch 133/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0220 - val_loss: 1.1292\n",
      "Epoch 134/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 1.0250 - val_loss: 0.9694\n",
      "Epoch 135/2000\n",
      "567440/567440 [==============================] - 6s 10us/step - loss: 1.0045 - val_loss: 1.0255\n",
      "Epoch 136/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 1.0025 - val_loss: 0.9701\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 137/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9818 - val_loss: 0.8936\n",
      "Epoch 138/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9521 - val_loss: 1.0540\n",
      "Epoch 139/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9466 - val_loss: 0.9497\n",
      "Epoch 140/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9485 - val_loss: 0.8872\n",
      "Epoch 141/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9609 - val_loss: 0.8914\n",
      "Epoch 142/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9488 - val_loss: 0.9903\n",
      "Epoch 143/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9546 - val_loss: 0.9289\n",
      "Epoch 144/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9534 - val_loss: 0.9564\n",
      "Epoch 145/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9427 - val_loss: 0.8964\n",
      "Epoch 146/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9516 - val_loss: 0.8808\n",
      "Epoch 147/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9615 - val_loss: 0.9308\n",
      "Epoch 148/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9484 - val_loss: 0.8883\n",
      "Epoch 149/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9255 - val_loss: 0.8582\n",
      "Epoch 150/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9405 - val_loss: 0.9871\n",
      "Epoch 151/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9442 - val_loss: 0.8731\n",
      "Epoch 152/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9556 - val_loss: 0.8666\n",
      "Epoch 153/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9409 - val_loss: 0.8997\n",
      "Epoch 154/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9444 - val_loss: 0.8907\n",
      "Epoch 155/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9445 - val_loss: 0.8721\n",
      "Epoch 156/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9358 - val_loss: 0.8952\n",
      "Epoch 157/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9371 - val_loss: 0.9270\n",
      "Epoch 158/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9354 - val_loss: 0.8917\n",
      "Epoch 159/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9510 - val_loss: 0.8586\n",
      "Epoch 160/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9517 - val_loss: 0.8823\n",
      "Epoch 161/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9464 - val_loss: 0.8733\n",
      "Epoch 162/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9538 - val_loss: 0.8955\n",
      "Epoch 163/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9424 - val_loss: 0.9312\n",
      "Epoch 164/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9254 - val_loss: 0.9651\n",
      "Epoch 165/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9461 - val_loss: 0.8655\n",
      "Epoch 166/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9327 - val_loss: 0.8989\n",
      "Epoch 167/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9326 - val_loss: 0.8859\n",
      "Epoch 168/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9422 - val_loss: 0.9095\n",
      "Epoch 169/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9439 - val_loss: 0.9754\n",
      "Epoch 170/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9525 - val_loss: 0.8909\n",
      "Epoch 171/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9380 - val_loss: 0.8660\n",
      "Epoch 172/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9398 - val_loss: 0.9128\n",
      "Epoch 173/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9329 - val_loss: 1.0016\n",
      "Epoch 174/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9305 - val_loss: 0.9169\n",
      "Epoch 175/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9471 - val_loss: 0.8890\n",
      "Epoch 176/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9524 - val_loss: 0.8754\n",
      "Epoch 177/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9360 - val_loss: 0.9391\n",
      "Epoch 178/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9359 - val_loss: 0.8767\n",
      "Epoch 179/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9291 - val_loss: 0.9465\n",
      "\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 180/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9078 - val_loss: 0.8373\n",
      "Epoch 181/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9138 - val_loss: 0.8619\n",
      "Epoch 182/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9014 - val_loss: 0.8872\n",
      "Epoch 183/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8946 - val_loss: 0.8341\n",
      "Epoch 184/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9090 - val_loss: 0.8256\n",
      "Epoch 185/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8905 - val_loss: 0.8441\n",
      "Epoch 186/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8911 - val_loss: 0.8384\n",
      "Epoch 187/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8931 - val_loss: 0.8278\n",
      "Epoch 188/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9211 - val_loss: 0.8492\n",
      "Epoch 189/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9092 - val_loss: 0.8343\n",
      "Epoch 190/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8954 - val_loss: 0.8459\n",
      "Epoch 191/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8953 - val_loss: 0.9256\n",
      "Epoch 192/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9031 - val_loss: 0.8608\n",
      "Epoch 193/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8966 - val_loss: 0.8449\n",
      "Epoch 194/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9170 - val_loss: 0.8466\n",
      "Epoch 195/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8956 - val_loss: 0.8437\n",
      "Epoch 196/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9032 - val_loss: 0.8506\n",
      "Epoch 197/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8978 - val_loss: 0.8618\n",
      "Epoch 198/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8985 - val_loss: 0.8304\n",
      "Epoch 199/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8870 - val_loss: 0.8379\n",
      "Epoch 200/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8982 - val_loss: 0.8398\n",
      "Epoch 201/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9011 - val_loss: 0.8393\n",
      "Epoch 202/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8910 - val_loss: 0.8355\n",
      "Epoch 203/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9018 - val_loss: 0.8360\n",
      "Epoch 204/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8929 - val_loss: 0.9082\n",
      "Epoch 205/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8940 - val_loss: 0.8207\n",
      "Epoch 206/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8849 - val_loss: 0.8387\n",
      "Epoch 207/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8927 - val_loss: 0.8251\n",
      "Epoch 208/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8955 - val_loss: 0.8251\n",
      "Epoch 209/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8966 - val_loss: 0.8462\n",
      "Epoch 210/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8917 - val_loss: 0.8298\n",
      "Epoch 211/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8857 - val_loss: 0.8365\n",
      "Epoch 212/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8963 - val_loss: 0.8315\n",
      "Epoch 213/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9019 - val_loss: 0.8544\n",
      "Epoch 214/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9016 - val_loss: 0.8251\n",
      "Epoch 215/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8954 - val_loss: 0.8202\n",
      "Epoch 216/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8922 - val_loss: 0.8280\n",
      "Epoch 217/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8823 - val_loss: 0.8223\n",
      "Epoch 218/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8961 - val_loss: 0.8401\n",
      "Epoch 219/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8837 - val_loss: 0.8287\n",
      "Epoch 220/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8901 - val_loss: 0.8853\n",
      "Epoch 221/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8960 - val_loss: 0.8297\n",
      "Epoch 222/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8950 - val_loss: 0.8533\n",
      "Epoch 223/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8756 - val_loss: 0.8545\n",
      "Epoch 224/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8739 - val_loss: 0.8292\n",
      "Epoch 225/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8957 - val_loss: 0.8524\n",
      "Epoch 226/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8966 - val_loss: 0.8274\n",
      "Epoch 227/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8900 - val_loss: 0.8236\n",
      "Epoch 228/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8887 - val_loss: 0.8177\n",
      "Epoch 229/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8907 - val_loss: 0.8238\n",
      "Epoch 230/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8929 - val_loss: 0.8301\n",
      "Epoch 231/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9006 - val_loss: 0.8324\n",
      "Epoch 232/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8898 - val_loss: 0.8342\n",
      "Epoch 233/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8788 - val_loss: 0.8282\n",
      "Epoch 234/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8910 - val_loss: 0.8520\n",
      "Epoch 235/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8946 - val_loss: 0.8330\n",
      "Epoch 236/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9016 - val_loss: 0.8213\n",
      "Epoch 237/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8872 - val_loss: 0.8643\n",
      "Epoch 238/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8945 - val_loss: 0.8198\n",
      "Epoch 239/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8865 - val_loss: 0.8431\n",
      "Epoch 240/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8904 - val_loss: 0.8417\n",
      "Epoch 241/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8761 - val_loss: 0.8357\n",
      "Epoch 242/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9044 - val_loss: 0.8238\n",
      "Epoch 243/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8869 - val_loss: 0.8215\n",
      "Epoch 244/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8872 - val_loss: 0.8272\n",
      "Epoch 245/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8854 - val_loss: 0.8311\n",
      "Epoch 246/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8841 - val_loss: 0.8396\n",
      "Epoch 247/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8914 - val_loss: 0.8358\n",
      "Epoch 248/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8742 - val_loss: 0.8229\n",
      "Epoch 249/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8918 - val_loss: 0.8248\n",
      "Epoch 250/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8976 - val_loss: 0.8604\n",
      "Epoch 251/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8954 - val_loss: 0.8946\n",
      "Epoch 252/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.9017 - val_loss: 0.8221\n",
      "Epoch 253/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8795 - val_loss: 0.8455\n",
      "Epoch 254/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8920 - val_loss: 0.8348\n",
      "Epoch 255/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8796 - val_loss: 0.8147\n",
      "Epoch 256/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8848 - val_loss: 0.8211\n",
      "Epoch 257/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8847 - val_loss: 0.8528\n",
      "Epoch 258/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8765 - val_loss: 0.8205\n",
      "Epoch 259/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8928 - val_loss: 0.8177\n",
      "Epoch 260/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.9201 - val_loss: 0.8277\n",
      "Epoch 261/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8792 - val_loss: 0.8173\n",
      "Epoch 262/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8962 - val_loss: 0.8148\n",
      "Epoch 263/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8816 - val_loss: 0.8281\n",
      "Epoch 264/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8777 - val_loss: 0.8254\n",
      "Epoch 265/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8918 - val_loss: 0.8650\n",
      "Epoch 266/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8836 - val_loss: 0.8156\n",
      "Epoch 267/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8819 - val_loss: 0.8404\n",
      "Epoch 268/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8870 - val_loss: 0.8674\n",
      "Epoch 269/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8951 - val_loss: 0.8210\n",
      "Epoch 270/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8901 - val_loss: 0.8227\n",
      "Epoch 271/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8744 - val_loss: 0.8162\n",
      "Epoch 272/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8840 - val_loss: 0.8119\n",
      "Epoch 273/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8735 - val_loss: 0.8229\n",
      "Epoch 274/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8990 - val_loss: 0.8219\n",
      "Epoch 275/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8910 - val_loss: 0.8202\n",
      "Epoch 276/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8891 - val_loss: 0.8145\n",
      "Epoch 277/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8850 - val_loss: 0.8158\n",
      "Epoch 278/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8934 - val_loss: 0.8175\n",
      "Epoch 279/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8895 - val_loss: 0.8119\n",
      "Epoch 280/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8793 - val_loss: 0.8260\n",
      "Epoch 281/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8836 - val_loss: 0.8322\n",
      "Epoch 282/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8819 - val_loss: 0.8164\n",
      "Epoch 283/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8828 - val_loss: 0.8123\n",
      "Epoch 284/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8865 - val_loss: 0.8121\n",
      "Epoch 285/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8848 - val_loss: 0.8244\n",
      "Epoch 286/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8776 - val_loss: 0.8193\n",
      "Epoch 287/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8737 - val_loss: 0.8530\n",
      "Epoch 288/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8839 - val_loss: 0.8136\n",
      "Epoch 289/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8963 - val_loss: 0.8251\n",
      "Epoch 290/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8815 - val_loss: 0.8541\n",
      "Epoch 291/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8857 - val_loss: 0.8364\n",
      "Epoch 292/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8680 - val_loss: 0.8223\n",
      "Epoch 293/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8799 - val_loss: 0.8254\n",
      "Epoch 294/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8853 - val_loss: 0.8666\n",
      "Epoch 295/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8799 - val_loss: 0.8165\n",
      "Epoch 296/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8834 - val_loss: 0.8135\n",
      "Epoch 297/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8804 - val_loss: 0.8069\n",
      "Epoch 298/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8728 - val_loss: 0.8212\n",
      "Epoch 299/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8738 - val_loss: 0.8122\n",
      "Epoch 300/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8745 - val_loss: 0.8177\n",
      "Epoch 301/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8809 - val_loss: 0.8223\n",
      "Epoch 302/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8670 - val_loss: 0.8049\n",
      "Epoch 303/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8907 - val_loss: 0.8270\n",
      "Epoch 304/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8872 - val_loss: 0.8545\n",
      "Epoch 305/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8818 - val_loss: 0.8379\n",
      "Epoch 306/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8751 - val_loss: 0.8384\n",
      "Epoch 307/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8849 - val_loss: 0.8159\n",
      "Epoch 308/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8802 - val_loss: 0.8066\n",
      "Epoch 309/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8786 - val_loss: 0.8074\n",
      "Epoch 310/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8848 - val_loss: 0.8361\n",
      "Epoch 311/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8630 - val_loss: 0.8250\n",
      "Epoch 312/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8814 - val_loss: 0.8552\n",
      "Epoch 313/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8734 - val_loss: 0.8210\n",
      "Epoch 314/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8744 - val_loss: 0.8485\n",
      "Epoch 315/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8781 - val_loss: 0.8352\n",
      "Epoch 316/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8738 - val_loss: 0.8199\n",
      "Epoch 317/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8772 - val_loss: 0.8409\n",
      "Epoch 318/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8775 - val_loss: 0.8327\n",
      "Epoch 319/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8704 - val_loss: 0.8130\n",
      "Epoch 320/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8764 - val_loss: 0.8093\n",
      "Epoch 321/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8945 - val_loss: 0.8168\n",
      "Epoch 322/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8794 - val_loss: 0.8826\n",
      "Epoch 323/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8649 - val_loss: 0.8059\n",
      "Epoch 324/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8849 - val_loss: 0.8202\n",
      "Epoch 325/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8619 - val_loss: 0.8343\n",
      "Epoch 326/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8693 - val_loss: 0.8293\n",
      "Epoch 327/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8832 - val_loss: 0.8307\n",
      "Epoch 328/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8664 - val_loss: 0.8200\n",
      "Epoch 329/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8636 - val_loss: 0.8245\n",
      "Epoch 330/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8693 - val_loss: 0.8111\n",
      "Epoch 331/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8808 - val_loss: 0.8084\n",
      "Epoch 332/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8699 - val_loss: 0.8128\n",
      "\n",
      "Epoch 00332: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 333/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8876 - val_loss: 0.8259\n",
      "Epoch 334/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8770 - val_loss: 0.8009\n",
      "Epoch 335/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8571 - val_loss: 0.7990\n",
      "Epoch 336/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8429 - val_loss: 0.7943\n",
      "Epoch 337/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8568 - val_loss: 0.7967\n",
      "Epoch 338/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8604 - val_loss: 0.7964\n",
      "Epoch 339/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8643 - val_loss: 0.8043\n",
      "Epoch 340/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8650 - val_loss: 0.7989\n",
      "Epoch 341/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8602 - val_loss: 0.8004\n",
      "Epoch 342/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8587 - val_loss: 0.7995\n",
      "Epoch 343/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8606 - val_loss: 0.7953\n",
      "Epoch 344/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8589 - val_loss: 0.8010\n",
      "Epoch 345/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8676 - val_loss: 0.8164\n",
      "Epoch 346/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8631 - val_loss: 0.7939\n",
      "Epoch 347/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8483 - val_loss: 0.7927\n",
      "Epoch 348/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8596 - val_loss: 0.7937\n",
      "Epoch 349/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8633 - val_loss: 0.8024\n",
      "Epoch 350/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8484 - val_loss: 0.7915\n",
      "Epoch 351/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8589 - val_loss: 0.8063\n",
      "Epoch 352/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8623 - val_loss: 0.8151\n",
      "Epoch 353/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8519 - val_loss: 0.8035\n",
      "Epoch 354/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8548 - val_loss: 0.7928\n",
      "Epoch 355/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8715 - val_loss: 0.8013\n",
      "Epoch 356/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8585 - val_loss: 0.8018\n",
      "Epoch 357/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8717 - val_loss: 0.7986\n",
      "Epoch 358/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8604 - val_loss: 0.7921\n",
      "Epoch 359/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8569 - val_loss: 0.7977\n",
      "Epoch 360/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8585 - val_loss: 0.7937\n",
      "Epoch 361/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8506 - val_loss: 0.7922\n",
      "Epoch 362/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8532 - val_loss: 0.7960\n",
      "Epoch 363/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8638 - val_loss: 0.8037\n",
      "Epoch 364/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8560 - val_loss: 0.7943\n",
      "Epoch 365/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8560 - val_loss: 0.8133\n",
      "Epoch 366/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8691 - val_loss: 0.7938\n",
      "Epoch 367/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8573 - val_loss: 0.7894\n",
      "Epoch 368/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8487 - val_loss: 0.8064\n",
      "Epoch 369/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8621 - val_loss: 0.8110\n",
      "Epoch 370/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8622 - val_loss: 0.8015\n",
      "Epoch 371/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8545 - val_loss: 0.8012\n",
      "Epoch 372/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8657 - val_loss: 0.8019\n",
      "Epoch 373/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8682 - val_loss: 0.7962\n",
      "Epoch 374/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8600 - val_loss: 0.8241\n",
      "Epoch 375/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8497 - val_loss: 0.8005\n",
      "Epoch 376/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8485 - val_loss: 0.7967\n",
      "Epoch 377/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8622 - val_loss: 0.7903\n",
      "Epoch 378/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8504 - val_loss: 0.7906\n",
      "Epoch 379/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8660 - val_loss: 0.7975\n",
      "Epoch 380/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8634 - val_loss: 0.7949\n",
      "Epoch 381/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8477 - val_loss: 0.8040\n",
      "Epoch 382/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8452 - val_loss: 0.7899\n",
      "Epoch 383/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8497 - val_loss: 0.7904\n",
      "Epoch 384/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8669 - val_loss: 0.7998\n",
      "Epoch 385/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8424 - val_loss: 0.7971\n",
      "Epoch 386/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8587 - val_loss: 0.7978\n",
      "Epoch 387/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8567 - val_loss: 0.7928\n",
      "Epoch 388/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8479 - val_loss: 0.7920\n",
      "Epoch 389/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8449 - val_loss: 0.7921\n",
      "Epoch 390/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8449 - val_loss: 0.7991\n",
      "Epoch 391/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8515 - val_loss: 0.7951\n",
      "Epoch 392/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8448 - val_loss: 0.7946\n",
      "Epoch 393/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8485 - val_loss: 0.7937\n",
      "Epoch 394/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8407 - val_loss: 0.8026\n",
      "Epoch 395/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8558 - val_loss: 0.8033\n",
      "Epoch 396/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8603 - val_loss: 0.7961\n",
      "Epoch 397/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8605 - val_loss: 0.7989\n",
      "\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 398/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8379 - val_loss: 0.7857\n",
      "Epoch 399/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8536 - val_loss: 0.7894\n",
      "Epoch 400/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8476 - val_loss: 0.7819\n",
      "Epoch 401/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8431 - val_loss: 0.7928\n",
      "Epoch 402/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8450 - val_loss: 0.7835\n",
      "Epoch 403/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8323 - val_loss: 0.7875\n",
      "Epoch 404/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8366 - val_loss: 0.7979\n",
      "Epoch 405/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8453 - val_loss: 0.7924\n",
      "Epoch 406/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8315 - val_loss: 0.7931\n",
      "Epoch 407/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8414 - val_loss: 0.7832\n",
      "Epoch 408/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8392 - val_loss: 0.7890\n",
      "Epoch 409/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8475 - val_loss: 0.7841\n",
      "Epoch 410/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8413 - val_loss: 0.7833\n",
      "Epoch 411/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8373 - val_loss: 0.7830\n",
      "Epoch 412/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8412 - val_loss: 0.7813\n",
      "Epoch 413/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8377 - val_loss: 0.7845\n",
      "Epoch 414/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8572 - val_loss: 0.7917\n",
      "Epoch 415/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8525 - val_loss: 0.7893\n",
      "Epoch 416/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8404 - val_loss: 0.7854\n",
      "Epoch 417/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8380 - val_loss: 0.7826\n",
      "Epoch 418/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8470 - val_loss: 0.7839\n",
      "Epoch 419/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8410 - val_loss: 0.7851\n",
      "Epoch 420/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8394 - val_loss: 0.7907\n",
      "Epoch 421/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8441 - val_loss: 0.7950\n",
      "Epoch 422/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8509 - val_loss: 0.7849\n",
      "Epoch 423/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8377 - val_loss: 0.7848\n",
      "Epoch 424/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8373 - val_loss: 0.7817\n",
      "Epoch 425/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8341 - val_loss: 0.7818\n",
      "Epoch 426/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8470 - val_loss: 0.7840\n",
      "Epoch 427/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8498 - val_loss: 0.7853\n",
      "Epoch 428/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8353 - val_loss: 0.7848\n",
      "Epoch 429/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8514 - val_loss: 0.7832\n",
      "Epoch 430/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8370 - val_loss: 0.7889\n",
      "Epoch 431/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8303 - val_loss: 0.7817\n",
      "Epoch 432/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8250 - val_loss: 0.7912\n",
      "Epoch 433/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8373 - val_loss: 0.7829\n",
      "Epoch 434/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8613 - val_loss: 0.7841\n",
      "Epoch 435/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8513 - val_loss: 0.7893\n",
      "Epoch 436/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8538 - val_loss: 0.7845\n",
      "Epoch 437/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8550 - val_loss: 0.7816\n",
      "Epoch 438/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8332 - val_loss: 0.7868\n",
      "Epoch 439/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8454 - val_loss: 0.7865\n",
      "Epoch 440/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8440 - val_loss: 0.7885\n",
      "Epoch 441/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8333 - val_loss: 0.7818\n",
      "Epoch 442/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8462 - val_loss: 0.7852\n",
      "\n",
      "Epoch 00442: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 443/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8356 - val_loss: 0.7823\n",
      "Epoch 444/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8447 - val_loss: 0.7782\n",
      "Epoch 445/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8360 - val_loss: 0.7789\n",
      "Epoch 446/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8214 - val_loss: 0.7821\n",
      "Epoch 447/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8371 - val_loss: 0.7787\n",
      "Epoch 448/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8417 - val_loss: 0.7845\n",
      "Epoch 449/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8406 - val_loss: 0.7809\n",
      "Epoch 450/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8291 - val_loss: 0.7821\n",
      "Epoch 451/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8477 - val_loss: 0.7834\n",
      "Epoch 452/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8335 - val_loss: 0.7836\n",
      "Epoch 453/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8476 - val_loss: 0.7846\n",
      "Epoch 454/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8342 - val_loss: 0.7795\n",
      "Epoch 455/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8401 - val_loss: 0.7795\n",
      "Epoch 456/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8436 - val_loss: 0.7822\n",
      "Epoch 457/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8385 - val_loss: 0.7811\n",
      "Epoch 458/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8329 - val_loss: 0.7798\n",
      "Epoch 459/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8276 - val_loss: 0.7821\n",
      "Epoch 460/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8457 - val_loss: 0.7806\n",
      "Epoch 461/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8301 - val_loss: 0.7809\n",
      "Epoch 462/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8510 - val_loss: 0.7776\n",
      "Epoch 463/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8431 - val_loss: 0.7805\n",
      "Epoch 464/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8434 - val_loss: 0.7832\n",
      "Epoch 465/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8344 - val_loss: 0.7828\n",
      "Epoch 466/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8321 - val_loss: 0.7805\n",
      "Epoch 467/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8541 - val_loss: 0.7797\n",
      "Epoch 468/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8336 - val_loss: 0.7783\n",
      "Epoch 469/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8541 - val_loss: 0.7835\n",
      "Epoch 470/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8346 - val_loss: 0.7776\n",
      "Epoch 471/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8348 - val_loss: 0.7791\n",
      "Epoch 472/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8236 - val_loss: 0.7786\n",
      "Epoch 473/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8211 - val_loss: 0.7800\n",
      "Epoch 474/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8347 - val_loss: 0.7798\n",
      "Epoch 475/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8317 - val_loss: 0.7812\n",
      "Epoch 476/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8227 - val_loss: 0.7799\n",
      "Epoch 477/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8408 - val_loss: 0.7786\n",
      "Epoch 478/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8307 - val_loss: 0.7817\n",
      "Epoch 479/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8312 - val_loss: 0.7802\n",
      "Epoch 480/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8378 - val_loss: 0.7802\n",
      "Epoch 481/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8372 - val_loss: 0.7816\n",
      "Epoch 482/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8334 - val_loss: 0.7831\n",
      "Epoch 483/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8322 - val_loss: 0.7774\n",
      "Epoch 484/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8353 - val_loss: 0.7795\n",
      "Epoch 485/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8277 - val_loss: 0.7805\n",
      "Epoch 486/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8342 - val_loss: 0.7820\n",
      "Epoch 487/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8397 - val_loss: 0.7806\n",
      "Epoch 488/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8495 - val_loss: 0.7796\n",
      "Epoch 489/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8248 - val_loss: 0.7771\n",
      "Epoch 490/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8248 - val_loss: 0.7825\n",
      "Epoch 491/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8339 - val_loss: 0.7793\n",
      "Epoch 492/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8327 - val_loss: 0.7816\n",
      "Epoch 493/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8386 - val_loss: 0.7779\n",
      "Epoch 494/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8333 - val_loss: 0.7786\n",
      "Epoch 495/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8281 - val_loss: 0.7809\n",
      "Epoch 496/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8315 - val_loss: 0.7798\n",
      "Epoch 497/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8393 - val_loss: 0.7781\n",
      "Epoch 498/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8324 - val_loss: 0.7793\n",
      "Epoch 499/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8414 - val_loss: 0.7837\n",
      "Epoch 500/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8234 - val_loss: 0.7784\n",
      "Epoch 501/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8291 - val_loss: 0.7801\n",
      "Epoch 502/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8293 - val_loss: 0.7793\n",
      "Epoch 503/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8273 - val_loss: 0.7780\n",
      "Epoch 504/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8346 - val_loss: 0.7780\n",
      "Epoch 505/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8431 - val_loss: 0.7851\n",
      "Epoch 506/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8450 - val_loss: 0.7779\n",
      "Epoch 507/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8377 - val_loss: 0.7795\n",
      "Epoch 508/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8419 - val_loss: 0.7806\n",
      "Epoch 509/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8383 - val_loss: 0.7770\n",
      "Epoch 510/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8445 - val_loss: 0.7810\n",
      "Epoch 511/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8509 - val_loss: 0.7789\n",
      "Epoch 512/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8363 - val_loss: 0.7796\n",
      "Epoch 513/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8319 - val_loss: 0.7835\n",
      "Epoch 514/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8370 - val_loss: 0.7772\n",
      "Epoch 515/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8306 - val_loss: 0.7807\n",
      "Epoch 516/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8306 - val_loss: 0.7790\n",
      "Epoch 517/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8497 - val_loss: 0.7783\n",
      "Epoch 518/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8327 - val_loss: 0.7833\n",
      "Epoch 519/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8414 - val_loss: 0.7770\n",
      "Epoch 520/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8376 - val_loss: 0.7807\n",
      "Epoch 521/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8338 - val_loss: 0.7772\n",
      "Epoch 522/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8374 - val_loss: 0.7776\n",
      "Epoch 523/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8292 - val_loss: 0.7863\n",
      "Epoch 524/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8331 - val_loss: 0.7832\n",
      "Epoch 525/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8482 - val_loss: 0.7816\n",
      "Epoch 526/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8299 - val_loss: 0.7800\n",
      "Epoch 527/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8356 - val_loss: 0.7813\n",
      "Epoch 528/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8259 - val_loss: 0.7813\n",
      "Epoch 529/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8362 - val_loss: 0.7802\n",
      "Epoch 530/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8340 - val_loss: 0.7788\n",
      "Epoch 531/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8401 - val_loss: 0.7789\n",
      "Epoch 532/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8391 - val_loss: 0.7873\n",
      "Epoch 533/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8407 - val_loss: 0.7812\n",
      "Epoch 534/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8362 - val_loss: 0.7790\n",
      "Epoch 535/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8446 - val_loss: 0.7782\n",
      "Epoch 536/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8317 - val_loss: 0.7805\n",
      "Epoch 537/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8354 - val_loss: 0.7779\n",
      "Epoch 538/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8274 - val_loss: 0.7781\n",
      "Epoch 539/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8453 - val_loss: 0.7793\n",
      "Epoch 540/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8289 - val_loss: 0.7814\n",
      "Epoch 541/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8374 - val_loss: 0.7783\n",
      "Epoch 542/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8388 - val_loss: 0.7796\n",
      "Epoch 543/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8319 - val_loss: 0.7805\n",
      "Epoch 544/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8275 - val_loss: 0.7808\n",
      "Epoch 545/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8427 - val_loss: 0.7782\n",
      "Epoch 546/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8289 - val_loss: 0.7786\n",
      "Epoch 547/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8473 - val_loss: 0.7789\n",
      "Epoch 548/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8308 - val_loss: 0.7780\n",
      "Epoch 549/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8392 - val_loss: 0.7797\n",
      "\n",
      "Epoch 00549: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 550/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8339 - val_loss: 0.7768\n",
      "Epoch 551/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8273 - val_loss: 0.7753\n",
      "Epoch 552/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8455 - val_loss: 0.7765\n",
      "Epoch 553/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8428 - val_loss: 0.7763\n",
      "Epoch 554/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8328 - val_loss: 0.7775\n",
      "Epoch 555/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8166 - val_loss: 0.7777\n",
      "Epoch 556/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8378 - val_loss: 0.7763\n",
      "Epoch 557/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8281 - val_loss: 0.7804\n",
      "Epoch 558/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8355 - val_loss: 0.7802\n",
      "Epoch 559/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8220 - val_loss: 0.7779\n",
      "Epoch 560/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8312 - val_loss: 0.7770\n",
      "Epoch 561/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8417 - val_loss: 0.7780\n",
      "Epoch 562/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8306 - val_loss: 0.7771\n",
      "Epoch 563/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8209 - val_loss: 0.7765\n",
      "Epoch 564/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8328 - val_loss: 0.7788\n",
      "Epoch 565/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8376 - val_loss: 0.7796\n",
      "Epoch 566/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8228 - val_loss: 0.7776\n",
      "Epoch 567/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8294 - val_loss: 0.7787\n",
      "Epoch 568/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8411 - val_loss: 0.7785\n",
      "Epoch 569/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8241 - val_loss: 0.7781\n",
      "Epoch 570/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8309 - val_loss: 0.7811\n",
      "Epoch 571/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8282 - val_loss: 0.7787\n",
      "Epoch 572/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8299 - val_loss: 0.7770\n",
      "Epoch 573/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8274 - val_loss: 0.7775\n",
      "Epoch 574/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8480 - val_loss: 0.7765\n",
      "Epoch 575/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8268 - val_loss: 0.7754\n",
      "Epoch 576/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8258 - val_loss: 0.7766\n",
      "Epoch 577/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8356 - val_loss: 0.7767\n",
      "Epoch 578/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8352 - val_loss: 0.7750\n",
      "Epoch 579/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8244 - val_loss: 0.7809\n",
      "Epoch 580/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8208 - val_loss: 0.7786\n",
      "Epoch 581/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8276 - val_loss: 0.7778\n",
      "Epoch 582/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8271 - val_loss: 0.7785\n",
      "Epoch 583/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8387 - val_loss: 0.7781\n",
      "Epoch 584/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8314 - val_loss: 0.7758\n",
      "Epoch 585/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8406 - val_loss: 0.7785\n",
      "Epoch 586/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8421 - val_loss: 0.7762\n",
      "Epoch 587/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8432 - val_loss: 0.7755\n",
      "Epoch 588/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8230 - val_loss: 0.7755\n",
      "Epoch 589/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8457 - val_loss: 0.7790\n",
      "Epoch 590/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8208 - val_loss: 0.7768\n",
      "Epoch 591/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8211 - val_loss: 0.7770\n",
      "Epoch 592/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8257 - val_loss: 0.7759\n",
      "Epoch 593/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8391 - val_loss: 0.7800\n",
      "Epoch 594/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8555 - val_loss: 0.7760\n",
      "Epoch 595/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8211 - val_loss: 0.7762\n",
      "Epoch 596/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8248 - val_loss: 0.7773\n",
      "Epoch 597/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8418 - val_loss: 0.7768\n",
      "Epoch 598/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8441 - val_loss: 0.7776\n",
      "Epoch 599/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8386 - val_loss: 0.7768\n",
      "Epoch 600/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8191 - val_loss: 0.7798\n",
      "Epoch 601/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8308 - val_loss: 0.7769\n",
      "Epoch 602/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8423 - val_loss: 0.7778\n",
      "Epoch 603/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8317 - val_loss: 0.7820\n",
      "Epoch 604/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8273 - val_loss: 0.7775\n",
      "Epoch 605/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8296 - val_loss: 0.7768\n",
      "Epoch 606/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8445 - val_loss: 0.7773\n",
      "Epoch 607/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8311 - val_loss: 0.7760\n",
      "Epoch 608/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8331 - val_loss: 0.7756\n",
      "\n",
      "Epoch 00608: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 609/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8298 - val_loss: 0.7744\n",
      "Epoch 610/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8181 - val_loss: 0.7787\n",
      "Epoch 611/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8324 - val_loss: 0.7779\n",
      "Epoch 612/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8328 - val_loss: 0.7756\n",
      "Epoch 613/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8460 - val_loss: 0.7786\n",
      "Epoch 614/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8415 - val_loss: 0.7732\n",
      "Epoch 615/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8324 - val_loss: 0.7745\n",
      "Epoch 616/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8409 - val_loss: 0.7790\n",
      "Epoch 617/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8373 - val_loss: 0.7781\n",
      "Epoch 618/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8342 - val_loss: 0.7759\n",
      "Epoch 619/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8336 - val_loss: 0.7756\n",
      "Epoch 620/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8428 - val_loss: 0.7762\n",
      "Epoch 621/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8269 - val_loss: 0.7784\n",
      "Epoch 622/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8278 - val_loss: 0.7767\n",
      "Epoch 623/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8229 - val_loss: 0.7757\n",
      "Epoch 624/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8226 - val_loss: 0.7744\n",
      "Epoch 625/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8276 - val_loss: 0.7758\n",
      "Epoch 626/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8410 - val_loss: 0.7748\n",
      "Epoch 627/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8370 - val_loss: 0.7760\n",
      "Epoch 628/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8351 - val_loss: 0.7785\n",
      "Epoch 629/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8321 - val_loss: 0.7758\n",
      "Epoch 630/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8240 - val_loss: 0.7745\n",
      "Epoch 631/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8193 - val_loss: 0.7764\n",
      "Epoch 632/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8328 - val_loss: 0.7765\n",
      "Epoch 633/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8332 - val_loss: 0.7745\n",
      "Epoch 634/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8384 - val_loss: 0.7776\n",
      "Epoch 635/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8247 - val_loss: 0.7771\n",
      "Epoch 636/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8320 - val_loss: 0.7818\n",
      "Epoch 637/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8299 - val_loss: 0.7797\n",
      "Epoch 638/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8290 - val_loss: 0.7760\n",
      "Epoch 639/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8183 - val_loss: 0.7747\n",
      "Epoch 640/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8236 - val_loss: 0.7797\n",
      "Epoch 641/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8386 - val_loss: 0.7766\n",
      "Epoch 642/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8217 - val_loss: 0.7739\n",
      "Epoch 643/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8314 - val_loss: 0.7773\n",
      "Epoch 644/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8387 - val_loss: 0.7754\n",
      "\n",
      "Epoch 00644: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 645/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8352 - val_loss: 0.7780\n",
      "Epoch 646/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8280 - val_loss: 0.7757\n",
      "Epoch 647/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8381 - val_loss: 0.7775\n",
      "Epoch 648/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8393 - val_loss: 0.7834\n",
      "Epoch 649/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8568 - val_loss: 0.7754\n",
      "Epoch 650/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8312 - val_loss: 0.7795\n",
      "Epoch 651/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8230 - val_loss: 0.7748\n",
      "Epoch 652/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8146 - val_loss: 0.7766\n",
      "Epoch 653/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8281 - val_loss: 0.7749\n",
      "Epoch 654/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8334 - val_loss: 0.7743\n",
      "Epoch 655/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8413 - val_loss: 0.7775\n",
      "Epoch 656/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8447 - val_loss: 0.7754\n",
      "Epoch 657/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8253 - val_loss: 0.7780\n",
      "Epoch 658/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8255 - val_loss: 0.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8348 - val_loss: 0.7763\n",
      "Epoch 660/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8524 - val_loss: 0.7749\n",
      "Epoch 661/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8344 - val_loss: 0.7744\n",
      "Epoch 662/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8256 - val_loss: 0.7763\n",
      "Epoch 663/2000\n",
      "567440/567440 [==============================] - 5s 9us/step - loss: 0.8333 - val_loss: 0.7744\n",
      "Epoch 664/2000\n",
      "567440/567440 [==============================] - 5s 10us/step - loss: 0.8278 - val_loss: 0.7742\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00664: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMwp+EZEyo+AXESkzCn4RkTLz/wFcay/lKRSsxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2572463894651664\n",
      "Training 2JHH out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 301863 samples, validate on 76173 samples\n",
      "Epoch 1/2000\n",
      "301863/301863 [==============================] - 3s 11us/step - loss: 3.4620 - val_loss: 0.8364\n",
      "Epoch 2/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.6210 - val_loss: 0.9687\n",
      "Epoch 3/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.5223 - val_loss: 1.8773\n",
      "Epoch 4/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.4473 - val_loss: 2.7112\n",
      "Epoch 5/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.4003 - val_loss: 1.9345\n",
      "Epoch 6/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3788 - val_loss: 1.5532\n",
      "Epoch 7/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3624 - val_loss: 1.1685\n",
      "Epoch 8/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.3459 - val_loss: 1.2420\n",
      "Epoch 9/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.3394 - val_loss: 1.0401\n",
      "Epoch 10/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.3306 - val_loss: 0.8630\n",
      "Epoch 11/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3235 - val_loss: 0.7344\n",
      "Epoch 12/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3219 - val_loss: 0.8816\n",
      "Epoch 13/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.3112 - val_loss: 0.8156\n",
      "Epoch 14/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.3041 - val_loss: 0.8496\n",
      "Epoch 15/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3035 - val_loss: 0.8441\n",
      "Epoch 16/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3000 - val_loss: 0.6675\n",
      "Epoch 17/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.3002 - val_loss: 0.7403\n",
      "Epoch 18/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2936 - val_loss: 0.8037\n",
      "Epoch 19/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2915 - val_loss: 0.7579\n",
      "Epoch 20/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2845 - val_loss: 0.5443\n",
      "Epoch 21/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2793 - val_loss: 0.5873\n",
      "Epoch 22/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2857 - val_loss: 0.6525\n",
      "Epoch 23/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2761 - val_loss: 0.6906\n",
      "Epoch 24/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2743 - val_loss: 0.5577\n",
      "Epoch 25/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2738 - val_loss: 0.4630\n",
      "Epoch 26/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2651 - val_loss: 0.5045\n",
      "Epoch 27/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2713 - val_loss: 0.4985\n",
      "Epoch 28/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2668 - val_loss: 0.5962\n",
      "Epoch 29/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2635 - val_loss: 0.4259\n",
      "Epoch 30/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2617 - val_loss: 0.4354\n",
      "Epoch 31/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2662 - val_loss: 0.4645\n",
      "Epoch 32/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2583 - val_loss: 0.3601\n",
      "Epoch 33/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2633 - val_loss: 0.5665\n",
      "Epoch 34/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2548 - val_loss: 0.4852\n",
      "Epoch 35/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2486 - val_loss: 0.3646\n",
      "Epoch 36/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2545 - val_loss: 0.4203\n",
      "Epoch 37/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2493 - val_loss: 0.3542\n",
      "Epoch 38/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2498 - val_loss: 0.4818\n",
      "Epoch 39/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2514 - val_loss: 0.3844\n",
      "Epoch 40/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2538 - val_loss: 0.3322\n",
      "Epoch 41/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2457 - val_loss: 0.3626\n",
      "Epoch 42/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2511 - val_loss: 0.2973\n",
      "Epoch 43/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2422 - val_loss: 0.5231\n",
      "Epoch 44/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2454 - val_loss: 0.4251\n",
      "Epoch 45/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2406 - val_loss: 0.3090\n",
      "Epoch 46/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2497 - val_loss: 0.4667\n",
      "Epoch 47/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2472 - val_loss: 0.3655\n",
      "Epoch 48/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2373 - val_loss: 0.3734\n",
      "Epoch 49/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2412 - val_loss: 0.3508\n",
      "Epoch 50/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2399 - val_loss: 0.2412\n",
      "Epoch 51/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2341 - val_loss: 0.3062\n",
      "Epoch 52/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2329 - val_loss: 0.2528\n",
      "Epoch 53/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2376 - val_loss: 0.2728\n",
      "Epoch 54/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2381 - val_loss: 0.3993\n",
      "Epoch 55/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2333 - val_loss: 0.3378\n",
      "Epoch 56/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2376 - val_loss: 0.2938\n",
      "Epoch 57/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2311 - val_loss: 0.5100\n",
      "Epoch 58/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2351 - val_loss: 0.2829\n",
      "Epoch 59/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2293 - val_loss: 0.2903\n",
      "Epoch 60/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2365 - val_loss: 0.4140\n",
      "Epoch 61/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2339 - val_loss: 0.2611\n",
      "Epoch 62/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2283 - val_loss: 0.2770\n",
      "Epoch 63/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2298 - val_loss: 0.4440\n",
      "Epoch 64/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2279 - val_loss: 0.2811\n",
      "Epoch 65/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2240 - val_loss: 0.3570\n",
      "Epoch 66/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2252 - val_loss: 0.3232\n",
      "Epoch 67/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2254 - val_loss: 0.3378\n",
      "Epoch 68/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2192 - val_loss: 0.2675\n",
      "Epoch 69/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2319 - val_loss: 0.3743\n",
      "Epoch 70/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2222 - val_loss: 0.2618\n",
      "Epoch 71/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2246 - val_loss: 0.2749\n",
      "Epoch 72/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2207 - val_loss: 0.2626\n",
      "Epoch 73/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2233 - val_loss: 0.3068\n",
      "Epoch 74/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2215 - val_loss: 0.2402\n",
      "Epoch 75/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2230 - val_loss: 0.2224\n",
      "Epoch 76/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2250 - val_loss: 0.3266\n",
      "Epoch 77/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2175 - val_loss: 0.2757\n",
      "Epoch 78/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2164 - val_loss: 0.2509\n",
      "Epoch 79/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2231 - val_loss: 0.2309\n",
      "Epoch 80/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2199 - val_loss: 0.2481\n",
      "Epoch 81/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2151 - val_loss: 0.2209\n",
      "Epoch 82/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2208 - val_loss: 0.2399\n",
      "Epoch 83/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2153 - val_loss: 0.2603\n",
      "Epoch 84/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2232 - val_loss: 0.1989\n",
      "Epoch 85/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2168 - val_loss: 0.2296\n",
      "Epoch 86/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2171 - val_loss: 0.2647\n",
      "Epoch 87/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2245 - val_loss: 0.3450\n",
      "Epoch 88/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2180 - val_loss: 0.2687\n",
      "Epoch 89/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2174 - val_loss: 0.2745\n",
      "Epoch 90/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2189 - val_loss: 0.2419\n",
      "Epoch 91/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2189 - val_loss: 0.3306\n",
      "Epoch 92/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2123 - val_loss: 0.2160\n",
      "Epoch 93/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2116 - val_loss: 0.2822\n",
      "Epoch 94/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2186 - val_loss: 0.2501\n",
      "Epoch 95/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2161 - val_loss: 0.2372\n",
      "Epoch 96/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2165 - val_loss: 0.3182\n",
      "Epoch 97/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2108 - val_loss: 0.3067\n",
      "Epoch 98/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2110 - val_loss: 0.2465\n",
      "Epoch 99/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2162 - val_loss: 0.2219\n",
      "Epoch 100/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2117 - val_loss: 0.2882\n",
      "Epoch 101/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2106 - val_loss: 0.2307\n",
      "Epoch 102/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2131 - val_loss: 0.2598\n",
      "Epoch 103/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2122 - val_loss: 0.1967\n",
      "Epoch 104/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2195 - val_loss: 0.3147\n",
      "Epoch 105/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2087 - val_loss: 0.2244\n",
      "Epoch 106/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2117 - val_loss: 0.3519\n",
      "Epoch 107/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2085 - val_loss: 0.2900\n",
      "Epoch 108/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2045 - val_loss: 0.2366\n",
      "Epoch 109/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2157 - val_loss: 0.2538\n",
      "Epoch 110/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2148 - val_loss: 0.2363\n",
      "Epoch 111/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2085 - val_loss: 0.2910\n",
      "Epoch 112/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2047 - val_loss: 0.2034\n",
      "Epoch 113/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2054 - val_loss: 0.2967\n",
      "Epoch 114/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2094 - val_loss: 0.2292\n",
      "Epoch 115/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1996 - val_loss: 0.1865\n",
      "Epoch 116/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2045 - val_loss: 0.2094\n",
      "Epoch 117/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2040 - val_loss: 0.1957\n",
      "Epoch 118/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2111 - val_loss: 0.2251\n",
      "Epoch 119/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2044 - val_loss: 0.2049\n",
      "Epoch 120/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2074 - val_loss: 0.2191\n",
      "Epoch 121/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2027 - val_loss: 0.2667\n",
      "Epoch 122/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2060 - val_loss: 0.3238\n",
      "Epoch 123/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2083 - val_loss: 0.2084\n",
      "Epoch 124/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2087 - val_loss: 0.1957\n",
      "Epoch 125/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2080 - val_loss: 0.2817\n",
      "Epoch 126/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2056 - val_loss: 0.2816\n",
      "Epoch 127/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2028 - val_loss: 0.2240\n",
      "Epoch 128/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1992 - val_loss: 0.2570\n",
      "Epoch 129/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2092 - val_loss: 0.2057\n",
      "Epoch 130/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2020 - val_loss: 0.2466\n",
      "Epoch 131/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2055 - val_loss: 0.2471\n",
      "Epoch 132/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2032 - val_loss: 0.1980\n",
      "Epoch 133/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2050 - val_loss: 0.2732\n",
      "Epoch 134/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2083 - val_loss: 0.2287\n",
      "Epoch 135/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2011 - val_loss: 0.2718\n",
      "Epoch 136/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1987 - val_loss: 0.2145\n",
      "Epoch 137/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2012 - val_loss: 0.2138\n",
      "Epoch 138/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2054 - val_loss: 0.1848\n",
      "Epoch 139/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1982 - val_loss: 0.2049\n",
      "Epoch 140/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1964 - val_loss: 0.1881\n",
      "Epoch 141/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2037 - val_loss: 0.2156\n",
      "Epoch 142/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1980 - val_loss: 0.2060\n",
      "Epoch 143/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2025 - val_loss: 0.2144\n",
      "Epoch 144/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1964 - val_loss: 0.2127\n",
      "Epoch 145/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1989 - val_loss: 0.1990\n",
      "Epoch 146/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2000 - val_loss: 0.2092\n",
      "Epoch 147/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1981 - val_loss: 0.1920\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1955 - val_loss: 0.2038\n",
      "Epoch 149/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1995 - val_loss: 0.3556\n",
      "Epoch 150/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2035 - val_loss: 0.2131\n",
      "Epoch 151/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2024 - val_loss: 0.2092\n",
      "Epoch 152/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1982 - val_loss: 0.2312\n",
      "Epoch 153/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2048 - val_loss: 0.2805\n",
      "Epoch 154/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1967 - val_loss: 0.3233\n",
      "Epoch 155/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1994 - val_loss: 0.2232\n",
      "Epoch 156/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1971 - val_loss: 0.2644\n",
      "Epoch 157/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2069 - val_loss: 0.2452\n",
      "Epoch 158/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1956 - val_loss: 0.1972\n",
      "Epoch 159/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1932 - val_loss: 0.1820\n",
      "Epoch 160/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1964 - val_loss: 0.2287\n",
      "Epoch 161/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1938 - val_loss: 0.1844\n",
      "Epoch 162/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1959 - val_loss: 0.2154\n",
      "Epoch 163/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1902 - val_loss: 0.2246\n",
      "Epoch 164/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1940 - val_loss: 0.1898\n",
      "Epoch 165/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1996 - val_loss: 0.1806\n",
      "Epoch 166/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1950 - val_loss: 0.2056\n",
      "Epoch 167/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.2006 - val_loss: 0.2657\n",
      "Epoch 168/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1952 - val_loss: 0.1939\n",
      "Epoch 169/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1916 - val_loss: 0.2096\n",
      "Epoch 170/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1953 - val_loss: 0.2126\n",
      "Epoch 171/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1944 - val_loss: 0.2069\n",
      "Epoch 172/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1941 - val_loss: 0.2622\n",
      "Epoch 173/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1942 - val_loss: 0.1928\n",
      "Epoch 174/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1979 - val_loss: 0.2224\n",
      "Epoch 175/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1923 - val_loss: 0.1768\n",
      "Epoch 176/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1969 - val_loss: 0.2053\n",
      "Epoch 177/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1923 - val_loss: 0.1997\n",
      "Epoch 178/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1972 - val_loss: 0.2000\n",
      "Epoch 179/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1922 - val_loss: 0.2025\n",
      "Epoch 180/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1953 - val_loss: 0.2534\n",
      "Epoch 181/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1948 - val_loss: 0.2555\n",
      "Epoch 182/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1921 - val_loss: 0.2231\n",
      "Epoch 183/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1964 - val_loss: 0.1782\n",
      "Epoch 184/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1869 - val_loss: 0.2057\n",
      "Epoch 185/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1884 - val_loss: 0.2227\n",
      "Epoch 186/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1990 - val_loss: 0.1854\n",
      "Epoch 187/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1948 - val_loss: 0.1835\n",
      "Epoch 188/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1943 - val_loss: 0.2020\n",
      "Epoch 189/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1949 - val_loss: 0.2290\n",
      "Epoch 190/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1931 - val_loss: 0.2206\n",
      "Epoch 191/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1881 - val_loss: 0.2002\n",
      "Epoch 192/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1950 - val_loss: 0.2282\n",
      "Epoch 193/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1904 - val_loss: 0.2103\n",
      "Epoch 194/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1852 - val_loss: 0.2459\n",
      "Epoch 195/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1982 - val_loss: 0.1890\n",
      "Epoch 196/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1866 - val_loss: 0.2020\n",
      "Epoch 197/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1962 - val_loss: 0.1995\n",
      "Epoch 198/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1934 - val_loss: 0.1888\n",
      "Epoch 199/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1875 - val_loss: 0.2087\n",
      "Epoch 200/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.2011 - val_loss: 0.2125\n",
      "Epoch 201/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1885 - val_loss: 0.2019\n",
      "Epoch 202/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1911 - val_loss: 0.1835\n",
      "Epoch 203/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1935 - val_loss: 0.1949\n",
      "Epoch 204/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1953 - val_loss: 0.2136\n",
      "Epoch 205/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1872 - val_loss: 0.1736\n",
      "Epoch 206/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1850 - val_loss: 0.1801\n",
      "Epoch 207/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1930 - val_loss: 0.1783\n",
      "Epoch 208/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1877 - val_loss: 0.1930\n",
      "Epoch 209/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1909 - val_loss: 0.1941\n",
      "Epoch 210/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1923 - val_loss: 0.2185\n",
      "Epoch 211/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1944 - val_loss: 0.1953\n",
      "Epoch 212/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1889 - val_loss: 0.1920\n",
      "Epoch 213/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1915 - val_loss: 0.1739\n",
      "Epoch 214/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1935 - val_loss: 0.2128\n",
      "Epoch 215/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1976 - val_loss: 0.2632\n",
      "Epoch 216/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1836 - val_loss: 0.2601\n",
      "Epoch 217/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1934 - val_loss: 0.1698\n",
      "Epoch 218/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1953 - val_loss: 0.2223\n",
      "Epoch 219/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1907 - val_loss: 0.2812\n",
      "Epoch 220/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1907 - val_loss: 0.1854\n",
      "Epoch 221/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1899 - val_loss: 0.2855\n",
      "Epoch 222/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1887 - val_loss: 0.2021\n",
      "Epoch 223/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1928 - val_loss: 0.1716\n",
      "Epoch 224/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1862 - val_loss: 0.2508\n",
      "Epoch 225/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1819 - val_loss: 0.1924\n",
      "Epoch 226/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1889 - val_loss: 0.2431\n",
      "Epoch 227/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1837 - val_loss: 0.1765\n",
      "Epoch 228/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1971 - val_loss: 0.2094\n",
      "Epoch 229/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1896 - val_loss: 0.1916\n",
      "Epoch 230/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1906 - val_loss: 0.2193\n",
      "Epoch 231/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1862 - val_loss: 0.1687\n",
      "Epoch 232/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1835 - val_loss: 0.1710\n",
      "Epoch 233/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1871 - val_loss: 0.1915\n",
      "Epoch 234/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1924 - val_loss: 0.2166\n",
      "Epoch 235/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1954 - val_loss: 0.1829\n",
      "Epoch 236/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1826 - val_loss: 0.2139\n",
      "Epoch 237/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1834 - val_loss: 0.2287\n",
      "Epoch 238/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1852 - val_loss: 0.2757\n",
      "Epoch 239/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1892 - val_loss: 0.2309\n",
      "Epoch 240/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1882 - val_loss: 0.2076\n",
      "Epoch 241/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1895 - val_loss: 0.2072\n",
      "Epoch 242/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1921 - val_loss: 0.2591\n",
      "Epoch 243/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1829 - val_loss: 0.2119\n",
      "Epoch 244/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1852 - val_loss: 0.1754\n",
      "Epoch 245/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1807 - val_loss: 0.3203\n",
      "Epoch 246/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1852 - val_loss: 0.1958\n",
      "Epoch 247/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1896 - val_loss: 0.2074\n",
      "Epoch 248/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1870 - val_loss: 0.1769\n",
      "Epoch 249/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1903 - val_loss: 0.1742\n",
      "Epoch 250/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1865 - val_loss: 0.2241\n",
      "Epoch 251/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1865 - val_loss: 0.2969\n",
      "Epoch 252/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1874 - val_loss: 0.1882\n",
      "Epoch 253/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1823 - val_loss: 0.2736\n",
      "Epoch 254/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1837 - val_loss: 0.2673\n",
      "Epoch 255/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1938 - val_loss: 0.1756\n",
      "Epoch 256/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1824 - val_loss: 0.1666\n",
      "Epoch 257/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1917 - val_loss: 0.3378\n",
      "Epoch 258/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1935 - val_loss: 0.2294\n",
      "Epoch 259/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1877 - val_loss: 0.1765\n",
      "Epoch 260/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1811 - val_loss: 0.1704\n",
      "Epoch 261/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1813 - val_loss: 0.1862\n",
      "Epoch 262/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1859 - val_loss: 0.2318\n",
      "Epoch 263/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1852 - val_loss: 0.1782\n",
      "Epoch 264/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1879 - val_loss: 0.2324\n",
      "Epoch 265/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1847 - val_loss: 0.1880\n",
      "Epoch 266/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1910 - val_loss: 0.1871\n",
      "Epoch 267/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1882 - val_loss: 0.2007\n",
      "Epoch 268/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1872 - val_loss: 0.2277\n",
      "Epoch 269/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1855 - val_loss: 0.1755\n",
      "Epoch 270/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1953 - val_loss: 0.2015\n",
      "Epoch 271/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1880 - val_loss: 0.1925\n",
      "Epoch 272/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1844 - val_loss: 0.1827\n",
      "Epoch 273/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1827 - val_loss: 0.2193\n",
      "Epoch 274/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1817 - val_loss: 0.2771\n",
      "Epoch 275/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1837 - val_loss: 0.2021\n",
      "Epoch 276/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1878 - val_loss: 0.1695\n",
      "Epoch 277/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1796 - val_loss: 0.1788\n",
      "Epoch 278/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1801 - val_loss: 0.2796\n",
      "Epoch 279/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1789 - val_loss: 0.2296\n",
      "Epoch 280/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1827 - val_loss: 0.1813\n",
      "Epoch 281/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1821 - val_loss: 0.1834\n",
      "Epoch 282/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1875 - val_loss: 0.1847\n",
      "Epoch 283/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1837 - val_loss: 0.2332\n",
      "Epoch 284/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1825 - val_loss: 0.2537\n",
      "Epoch 285/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1814 - val_loss: 0.1679\n",
      "Epoch 286/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1807 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00286: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 287/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1689 - val_loss: 0.1603\n",
      "Epoch 288/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1749 - val_loss: 0.1757\n",
      "Epoch 289/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1762 - val_loss: 0.1737\n",
      "Epoch 290/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1703 - val_loss: 0.1688\n",
      "Epoch 291/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1654 - val_loss: 0.1556\n",
      "Epoch 292/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1723 - val_loss: 0.1732\n",
      "Epoch 293/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1734 - val_loss: 0.1743\n",
      "Epoch 294/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1691 - val_loss: 0.1587\n",
      "Epoch 295/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1698 - val_loss: 0.1746\n",
      "Epoch 296/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1730 - val_loss: 0.1528\n",
      "Epoch 297/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1682 - val_loss: 0.1660\n",
      "Epoch 298/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1662 - val_loss: 0.1509\n",
      "Epoch 299/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1700 - val_loss: 0.1628\n",
      "Epoch 300/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1621 - val_loss: 0.1526\n",
      "Epoch 301/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1604 - val_loss: 0.1540\n",
      "Epoch 302/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1692 - val_loss: 0.1541\n",
      "Epoch 303/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1711 - val_loss: 0.1533\n",
      "Epoch 304/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1678 - val_loss: 0.1706\n",
      "Epoch 305/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1627 - val_loss: 0.1544\n",
      "Epoch 306/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1679 - val_loss: 0.1541\n",
      "Epoch 307/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1657 - val_loss: 0.1676\n",
      "Epoch 308/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1662 - val_loss: 0.1540\n",
      "Epoch 309/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1681 - val_loss: 0.1692\n",
      "Epoch 310/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1592 - val_loss: 0.1531\n",
      "Epoch 311/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1700 - val_loss: 0.1487\n",
      "Epoch 312/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1726 - val_loss: 0.1753\n",
      "Epoch 313/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1699 - val_loss: 0.1589\n",
      "Epoch 314/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1665 - val_loss: 0.1832\n",
      "Epoch 315/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1698 - val_loss: 0.1500\n",
      "Epoch 316/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1704 - val_loss: 0.1732\n",
      "Epoch 317/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1639 - val_loss: 0.1687\n",
      "Epoch 318/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1670 - val_loss: 0.1872\n",
      "Epoch 319/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1652 - val_loss: 0.1651\n",
      "Epoch 320/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1777 - val_loss: 0.1704\n",
      "Epoch 321/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1630 - val_loss: 0.1655\n",
      "Epoch 322/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1624 - val_loss: 0.1753\n",
      "Epoch 323/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1617 - val_loss: 0.1707\n",
      "Epoch 324/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1632 - val_loss: 0.1660\n",
      "Epoch 325/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1666 - val_loss: 0.2105\n",
      "Epoch 326/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1626 - val_loss: 0.1562\n",
      "Epoch 327/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1646 - val_loss: 0.1625\n",
      "Epoch 328/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1672 - val_loss: 0.1560\n",
      "Epoch 329/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1681 - val_loss: 0.1530\n",
      "Epoch 330/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1687 - val_loss: 0.1800\n",
      "Epoch 331/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1585 - val_loss: 0.1616\n",
      "Epoch 332/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1679 - val_loss: 0.1677\n",
      "Epoch 333/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1682 - val_loss: 0.1530\n",
      "Epoch 334/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1660 - val_loss: 0.1497\n",
      "Epoch 335/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1677 - val_loss: 0.1771\n",
      "Epoch 336/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1684 - val_loss: 0.1527\n",
      "Epoch 337/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1651 - val_loss: 0.1511\n",
      "Epoch 338/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1675 - val_loss: 0.1498\n",
      "Epoch 339/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1675 - val_loss: 0.1535\n",
      "Epoch 340/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1657 - val_loss: 0.1923\n",
      "Epoch 341/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1722 - val_loss: 0.1515\n",
      "\n",
      "Epoch 00341: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 342/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1577 - val_loss: 0.1534\n",
      "Epoch 343/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1608 - val_loss: 0.1509\n",
      "Epoch 344/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1559 - val_loss: 0.1579\n",
      "Epoch 345/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1557 - val_loss: 0.1491\n",
      "Epoch 346/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1552 - val_loss: 0.1505\n",
      "Epoch 347/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1581 - val_loss: 0.1469\n",
      "Epoch 348/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1637 - val_loss: 0.1471\n",
      "Epoch 349/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1584 - val_loss: 0.1420\n",
      "Epoch 350/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1610 - val_loss: 0.1775\n",
      "Epoch 351/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1560 - val_loss: 0.1546\n",
      "Epoch 352/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1564 - val_loss: 0.1464\n",
      "Epoch 353/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1552 - val_loss: 0.1466\n",
      "Epoch 354/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1585 - val_loss: 0.1511\n",
      "Epoch 355/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1613 - val_loss: 0.1454\n",
      "Epoch 356/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1559 - val_loss: 0.1438\n",
      "Epoch 357/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1559 - val_loss: 0.1438\n",
      "Epoch 358/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1509 - val_loss: 0.1447\n",
      "Epoch 359/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1570 - val_loss: 0.1633\n",
      "Epoch 360/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1563 - val_loss: 0.1444\n",
      "Epoch 361/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1611 - val_loss: 0.1569\n",
      "Epoch 362/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1579 - val_loss: 0.1487\n",
      "Epoch 363/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1598 - val_loss: 0.1824\n",
      "Epoch 364/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1548 - val_loss: 0.1620\n",
      "Epoch 365/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1562 - val_loss: 0.1557\n",
      "Epoch 366/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1571 - val_loss: 0.1455\n",
      "Epoch 367/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1569 - val_loss: 0.1610\n",
      "Epoch 368/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1585 - val_loss: 0.1527\n",
      "Epoch 369/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1531 - val_loss: 0.1428\n",
      "Epoch 370/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1596 - val_loss: 0.1537\n",
      "Epoch 371/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1563 - val_loss: 0.1519\n",
      "Epoch 372/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1591 - val_loss: 0.1504\n",
      "Epoch 373/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1634 - val_loss: 0.1474\n",
      "Epoch 374/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1588 - val_loss: 0.1486\n",
      "Epoch 375/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1600 - val_loss: 0.1479\n",
      "Epoch 376/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1593 - val_loss: 0.1426\n",
      "Epoch 377/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1578 - val_loss: 0.1648\n",
      "Epoch 378/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1513 - val_loss: 0.1596\n",
      "Epoch 379/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1572 - val_loss: 0.1461\n",
      "\n",
      "Epoch 00379: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 380/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1496 - val_loss: 0.1402\n",
      "Epoch 381/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1532 - val_loss: 0.1409\n",
      "Epoch 382/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1506 - val_loss: 0.1400\n",
      "Epoch 383/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1541 - val_loss: 0.1400\n",
      "Epoch 384/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1544 - val_loss: 0.1416\n",
      "Epoch 385/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1507 - val_loss: 0.1402\n",
      "Epoch 386/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1514 - val_loss: 0.1497\n",
      "Epoch 387/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1541 - val_loss: 0.1410\n",
      "Epoch 388/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1546 - val_loss: 0.1409\n",
      "Epoch 389/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1538 - val_loss: 0.1429\n",
      "Epoch 390/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1523 - val_loss: 0.1516\n",
      "Epoch 391/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1552 - val_loss: 0.1454\n",
      "Epoch 392/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1559 - val_loss: 0.1401\n",
      "Epoch 393/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1518 - val_loss: 0.1385\n",
      "Epoch 394/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1501 - val_loss: 0.1410\n",
      "Epoch 395/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1509 - val_loss: 0.1435\n",
      "Epoch 396/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1580 - val_loss: 0.1553\n",
      "Epoch 397/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1497 - val_loss: 0.1428\n",
      "Epoch 398/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1549 - val_loss: 0.1390\n",
      "Epoch 399/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1567 - val_loss: 0.1387\n",
      "Epoch 400/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1542 - val_loss: 0.1433\n",
      "Epoch 401/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1538 - val_loss: 0.1437\n",
      "Epoch 402/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1563 - val_loss: 0.1437\n",
      "Epoch 403/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1478 - val_loss: 0.1404\n",
      "Epoch 404/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1544 - val_loss: 0.1445\n",
      "Epoch 405/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1520 - val_loss: 0.1460\n",
      "Epoch 406/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1494 - val_loss: 0.1453\n",
      "Epoch 407/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1535 - val_loss: 0.1394\n",
      "Epoch 408/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1518 - val_loss: 0.1449\n",
      "Epoch 409/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1562 - val_loss: 0.1424\n",
      "Epoch 410/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1567 - val_loss: 0.1431\n",
      "Epoch 411/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1505 - val_loss: 0.1397\n",
      "Epoch 412/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1530 - val_loss: 0.1430\n",
      "Epoch 413/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1512 - val_loss: 0.1428\n",
      "Epoch 414/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1569 - val_loss: 0.1453\n",
      "Epoch 415/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1512 - val_loss: 0.1390\n",
      "Epoch 416/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1550 - val_loss: 0.1436\n",
      "Epoch 417/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1497 - val_loss: 0.1424\n",
      "Epoch 418/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1513 - val_loss: 0.1390\n",
      "Epoch 419/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1535 - val_loss: 0.1407\n",
      "Epoch 420/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1498 - val_loss: 0.1414\n",
      "Epoch 421/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1502 - val_loss: 0.1397\n",
      "Epoch 422/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1538 - val_loss: 0.1452\n",
      "Epoch 423/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1501 - val_loss: 0.1415\n",
      "\n",
      "Epoch 00423: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 424/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1516 - val_loss: 0.1376\n",
      "Epoch 425/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1494 - val_loss: 0.1407\n",
      "Epoch 426/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1503 - val_loss: 0.1376\n",
      "Epoch 427/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1512 - val_loss: 0.1421\n",
      "Epoch 428/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1520 - val_loss: 0.1369\n",
      "Epoch 429/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1508 - val_loss: 0.1403\n",
      "Epoch 430/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1427 - val_loss: 0.1369\n",
      "Epoch 431/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1474 - val_loss: 0.1385\n",
      "Epoch 432/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1516 - val_loss: 0.1401\n",
      "Epoch 433/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1523 - val_loss: 0.1367\n",
      "Epoch 434/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1515 - val_loss: 0.1409\n",
      "Epoch 435/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1532 - val_loss: 0.1372\n",
      "Epoch 436/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1536 - val_loss: 0.1368\n",
      "Epoch 437/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1493 - val_loss: 0.1380\n",
      "Epoch 438/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1519 - val_loss: 0.1382\n",
      "Epoch 439/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1511 - val_loss: 0.1369\n",
      "Epoch 440/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1490 - val_loss: 0.1399\n",
      "Epoch 441/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1500 - val_loss: 0.1384\n",
      "Epoch 442/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1512 - val_loss: 0.1362\n",
      "Epoch 443/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1515 - val_loss: 0.1388\n",
      "Epoch 444/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1546 - val_loss: 0.1386\n",
      "Epoch 445/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1490 - val_loss: 0.1367\n",
      "Epoch 446/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1502 - val_loss: 0.1367\n",
      "Epoch 447/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1471 - val_loss: 0.1398\n",
      "Epoch 448/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1492 - val_loss: 0.1372\n",
      "Epoch 449/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1532 - val_loss: 0.1413\n",
      "Epoch 450/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1508 - val_loss: 0.1408\n",
      "Epoch 451/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1503 - val_loss: 0.1389\n",
      "Epoch 452/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1498 - val_loss: 0.1367\n",
      "Epoch 453/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1464 - val_loss: 0.1385\n",
      "Epoch 454/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1540 - val_loss: 0.1375\n",
      "Epoch 455/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1531 - val_loss: 0.1372\n",
      "Epoch 456/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1490 - val_loss: 0.1393\n",
      "Epoch 457/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1498 - val_loss: 0.1373\n",
      "Epoch 458/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1502 - val_loss: 0.1372\n",
      "Epoch 459/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1474 - val_loss: 0.1380\n",
      "Epoch 460/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1552 - val_loss: 0.1406\n",
      "Epoch 461/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1475 - val_loss: 0.1381\n",
      "Epoch 462/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1518 - val_loss: 0.1382\n",
      "Epoch 463/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1515 - val_loss: 0.1406\n",
      "Epoch 464/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1493 - val_loss: 0.1376\n",
      "Epoch 465/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1519 - val_loss: 0.1370\n",
      "Epoch 466/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1495 - val_loss: 0.1365\n",
      "Epoch 467/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1509 - val_loss: 0.1368\n",
      "Epoch 468/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1526 - val_loss: 0.1380\n",
      "Epoch 469/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1458 - val_loss: 0.1371\n",
      "Epoch 470/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1528 - val_loss: 0.1378\n",
      "Epoch 471/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1493 - val_loss: 0.1396\n",
      "Epoch 472/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1530 - val_loss: 0.1386\n",
      "\n",
      "Epoch 00472: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 473/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1483 - val_loss: 0.1365\n",
      "Epoch 474/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1508 - val_loss: 0.1357\n",
      "Epoch 475/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1443 - val_loss: 0.1362\n",
      "Epoch 476/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1496 - val_loss: 0.1369\n",
      "Epoch 477/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1487 - val_loss: 0.1363\n",
      "Epoch 478/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1476 - val_loss: 0.1361\n",
      "Epoch 479/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1437 - val_loss: 0.1358\n",
      "Epoch 480/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1479 - val_loss: 0.1358\n",
      "Epoch 481/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1492 - val_loss: 0.1376\n",
      "Epoch 482/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1541 - val_loss: 0.1370\n",
      "Epoch 483/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1455 - val_loss: 0.1373\n",
      "Epoch 484/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1485 - val_loss: 0.1360\n",
      "Epoch 485/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1496 - val_loss: 0.1358\n",
      "Epoch 486/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1528 - val_loss: 0.1360\n",
      "Epoch 487/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1499 - val_loss: 0.1363\n",
      "Epoch 488/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1506 - val_loss: 0.1363\n",
      "Epoch 489/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1475 - val_loss: 0.1360\n",
      "Epoch 490/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1562 - val_loss: 0.1358\n",
      "Epoch 491/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1534 - val_loss: 0.1360\n",
      "Epoch 492/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1545 - val_loss: 0.1358\n",
      "Epoch 493/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1421 - val_loss: 0.1371\n",
      "Epoch 494/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1498 - val_loss: 0.1367\n",
      "Epoch 495/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1475 - val_loss: 0.1357\n",
      "Epoch 496/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1484 - val_loss: 0.1372\n",
      "Epoch 497/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1504 - val_loss: 0.1371\n",
      "Epoch 498/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1502 - val_loss: 0.1358\n",
      "Epoch 499/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1528 - val_loss: 0.1358\n",
      "Epoch 500/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1424 - val_loss: 0.1365\n",
      "Epoch 501/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1508 - val_loss: 0.1356\n",
      "Epoch 502/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1449 - val_loss: 0.1371\n",
      "Epoch 503/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1478 - val_loss: 0.1359\n",
      "Epoch 504/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1468 - val_loss: 0.1386\n",
      "\n",
      "Epoch 00504: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 505/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1497 - val_loss: 0.1368\n",
      "Epoch 506/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1532 - val_loss: 0.1362\n",
      "Epoch 507/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1512 - val_loss: 0.1362\n",
      "Epoch 508/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1481 - val_loss: 0.1355\n",
      "Epoch 509/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1498 - val_loss: 0.1358\n",
      "Epoch 510/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1465 - val_loss: 0.1357\n",
      "Epoch 511/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1463 - val_loss: 0.1356\n",
      "Epoch 512/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1522 - val_loss: 0.1361\n",
      "Epoch 513/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1456 - val_loss: 0.1354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1474 - val_loss: 0.1359\n",
      "Epoch 515/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1439 - val_loss: 0.1356\n",
      "Epoch 516/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1453 - val_loss: 0.1353\n",
      "Epoch 517/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1483 - val_loss: 0.1359\n",
      "Epoch 518/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1450 - val_loss: 0.1359\n",
      "Epoch 519/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1517 - val_loss: 0.1358\n",
      "Epoch 520/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1446 - val_loss: 0.1357\n",
      "Epoch 521/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1537 - val_loss: 0.1356\n",
      "Epoch 522/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1463 - val_loss: 0.1357\n",
      "Epoch 523/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1467 - val_loss: 0.1355\n",
      "Epoch 524/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1522 - val_loss: 0.1365\n",
      "Epoch 525/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1521 - val_loss: 0.1359\n",
      "Epoch 526/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1503 - val_loss: 0.1353\n",
      "Epoch 527/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1496 - val_loss: 0.1362\n",
      "Epoch 528/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1443 - val_loss: 0.1365\n",
      "Epoch 529/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1475 - val_loss: 0.1354\n",
      "Epoch 530/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1517 - val_loss: 0.1358\n",
      "Epoch 531/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1470 - val_loss: 0.1358\n",
      "Epoch 532/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1470 - val_loss: 0.1354\n",
      "Epoch 533/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1488 - val_loss: 0.1358\n",
      "Epoch 534/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1458 - val_loss: 0.1357\n",
      "Epoch 535/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1443 - val_loss: 0.1362\n",
      "Epoch 536/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1471 - val_loss: 0.1354\n",
      "Epoch 537/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1464 - val_loss: 0.1358\n",
      "Epoch 538/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1453 - val_loss: 0.1357\n",
      "Epoch 539/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1538 - val_loss: 0.1362\n",
      "Epoch 540/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1482 - val_loss: 0.1357\n",
      "Epoch 541/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1470 - val_loss: 0.1358\n",
      "Epoch 542/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1477 - val_loss: 0.1356\n",
      "Epoch 543/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1484 - val_loss: 0.1363\n",
      "Epoch 544/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1476 - val_loss: 0.1357\n",
      "Epoch 545/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1486 - val_loss: 0.1356\n",
      "Epoch 546/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1486 - val_loss: 0.1363\n",
      "\n",
      "Epoch 00546: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 547/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1473 - val_loss: 0.1355\n",
      "Epoch 548/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1453 - val_loss: 0.1354\n",
      "Epoch 549/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1453 - val_loss: 0.1357\n",
      "Epoch 550/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1495 - val_loss: 0.1352\n",
      "Epoch 551/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1480 - val_loss: 0.1352\n",
      "Epoch 552/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1546 - val_loss: 0.1355\n",
      "Epoch 553/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1479 - val_loss: 0.1352\n",
      "Epoch 554/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1482 - val_loss: 0.1352\n",
      "Epoch 555/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1477 - val_loss: 0.1353\n",
      "Epoch 556/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1457 - val_loss: 0.1350\n",
      "Epoch 557/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1466 - val_loss: 0.1360\n",
      "Epoch 558/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1491 - val_loss: 0.1354\n",
      "Epoch 559/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1468 - val_loss: 0.1355\n",
      "Epoch 560/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1493 - val_loss: 0.1355\n",
      "Epoch 561/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1516 - val_loss: 0.1353\n",
      "Epoch 562/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1485 - val_loss: 0.1354\n",
      "Epoch 563/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1494 - val_loss: 0.1352\n",
      "Epoch 564/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1492 - val_loss: 0.1354\n",
      "Epoch 565/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1526 - val_loss: 0.1360\n",
      "Epoch 566/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1481 - val_loss: 0.1351\n",
      "Epoch 567/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1555 - val_loss: 0.1358\n",
      "Epoch 568/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1527 - val_loss: 0.1352\n",
      "Epoch 569/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1463 - val_loss: 0.1350\n",
      "Epoch 570/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1507 - val_loss: 0.1354\n",
      "Epoch 571/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1450 - val_loss: 0.1351\n",
      "Epoch 572/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1452 - val_loss: 0.1354\n",
      "Epoch 573/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1460 - val_loss: 0.1355\n",
      "Epoch 574/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1489 - val_loss: 0.1357\n",
      "Epoch 575/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1490 - val_loss: 0.1352\n",
      "Epoch 576/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1449 - val_loss: 0.1352\n",
      "Epoch 577/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1485 - val_loss: 0.1351\n",
      "Epoch 578/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1465 - val_loss: 0.1355\n",
      "Epoch 579/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1462 - val_loss: 0.1352\n",
      "Epoch 580/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1474 - val_loss: 0.1354\n",
      "Epoch 581/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1475 - val_loss: 0.1351\n",
      "Epoch 582/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1486 - val_loss: 0.1357\n",
      "Epoch 583/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1507 - val_loss: 0.1355\n",
      "Epoch 584/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1475 - val_loss: 0.1359\n",
      "Epoch 585/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1489 - val_loss: 0.1353\n",
      "Epoch 586/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1439 - val_loss: 0.1353\n",
      "\n",
      "Epoch 00586: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 587/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1511 - val_loss: 0.1358\n",
      "Epoch 588/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1487 - val_loss: 0.1350\n",
      "Epoch 589/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1496 - val_loss: 0.1352\n",
      "Epoch 590/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1467 - val_loss: 0.1356\n",
      "Epoch 591/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1478 - val_loss: 0.1350\n",
      "Epoch 592/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1450 - val_loss: 0.1356\n",
      "Epoch 593/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1453 - val_loss: 0.1350\n",
      "Epoch 594/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1463 - val_loss: 0.1349\n",
      "Epoch 595/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1467 - val_loss: 0.1350\n",
      "Epoch 596/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1455 - val_loss: 0.1357\n",
      "Epoch 597/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1480 - val_loss: 0.1348\n",
      "Epoch 598/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1513 - val_loss: 0.1358\n",
      "Epoch 599/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1466 - val_loss: 0.1350\n",
      "Epoch 600/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1472 - val_loss: 0.1353\n",
      "Epoch 601/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1409 - val_loss: 0.1349\n",
      "Epoch 602/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1511 - val_loss: 0.1353\n",
      "Epoch 603/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1521 - val_loss: 0.1358\n",
      "Epoch 604/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1502 - val_loss: 0.1353\n",
      "Epoch 605/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1452 - val_loss: 0.1359\n",
      "Epoch 606/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1444 - val_loss: 0.1350\n",
      "Epoch 607/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1453 - val_loss: 0.1358\n",
      "Epoch 608/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1496 - val_loss: 0.1353\n",
      "Epoch 609/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1524 - val_loss: 0.1356\n",
      "Epoch 610/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1511 - val_loss: 0.1352\n",
      "Epoch 611/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1423 - val_loss: 0.1355\n",
      "Epoch 612/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1509 - val_loss: 0.1350\n",
      "Epoch 613/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1507 - val_loss: 0.1351\n",
      "Epoch 614/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1481 - val_loss: 0.1354\n",
      "Epoch 615/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1477 - val_loss: 0.1349\n",
      "Epoch 616/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1473 - val_loss: 0.1351\n",
      "Epoch 617/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1461 - val_loss: 0.1351\n",
      "Epoch 618/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1497 - val_loss: 0.1353\n",
      "Epoch 619/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1528 - val_loss: 0.1351\n",
      "Epoch 620/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1498 - val_loss: 0.1351\n",
      "Epoch 621/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1506 - val_loss: 0.1354\n",
      "Epoch 622/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1462 - val_loss: 0.1350\n",
      "Epoch 623/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1438 - val_loss: 0.1351\n",
      "Epoch 624/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1506 - val_loss: 0.1350\n",
      "Epoch 625/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1479 - val_loss: 0.1355\n",
      "Epoch 626/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1474 - val_loss: 0.1351\n",
      "Epoch 627/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1462 - val_loss: 0.1351\n",
      "\n",
      "Epoch 00627: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 628/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1446 - val_loss: 0.1354\n",
      "Epoch 629/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1480 - val_loss: 0.1350\n",
      "Epoch 630/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1520 - val_loss: 0.1355\n",
      "Epoch 631/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1464 - val_loss: 0.1354\n",
      "Epoch 632/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1504 - val_loss: 0.1359\n",
      "Epoch 633/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1479 - val_loss: 0.1354\n",
      "Epoch 634/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1475 - val_loss: 0.1354\n",
      "Epoch 635/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1498 - val_loss: 0.1350\n",
      "Epoch 636/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1488 - val_loss: 0.1351\n",
      "Epoch 637/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1487 - val_loss: 0.1351\n",
      "Epoch 638/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1459 - val_loss: 0.1355\n",
      "Epoch 639/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1450 - val_loss: 0.1363\n",
      "Epoch 640/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1480 - val_loss: 0.1351\n",
      "Epoch 641/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1467 - val_loss: 0.1352\n",
      "Epoch 642/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1484 - val_loss: 0.1352\n",
      "Epoch 643/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1476 - val_loss: 0.1349\n",
      "Epoch 644/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1480 - val_loss: 0.1350\n",
      "Epoch 645/2000\n",
      "301863/301863 [==============================] - 3s 10us/step - loss: 0.1486 - val_loss: 0.1352\n",
      "Epoch 646/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1444 - val_loss: 0.1353\n",
      "Epoch 647/2000\n",
      "301863/301863 [==============================] - 3s 9us/step - loss: 0.1486 - val_loss: 0.1355\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00647: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYhIzCkRiIjEnBKBiEjM/T+FcGYDnniTcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0039402028309588\n",
      "Training 1JHN out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 34764 samples, validate on 8599 samples\n",
      "Epoch 1/2000\n",
      "34764/34764 [==============================] - 1s 25us/step - loss: 47.1303 - val_loss: 46.0962\n",
      "Epoch 2/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 45.6382 - val_loss: 43.9385\n",
      "Epoch 3/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 42.8533 - val_loss: 40.3940\n",
      "Epoch 4/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 38.5654 - val_loss: 35.3702\n",
      "Epoch 5/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 32.6934 - val_loss: 17.1606\n",
      "Epoch 6/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 25.2375 - val_loss: 30.3379\n",
      "Epoch 7/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 16.1536 - val_loss: 26.5833\n",
      "Epoch 8/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 6.0670 - val_loss: 12.2631\n",
      "Epoch 9/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 1.8285 - val_loss: 10.6113\n",
      "Epoch 10/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 1.3971 - val_loss: 7.4555\n",
      "Epoch 11/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 1.1935 - val_loss: 5.5276\n",
      "Epoch 12/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 1.1481 - val_loss: 3.3529\n",
      "Epoch 13/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 1.0732 - val_loss: 2.6217\n",
      "Epoch 14/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 1.0197 - val_loss: 2.4715\n",
      "Epoch 15/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.9480 - val_loss: 1.8573\n",
      "Epoch 16/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.9088 - val_loss: 2.5250\n",
      "Epoch 17/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.9420 - val_loss: 1.5380\n",
      "Epoch 18/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.8746 - val_loss: 1.4492\n",
      "Epoch 19/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.9132 - val_loss: 1.6218\n",
      "Epoch 20/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.8453 - val_loss: 1.0126\n",
      "Epoch 21/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.8539 - val_loss: 1.2414\n",
      "Epoch 22/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7958 - val_loss: 1.1648\n",
      "Epoch 23/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7891 - val_loss: 1.1471\n",
      "Epoch 24/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.8009 - val_loss: 0.9752\n",
      "Epoch 25/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7996 - val_loss: 1.0176\n",
      "Epoch 26/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7853 - val_loss: 1.3743\n",
      "Epoch 27/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7726 - val_loss: 0.9672\n",
      "Epoch 28/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7709 - val_loss: 1.2809\n",
      "Epoch 29/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7737 - val_loss: 1.1782\n",
      "Epoch 30/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7488 - val_loss: 0.9185\n",
      "Epoch 31/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7871 - val_loss: 1.1327\n",
      "Epoch 32/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7528 - val_loss: 1.1067\n",
      "Epoch 33/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.7547 - val_loss: 0.9551\n",
      "Epoch 34/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7134 - val_loss: 1.0739\n",
      "Epoch 35/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7418 - val_loss: 1.1137\n",
      "Epoch 36/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7304 - val_loss: 0.9934\n",
      "Epoch 37/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6927 - val_loss: 0.9647\n",
      "Epoch 38/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6973 - val_loss: 1.1189\n",
      "Epoch 39/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6973 - val_loss: 1.0576\n",
      "Epoch 40/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6416 - val_loss: 1.2926\n",
      "Epoch 41/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6377 - val_loss: 1.2944\n",
      "Epoch 42/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6761 - val_loss: 1.2338\n",
      "Epoch 43/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6699 - val_loss: 1.2410\n",
      "Epoch 44/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6636 - val_loss: 1.1741\n",
      "Epoch 45/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6260 - val_loss: 1.2259\n",
      "Epoch 46/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6687 - val_loss: 0.8697\n",
      "Epoch 47/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6377 - val_loss: 0.9933\n",
      "Epoch 48/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6844 - val_loss: 0.9792\n",
      "Epoch 49/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6619 - val_loss: 1.1780\n",
      "Epoch 50/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6445 - val_loss: 0.9795\n",
      "Epoch 51/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6338 - val_loss: 1.1448\n",
      "Epoch 52/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6703 - val_loss: 1.3846\n",
      "Epoch 53/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6614 - val_loss: 1.0223\n",
      "Epoch 54/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6441 - val_loss: 1.4648\n",
      "Epoch 55/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6209 - val_loss: 1.3104\n",
      "Epoch 56/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6370 - val_loss: 1.3487\n",
      "Epoch 57/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6431 - val_loss: 0.9646\n",
      "Epoch 58/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6056 - val_loss: 0.8958\n",
      "Epoch 59/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6553 - val_loss: 1.4581\n",
      "Epoch 60/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6126 - val_loss: 1.2599\n",
      "Epoch 61/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6327 - val_loss: 1.3211\n",
      "Epoch 62/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5955 - val_loss: 1.1519\n",
      "Epoch 63/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5968 - val_loss: 1.2577\n",
      "Epoch 64/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5933 - val_loss: 1.2331\n",
      "Epoch 65/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6000 - val_loss: 1.3237\n",
      "Epoch 66/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5936 - val_loss: 1.4604\n",
      "Epoch 67/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6263 - val_loss: 1.1867\n",
      "Epoch 68/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5892 - val_loss: 1.5913\n",
      "Epoch 69/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6435 - val_loss: 1.1134\n",
      "Epoch 70/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.7006 - val_loss: 1.1721\n",
      "Epoch 71/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5942 - val_loss: 1.2375\n",
      "Epoch 72/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6063 - val_loss: 1.1056\n",
      "Epoch 73/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.6078 - val_loss: 1.2888\n",
      "Epoch 74/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.6166 - val_loss: 1.1953\n",
      "Epoch 75/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5637 - val_loss: 1.5257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5728 - val_loss: 1.3202\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 77/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5438 - val_loss: 1.1362\n",
      "Epoch 78/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5349 - val_loss: 1.0382\n",
      "Epoch 79/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5381 - val_loss: 0.8384\n",
      "Epoch 80/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5392 - val_loss: 0.8478\n",
      "Epoch 81/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5303 - val_loss: 0.6566\n",
      "Epoch 82/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5374 - val_loss: 0.6838\n",
      "Epoch 83/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5402 - val_loss: 0.7612\n",
      "Epoch 84/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5190 - val_loss: 0.7478\n",
      "Epoch 85/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5143 - val_loss: 0.5912\n",
      "Epoch 86/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5231 - val_loss: 0.7307\n",
      "Epoch 87/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5621 - val_loss: 0.6464\n",
      "Epoch 88/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5041 - val_loss: 0.7886\n",
      "Epoch 89/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5252 - val_loss: 0.6354\n",
      "Epoch 90/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5239 - val_loss: 0.5378\n",
      "Epoch 91/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5240 - val_loss: 0.7208\n",
      "Epoch 92/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5409 - val_loss: 0.8001\n",
      "Epoch 93/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5241 - val_loss: 0.8313\n",
      "Epoch 94/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5452 - val_loss: 0.9639\n",
      "Epoch 95/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5279 - val_loss: 0.6762\n",
      "Epoch 96/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5252 - val_loss: 0.8824\n",
      "Epoch 97/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5448 - val_loss: 0.7258\n",
      "Epoch 98/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5151 - val_loss: 0.6902\n",
      "Epoch 99/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4924 - val_loss: 0.6009\n",
      "Epoch 100/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5631 - val_loss: 0.7445\n",
      "Epoch 101/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5006 - val_loss: 0.6381\n",
      "Epoch 102/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5709 - val_loss: 0.6174\n",
      "Epoch 103/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5461 - val_loss: 0.6422\n",
      "Epoch 104/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4800 - val_loss: 0.7085\n",
      "Epoch 105/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5004 - val_loss: 0.5856\n",
      "Epoch 106/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5297 - val_loss: 0.5876\n",
      "Epoch 107/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5197 - val_loss: 0.5662\n",
      "Epoch 108/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5923 - val_loss: 0.9730\n",
      "Epoch 109/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5191 - val_loss: 0.6563\n",
      "Epoch 110/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5154 - val_loss: 0.6618\n",
      "Epoch 111/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5297 - val_loss: 0.7774\n",
      "Epoch 112/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4817 - val_loss: 0.5993\n",
      "Epoch 113/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5388 - val_loss: 0.6485\n",
      "Epoch 114/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5177 - val_loss: 0.7232\n",
      "Epoch 115/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5316 - val_loss: 0.8413\n",
      "Epoch 116/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4866 - val_loss: 0.7707\n",
      "Epoch 117/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4896 - val_loss: 0.8217\n",
      "Epoch 118/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5239 - val_loss: 0.7744\n",
      "Epoch 119/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5008 - val_loss: 0.8026\n",
      "Epoch 120/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5598 - val_loss: 0.8680\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 121/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4748 - val_loss: 0.7021\n",
      "Epoch 122/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4569 - val_loss: 0.5348\n",
      "Epoch 123/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4767 - val_loss: 0.4849\n",
      "Epoch 124/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4842 - val_loss: 0.5249\n",
      "Epoch 125/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4776 - val_loss: 0.6373\n",
      "Epoch 126/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4797 - val_loss: 0.5706\n",
      "Epoch 127/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4215 - val_loss: 0.6172\n",
      "Epoch 128/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4844 - val_loss: 0.5763\n",
      "Epoch 129/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4675 - val_loss: 0.5469\n",
      "Epoch 130/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4751 - val_loss: 0.5203\n",
      "Epoch 131/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4874 - val_loss: 0.6985\n",
      "Epoch 132/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4644 - val_loss: 0.5268\n",
      "Epoch 133/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5014 - val_loss: 0.5923\n",
      "Epoch 134/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4498 - val_loss: 0.5837\n",
      "Epoch 135/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4804 - val_loss: 0.4953\n",
      "Epoch 136/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4436 - val_loss: 0.4780\n",
      "Epoch 137/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4897 - val_loss: 0.5393\n",
      "Epoch 138/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4707 - val_loss: 0.5102\n",
      "Epoch 139/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4691 - val_loss: 0.6419\n",
      "Epoch 140/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4360 - val_loss: 0.6142\n",
      "Epoch 141/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4548 - val_loss: 0.5606\n",
      "Epoch 142/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4354 - val_loss: 0.5287\n",
      "Epoch 143/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4431 - val_loss: 0.5254\n",
      "Epoch 144/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4890 - val_loss: 0.6002\n",
      "Epoch 145/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4361 - val_loss: 0.5629\n",
      "Epoch 146/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4691 - val_loss: 0.5859\n",
      "Epoch 147/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4498 - val_loss: 0.5906\n",
      "Epoch 148/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4532 - val_loss: 0.6218\n",
      "Epoch 149/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4814 - val_loss: 0.5116\n",
      "Epoch 150/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4770 - val_loss: 0.5064\n",
      "Epoch 151/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5005 - val_loss: 0.5183\n",
      "Epoch 152/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4468 - val_loss: 0.5519\n",
      "Epoch 153/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4654 - val_loss: 0.6028\n",
      "Epoch 154/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4742 - val_loss: 0.5120\n",
      "Epoch 155/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4708 - val_loss: 0.5240\n",
      "Epoch 156/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4411 - val_loss: 0.5694\n",
      "Epoch 157/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4782 - val_loss: 0.5424\n",
      "Epoch 158/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4989 - val_loss: 0.4771\n",
      "Epoch 159/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4607 - val_loss: 0.6098\n",
      "Epoch 160/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4940 - val_loss: 0.4910\n",
      "Epoch 161/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4649 - val_loss: 0.5108\n",
      "Epoch 162/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4333 - val_loss: 0.5255\n",
      "Epoch 163/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4398 - val_loss: 0.5105\n",
      "Epoch 164/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4083 - val_loss: 0.4967\n",
      "Epoch 165/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4249 - val_loss: 0.5275\n",
      "Epoch 166/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4828 - val_loss: 0.5989\n",
      "Epoch 167/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4400 - val_loss: 0.5205\n",
      "Epoch 168/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4432 - val_loss: 0.4839\n",
      "Epoch 169/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4343 - val_loss: 0.5317\n",
      "Epoch 170/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4624 - val_loss: 0.5408\n",
      "Epoch 171/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4415 - val_loss: 0.5107\n",
      "Epoch 172/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4609 - val_loss: 0.5657\n",
      "Epoch 173/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4325 - val_loss: 0.5715\n",
      "Epoch 174/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4166 - val_loss: 0.4926\n",
      "Epoch 175/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4132 - val_loss: 0.4957\n",
      "Epoch 176/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4048 - val_loss: 0.5086\n",
      "Epoch 177/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4364 - val_loss: 0.5639\n",
      "Epoch 178/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4563 - val_loss: 0.6612\n",
      "Epoch 179/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4865 - val_loss: 0.4669\n",
      "Epoch 180/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4733 - val_loss: 0.5248\n",
      "Epoch 181/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4440 - val_loss: 0.5234\n",
      "Epoch 182/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4479 - val_loss: 0.5876\n",
      "Epoch 183/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4633 - val_loss: 0.4874\n",
      "Epoch 184/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.5134 - val_loss: 0.5201\n",
      "Epoch 185/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4729 - val_loss: 0.4909\n",
      "Epoch 186/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4474 - val_loss: 0.4761\n",
      "Epoch 187/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4347 - val_loss: 0.4604\n",
      "Epoch 188/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4505 - val_loss: 0.5393\n",
      "Epoch 189/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4500 - val_loss: 0.5105\n",
      "Epoch 190/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4830 - val_loss: 0.7356\n",
      "Epoch 191/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4653 - val_loss: 0.5556\n",
      "Epoch 192/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4548 - val_loss: 0.5406\n",
      "Epoch 193/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4800 - val_loss: 0.5377\n",
      "Epoch 194/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4506 - val_loss: 0.5142\n",
      "Epoch 195/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4653 - val_loss: 0.5073\n",
      "Epoch 196/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3870 - val_loss: 0.5057\n",
      "Epoch 197/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4347 - val_loss: 0.5523\n",
      "Epoch 198/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4478 - val_loss: 0.6225\n",
      "Epoch 199/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4221 - val_loss: 0.4850\n",
      "Epoch 200/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4407 - val_loss: 0.5417\n",
      "Epoch 201/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4594 - val_loss: 0.5874\n",
      "Epoch 202/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4703 - val_loss: 0.7132\n",
      "Epoch 203/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4698 - val_loss: 0.5693\n",
      "Epoch 204/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4275 - val_loss: 0.5296\n",
      "Epoch 205/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4500 - val_loss: 0.6097\n",
      "Epoch 206/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.5025 - val_loss: 0.5477\n",
      "Epoch 207/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4434 - val_loss: 0.6249\n",
      "Epoch 208/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4190 - val_loss: 0.4832\n",
      "Epoch 209/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4457 - val_loss: 0.5758\n",
      "Epoch 210/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4670 - val_loss: 0.5279\n",
      "Epoch 211/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4311 - val_loss: 0.5513\n",
      "Epoch 212/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4910 - val_loss: 0.4616\n",
      "Epoch 213/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4042 - val_loss: 0.5834\n",
      "Epoch 214/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4295 - val_loss: 0.5532\n",
      "Epoch 215/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4536 - val_loss: 0.4772\n",
      "Epoch 216/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4721 - val_loss: 0.6880\n",
      "Epoch 217/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4458 - val_loss: 0.5457\n",
      "\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 218/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4649 - val_loss: 0.4720\n",
      "Epoch 219/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4366 - val_loss: 0.4741\n",
      "Epoch 220/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4175 - val_loss: 0.5974\n",
      "Epoch 221/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4545 - val_loss: 0.5424\n",
      "Epoch 222/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4121 - val_loss: 0.5435\n",
      "Epoch 223/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4265 - val_loss: 0.5347\n",
      "Epoch 224/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4047 - val_loss: 0.4497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3894 - val_loss: 0.4720\n",
      "Epoch 226/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4197 - val_loss: 0.4520\n",
      "Epoch 227/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4238 - val_loss: 0.4520\n",
      "Epoch 228/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4690 - val_loss: 0.4915\n",
      "Epoch 229/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4229 - val_loss: 0.5470\n",
      "Epoch 230/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4068 - val_loss: 0.4422\n",
      "Epoch 231/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4431 - val_loss: 0.4423\n",
      "Epoch 232/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4355 - val_loss: 0.5784\n",
      "Epoch 233/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4200 - val_loss: 0.4657\n",
      "Epoch 234/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4253 - val_loss: 0.5744\n",
      "Epoch 235/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4301 - val_loss: 0.4561\n",
      "Epoch 236/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3889 - val_loss: 0.4484\n",
      "Epoch 237/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4261 - val_loss: 0.4418\n",
      "Epoch 238/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3762 - val_loss: 0.4482\n",
      "Epoch 239/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4252 - val_loss: 0.4513\n",
      "Epoch 240/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3943 - val_loss: 0.4480\n",
      "Epoch 241/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3898 - val_loss: 0.4693\n",
      "Epoch 242/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4542 - val_loss: 0.4983\n",
      "Epoch 243/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4091 - val_loss: 0.4626\n",
      "Epoch 244/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4063 - val_loss: 0.4737\n",
      "Epoch 245/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4183 - val_loss: 0.4462\n",
      "Epoch 246/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3846 - val_loss: 0.4475\n",
      "Epoch 247/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4125 - val_loss: 0.4695\n",
      "Epoch 248/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4169 - val_loss: 0.5038\n",
      "Epoch 249/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4172 - val_loss: 0.4646\n",
      "Epoch 250/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4085 - val_loss: 0.4468\n",
      "Epoch 251/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3976 - val_loss: 0.4412\n",
      "Epoch 252/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4203 - val_loss: 0.4615\n",
      "Epoch 253/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3799 - val_loss: 0.4490\n",
      "Epoch 254/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4052 - val_loss: 0.4591\n",
      "Epoch 255/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3983 - val_loss: 0.4432\n",
      "Epoch 256/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4672 - val_loss: 0.4881\n",
      "Epoch 257/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4443 - val_loss: 0.5235\n",
      "Epoch 258/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3904 - val_loss: 0.4748\n",
      "Epoch 259/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4538 - val_loss: 0.4763\n",
      "Epoch 260/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4240 - val_loss: 0.4784\n",
      "Epoch 261/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4123 - val_loss: 0.5540\n",
      "Epoch 262/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4120 - val_loss: 0.4853\n",
      "Epoch 263/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4460 - val_loss: 0.4362\n",
      "Epoch 264/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3975 - val_loss: 0.4501\n",
      "Epoch 265/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4833 - val_loss: 0.4895\n",
      "Epoch 266/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4067 - val_loss: 0.4464\n",
      "Epoch 267/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4441 - val_loss: 0.5042\n",
      "Epoch 268/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4584 - val_loss: 0.5095\n",
      "Epoch 269/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4113 - val_loss: 0.4576\n",
      "Epoch 270/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3921 - val_loss: 0.4479\n",
      "Epoch 271/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4706 - val_loss: 0.5642\n",
      "Epoch 272/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4146 - val_loss: 0.4558\n",
      "Epoch 273/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4153 - val_loss: 0.4771\n",
      "Epoch 274/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4483 - val_loss: 0.4821\n",
      "Epoch 275/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4015 - val_loss: 0.4765\n",
      "Epoch 276/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4072 - val_loss: 0.4457\n",
      "Epoch 277/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4230 - val_loss: 0.4381\n",
      "Epoch 278/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3968 - val_loss: 0.4499\n",
      "Epoch 279/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4481 - val_loss: 0.4665\n",
      "Epoch 280/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4181 - val_loss: 0.5992\n",
      "Epoch 281/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4302 - val_loss: 0.4983\n",
      "Epoch 282/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4131 - val_loss: 0.4418\n",
      "Epoch 283/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4192 - val_loss: 0.5330\n",
      "Epoch 284/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3774 - val_loss: 0.4997\n",
      "Epoch 285/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4167 - val_loss: 0.4611\n",
      "Epoch 286/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4047 - val_loss: 0.4598\n",
      "Epoch 287/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4712 - val_loss: 0.4464\n",
      "Epoch 288/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4074 - val_loss: 0.4595\n",
      "Epoch 289/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3951 - val_loss: 0.4437\n",
      "Epoch 290/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4083 - val_loss: 0.5312\n",
      "Epoch 291/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4085 - val_loss: 0.4593\n",
      "Epoch 292/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4081 - val_loss: 0.4406\n",
      "Epoch 293/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4557 - val_loss: 0.4469\n",
      "\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 294/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3980 - val_loss: 0.4381\n",
      "Epoch 295/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4058 - val_loss: 0.4418\n",
      "Epoch 296/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3905 - val_loss: 0.4297\n",
      "Epoch 297/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3859 - val_loss: 0.4335\n",
      "Epoch 298/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3931 - val_loss: 0.4239\n",
      "Epoch 299/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4246 - val_loss: 0.4822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3803 - val_loss: 0.4247\n",
      "Epoch 301/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3728 - val_loss: 0.4214\n",
      "Epoch 302/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4797 - val_loss: 0.4241\n",
      "Epoch 303/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3994 - val_loss: 0.4631\n",
      "Epoch 304/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4267 - val_loss: 0.4303\n",
      "Epoch 305/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3957 - val_loss: 0.4737\n",
      "Epoch 306/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3842 - val_loss: 0.4427\n",
      "Epoch 307/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3851 - val_loss: 0.4777\n",
      "Epoch 308/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3982 - val_loss: 0.4397\n",
      "Epoch 309/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4036 - val_loss: 0.4353\n",
      "Epoch 310/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3907 - val_loss: 0.4310\n",
      "Epoch 311/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4110 - val_loss: 0.4224\n",
      "Epoch 312/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4107 - val_loss: 0.4386\n",
      "Epoch 313/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3944 - val_loss: 0.4708\n",
      "Epoch 314/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4392 - val_loss: 0.4480\n",
      "Epoch 315/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4152 - val_loss: 0.4376\n",
      "Epoch 316/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4002 - val_loss: 0.4404\n",
      "Epoch 317/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3809 - val_loss: 0.4366\n",
      "Epoch 318/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3960 - val_loss: 0.4420\n",
      "Epoch 319/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4119 - val_loss: 0.4273\n",
      "Epoch 320/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4161 - val_loss: 0.4365\n",
      "Epoch 321/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4205 - val_loss: 0.4226\n",
      "Epoch 322/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3873 - val_loss: 0.4249\n",
      "Epoch 323/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4308 - val_loss: 0.4262\n",
      "Epoch 324/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3770 - val_loss: 0.4358\n",
      "Epoch 325/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3821 - val_loss: 0.4422\n",
      "Epoch 326/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3998 - val_loss: 0.4438\n",
      "Epoch 327/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4027 - val_loss: 0.4437\n",
      "Epoch 328/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3992 - val_loss: 0.4390\n",
      "Epoch 329/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3985 - val_loss: 0.4889\n",
      "Epoch 330/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4048 - val_loss: 0.4254\n",
      "Epoch 331/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3481 - val_loss: 0.4659\n",
      "\n",
      "Epoch 00331: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 332/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4076 - val_loss: 0.4258\n",
      "Epoch 333/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3641 - val_loss: 0.4222\n",
      "Epoch 334/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3762 - val_loss: 0.4191\n",
      "Epoch 335/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4105 - val_loss: 0.4182\n",
      "Epoch 336/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4097 - val_loss: 0.4225\n",
      "Epoch 337/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4073 - val_loss: 0.4401\n",
      "Epoch 338/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4164 - val_loss: 0.4307\n",
      "Epoch 339/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3731 - val_loss: 0.4217\n",
      "Epoch 340/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4188 - val_loss: 0.4215\n",
      "Epoch 341/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3974 - val_loss: 0.4198\n",
      "Epoch 342/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4042 - val_loss: 0.4190\n",
      "Epoch 343/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3558 - val_loss: 0.4167\n",
      "Epoch 344/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3897 - val_loss: 0.4231\n",
      "Epoch 345/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4235 - val_loss: 0.4192\n",
      "Epoch 346/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4029 - val_loss: 0.4282\n",
      "Epoch 347/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3747 - val_loss: 0.4262\n",
      "Epoch 348/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3990 - val_loss: 0.4369\n",
      "Epoch 349/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3998 - val_loss: 0.4170\n",
      "Epoch 350/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4305 - val_loss: 0.4215\n",
      "Epoch 351/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3915 - val_loss: 0.4271\n",
      "Epoch 352/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3997 - val_loss: 0.4340\n",
      "Epoch 353/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4167 - val_loss: 0.4171\n",
      "Epoch 354/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3912 - val_loss: 0.4352\n",
      "Epoch 355/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3978 - val_loss: 0.4170\n",
      "Epoch 356/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4048 - val_loss: 0.4287\n",
      "Epoch 357/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4046 - val_loss: 0.4175\n",
      "Epoch 358/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3636 - val_loss: 0.4184\n",
      "Epoch 359/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3756 - val_loss: 0.4434\n",
      "Epoch 360/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4087 - val_loss: 0.4194\n",
      "Epoch 361/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3903 - val_loss: 0.4357\n",
      "Epoch 362/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4257 - val_loss: 0.4639\n",
      "Epoch 363/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4071 - val_loss: 0.4181\n",
      "Epoch 364/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3889 - val_loss: 0.4177\n",
      "Epoch 365/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3976 - val_loss: 0.4167\n",
      "Epoch 366/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4017 - val_loss: 0.4291\n",
      "Epoch 367/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4091 - val_loss: 0.4189\n",
      "Epoch 368/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4334 - val_loss: 0.4366\n",
      "Epoch 369/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3852 - val_loss: 0.4203\n",
      "Epoch 370/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4179 - val_loss: 0.4215\n",
      "Epoch 371/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3763 - val_loss: 0.4392\n",
      "Epoch 372/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4118 - val_loss: 0.4198\n",
      "Epoch 373/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4039 - val_loss: 0.4190\n",
      "\n",
      "Epoch 00373: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 374/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4294 - val_loss: 0.4159\n",
      "Epoch 375/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4223 - val_loss: 0.4172\n",
      "Epoch 376/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4354 - val_loss: 0.4172\n",
      "Epoch 377/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4072 - val_loss: 0.4162\n",
      "Epoch 378/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3870 - val_loss: 0.4186\n",
      "Epoch 379/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4047 - val_loss: 0.4192\n",
      "Epoch 380/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3836 - val_loss: 0.4152\n",
      "Epoch 381/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4013 - val_loss: 0.4494\n",
      "Epoch 382/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3743 - val_loss: 0.4186\n",
      "Epoch 383/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3826 - val_loss: 0.4273\n",
      "Epoch 384/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3596 - val_loss: 0.4164\n",
      "Epoch 385/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3969 - val_loss: 0.4183\n",
      "Epoch 386/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4171 - val_loss: 0.4170\n",
      "Epoch 387/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4187 - val_loss: 0.4203\n",
      "Epoch 388/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3781 - val_loss: 0.4174\n",
      "Epoch 389/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4148 - val_loss: 0.4154\n",
      "Epoch 390/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3765 - val_loss: 0.4217\n",
      "Epoch 391/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4056 - val_loss: 0.4182\n",
      "Epoch 392/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3901 - val_loss: 0.4182\n",
      "Epoch 393/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3833 - val_loss: 0.4154\n",
      "Epoch 394/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4001 - val_loss: 0.4269\n",
      "Epoch 395/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3746 - val_loss: 0.4231\n",
      "Epoch 396/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4021 - val_loss: 0.4164\n",
      "Epoch 397/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3907 - val_loss: 0.4181\n",
      "Epoch 398/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3953 - val_loss: 0.4154\n",
      "Epoch 399/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4063 - val_loss: 0.4171\n",
      "Epoch 400/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3952 - val_loss: 0.4178\n",
      "Epoch 401/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4274 - val_loss: 0.4148\n",
      "Epoch 402/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3723 - val_loss: 0.4149\n",
      "Epoch 403/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4318 - val_loss: 0.4155\n",
      "Epoch 404/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4047 - val_loss: 0.4177\n",
      "Epoch 405/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3756 - val_loss: 0.4144\n",
      "Epoch 406/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4314 - val_loss: 0.4225\n",
      "Epoch 407/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3785 - val_loss: 0.4175\n",
      "Epoch 408/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3738 - val_loss: 0.4156\n",
      "Epoch 409/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3858 - val_loss: 0.4151\n",
      "Epoch 410/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3971 - val_loss: 0.4176\n",
      "Epoch 411/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3989 - val_loss: 0.4145\n",
      "Epoch 412/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4042 - val_loss: 0.4192\n",
      "Epoch 413/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4032 - val_loss: 0.4186\n",
      "Epoch 414/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3785 - val_loss: 0.4151\n",
      "Epoch 415/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3530 - val_loss: 0.4157\n",
      "Epoch 416/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3629 - val_loss: 0.4180\n",
      "Epoch 417/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3956 - val_loss: 0.4183\n",
      "Epoch 418/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3946 - val_loss: 0.4150\n",
      "Epoch 419/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3490 - val_loss: 0.4148\n",
      "Epoch 420/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4385 - val_loss: 0.4190\n",
      "Epoch 421/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3971 - val_loss: 0.4171\n",
      "Epoch 422/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3617 - val_loss: 0.4211\n",
      "Epoch 423/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3910 - val_loss: 0.4173\n",
      "Epoch 424/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4162 - val_loss: 0.4151\n",
      "Epoch 425/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3803 - val_loss: 0.4144\n",
      "Epoch 426/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3693 - val_loss: 0.4144\n",
      "Epoch 427/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3806 - val_loss: 0.4190\n",
      "Epoch 428/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3703 - val_loss: 0.4247\n",
      "Epoch 429/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3719 - val_loss: 0.4180\n",
      "Epoch 430/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3477 - val_loss: 0.4200\n",
      "Epoch 431/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3889 - val_loss: 0.4222\n",
      "Epoch 432/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3895 - val_loss: 0.4152\n",
      "Epoch 433/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3518 - val_loss: 0.4140\n",
      "Epoch 434/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3970 - val_loss: 0.4191\n",
      "Epoch 435/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3506 - val_loss: 0.4219\n",
      "Epoch 436/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3830 - val_loss: 0.4147\n",
      "Epoch 437/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3888 - val_loss: 0.4262\n",
      "Epoch 438/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4204 - val_loss: 0.4185\n",
      "Epoch 439/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3864 - val_loss: 0.4147\n",
      "Epoch 440/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3563 - val_loss: 0.4154\n",
      "Epoch 441/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4088 - val_loss: 0.4242\n",
      "Epoch 442/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3617 - val_loss: 0.4183\n",
      "Epoch 443/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4295 - val_loss: 0.4162\n",
      "Epoch 444/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3770 - val_loss: 0.4162\n",
      "Epoch 445/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4059 - val_loss: 0.4178\n",
      "Epoch 446/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4008 - val_loss: 0.4145\n",
      "Epoch 447/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3923 - val_loss: 0.4153\n",
      "Epoch 448/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3942 - val_loss: 0.4339\n",
      "Epoch 449/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4226 - val_loss: 0.4201\n",
      "Epoch 450/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3794 - val_loss: 0.4242\n",
      "Epoch 451/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3868 - val_loss: 0.4147\n",
      "Epoch 452/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3684 - val_loss: 0.4138\n",
      "Epoch 453/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3502 - val_loss: 0.4308\n",
      "Epoch 454/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3367 - val_loss: 0.4159\n",
      "Epoch 455/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4052 - val_loss: 0.4153\n",
      "Epoch 456/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3739 - val_loss: 0.4150\n",
      "Epoch 457/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4191 - val_loss: 0.4193\n",
      "Epoch 458/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3950 - val_loss: 0.4192\n",
      "Epoch 459/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3709 - val_loss: 0.4226\n",
      "Epoch 460/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4580 - val_loss: 0.4146\n",
      "Epoch 461/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4039 - val_loss: 0.4159\n",
      "Epoch 462/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3540 - val_loss: 0.4165\n",
      "Epoch 463/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4065 - val_loss: 0.4175\n",
      "Epoch 464/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4577 - val_loss: 0.4163\n",
      "Epoch 465/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4085 - val_loss: 0.4171\n",
      "Epoch 466/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3900 - val_loss: 0.4143\n",
      "Epoch 467/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4315 - val_loss: 0.4220\n",
      "Epoch 468/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4111 - val_loss: 0.4155\n",
      "Epoch 469/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3792 - val_loss: 0.4222\n",
      "Epoch 470/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3924 - val_loss: 0.4147\n",
      "Epoch 471/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4298 - val_loss: 0.4161\n",
      "Epoch 472/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3666 - val_loss: 0.4146\n",
      "Epoch 473/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4098 - val_loss: 0.4145\n",
      "Epoch 474/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3485 - val_loss: 0.4158\n",
      "Epoch 475/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4136 - val_loss: 0.4199\n",
      "Epoch 476/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3884 - val_loss: 0.4147\n",
      "Epoch 477/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4032 - val_loss: 0.4154\n",
      "Epoch 478/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3852 - val_loss: 0.4148\n",
      "Epoch 479/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4085 - val_loss: 0.4155\n",
      "Epoch 480/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3953 - val_loss: 0.4224\n",
      "Epoch 481/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3829 - val_loss: 0.4186\n",
      "Epoch 482/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3670 - val_loss: 0.4157\n",
      "\n",
      "Epoch 00482: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 483/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3468 - val_loss: 0.4179\n",
      "Epoch 484/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3556 - val_loss: 0.4141\n",
      "Epoch 485/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3925 - val_loss: 0.4144\n",
      "Epoch 486/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4165 - val_loss: 0.4140\n",
      "Epoch 487/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4109 - val_loss: 0.4170\n",
      "Epoch 488/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3872 - val_loss: 0.4138\n",
      "Epoch 489/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3639 - val_loss: 0.4143\n",
      "Epoch 490/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3877 - val_loss: 0.4183\n",
      "Epoch 491/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3759 - val_loss: 0.4135\n",
      "Epoch 492/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3623 - val_loss: 0.4170\n",
      "Epoch 493/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3901 - val_loss: 0.4142\n",
      "Epoch 494/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3951 - val_loss: 0.4135\n",
      "Epoch 495/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3801 - val_loss: 0.4180\n",
      "Epoch 496/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4094 - val_loss: 0.4131\n",
      "Epoch 497/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4102 - val_loss: 0.4132\n",
      "Epoch 498/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3929 - val_loss: 0.4141\n",
      "Epoch 499/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4462 - val_loss: 0.4147\n",
      "Epoch 500/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3683 - val_loss: 0.4133\n",
      "Epoch 501/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3911 - val_loss: 0.4144\n",
      "Epoch 502/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3652 - val_loss: 0.4139\n",
      "Epoch 503/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3988 - val_loss: 0.4134\n",
      "Epoch 504/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3700 - val_loss: 0.4145\n",
      "Epoch 505/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3663 - val_loss: 0.4203\n",
      "Epoch 506/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3520 - val_loss: 0.4179\n",
      "Epoch 507/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3902 - val_loss: 0.4138\n",
      "Epoch 508/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4119 - val_loss: 0.4156\n",
      "Epoch 509/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3894 - val_loss: 0.4159\n",
      "Epoch 510/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3863 - val_loss: 0.4150\n",
      "Epoch 511/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4197 - val_loss: 0.4170\n",
      "Epoch 512/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3753 - val_loss: 0.4165\n",
      "Epoch 513/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3804 - val_loss: 0.4134\n",
      "Epoch 514/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3746 - val_loss: 0.4197\n",
      "Epoch 515/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3910 - val_loss: 0.4130\n",
      "Epoch 516/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3976 - val_loss: 0.4129\n",
      "Epoch 517/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3901 - val_loss: 0.4177\n",
      "Epoch 518/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3987 - val_loss: 0.4132\n",
      "Epoch 519/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4147 - val_loss: 0.4145\n",
      "Epoch 520/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3845 - val_loss: 0.4183\n",
      "Epoch 521/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3815 - val_loss: 0.4165\n",
      "Epoch 522/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3930 - val_loss: 0.4138\n",
      "Epoch 523/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3765 - val_loss: 0.4135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 524/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3914 - val_loss: 0.4140\n",
      "Epoch 525/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4524 - val_loss: 0.4137\n",
      "Epoch 526/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3971 - val_loss: 0.4137\n",
      "Epoch 527/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4030 - val_loss: 0.4146\n",
      "Epoch 528/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4114 - val_loss: 0.4133\n",
      "Epoch 529/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3737 - val_loss: 0.4140\n",
      "Epoch 530/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4130 - val_loss: 0.4149\n",
      "Epoch 531/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4123 - val_loss: 0.4150\n",
      "Epoch 532/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4292 - val_loss: 0.4145\n",
      "Epoch 533/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3966 - val_loss: 0.4131\n",
      "Epoch 534/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3839 - val_loss: 0.4137\n",
      "Epoch 535/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4027 - val_loss: 0.4186\n",
      "Epoch 536/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4043 - val_loss: 0.4135\n",
      "Epoch 537/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3986 - val_loss: 0.4130\n",
      "Epoch 538/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4194 - val_loss: 0.4132\n",
      "Epoch 539/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3908 - val_loss: 0.4139\n",
      "Epoch 540/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3875 - val_loss: 0.4133\n",
      "Epoch 541/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3812 - val_loss: 0.4130\n",
      "Epoch 542/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4006 - val_loss: 0.4140\n",
      "Epoch 543/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3726 - val_loss: 0.4126\n",
      "Epoch 544/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3918 - val_loss: 0.4136\n",
      "Epoch 545/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4153 - val_loss: 0.4155\n",
      "Epoch 546/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3868 - val_loss: 0.4141\n",
      "Epoch 547/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3381 - val_loss: 0.4141\n",
      "Epoch 548/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3438 - val_loss: 0.4129\n",
      "Epoch 549/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3967 - val_loss: 0.4128\n",
      "Epoch 550/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3581 - val_loss: 0.4166\n",
      "Epoch 551/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3869 - val_loss: 0.4128\n",
      "Epoch 552/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3840 - val_loss: 0.4130\n",
      "Epoch 553/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3637 - val_loss: 0.4132\n",
      "Epoch 554/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3644 - val_loss: 0.4157\n",
      "Epoch 555/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3858 - val_loss: 0.4138\n",
      "Epoch 556/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3830 - val_loss: 0.4149\n",
      "Epoch 557/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3908 - val_loss: 0.4133\n",
      "Epoch 558/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3980 - val_loss: 0.4140\n",
      "Epoch 559/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3579 - val_loss: 0.4145\n",
      "Epoch 560/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4151 - val_loss: 0.4139\n",
      "Epoch 561/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3491 - val_loss: 0.4131\n",
      "Epoch 562/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3910 - val_loss: 0.4156\n",
      "Epoch 563/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4477 - val_loss: 0.4127\n",
      "Epoch 564/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3611 - val_loss: 0.4130\n",
      "Epoch 565/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4027 - val_loss: 0.4134\n",
      "Epoch 566/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3756 - val_loss: 0.4132\n",
      "Epoch 567/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4035 - val_loss: 0.4157\n",
      "Epoch 568/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4109 - val_loss: 0.4129\n",
      "Epoch 569/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3920 - val_loss: 0.4132\n",
      "Epoch 570/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4061 - val_loss: 0.4138\n",
      "Epoch 571/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3588 - val_loss: 0.4128\n",
      "Epoch 572/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3873 - val_loss: 0.4131\n",
      "Epoch 573/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3915 - val_loss: 0.4131\n",
      "\n",
      "Epoch 00573: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 574/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3842 - val_loss: 0.4125\n",
      "Epoch 575/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3976 - val_loss: 0.4126\n",
      "Epoch 576/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3826 - val_loss: 0.4121\n",
      "Epoch 577/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3640 - val_loss: 0.4125\n",
      "Epoch 578/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3974 - val_loss: 0.4121\n",
      "Epoch 579/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3928 - val_loss: 0.4129\n",
      "Epoch 580/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3953 - val_loss: 0.4130\n",
      "Epoch 581/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4764 - val_loss: 0.4123\n",
      "Epoch 582/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4006 - val_loss: 0.4145\n",
      "Epoch 583/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3670 - val_loss: 0.4124\n",
      "Epoch 584/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3735 - val_loss: 0.4124\n",
      "Epoch 585/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3479 - val_loss: 0.4124\n",
      "Epoch 586/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4213 - val_loss: 0.4123\n",
      "Epoch 587/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4118 - val_loss: 0.4128\n",
      "Epoch 588/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3948 - val_loss: 0.4127\n",
      "Epoch 589/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3661 - val_loss: 0.4126\n",
      "Epoch 590/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4253 - val_loss: 0.4137\n",
      "Epoch 591/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4075 - val_loss: 0.4127\n",
      "Epoch 592/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3677 - val_loss: 0.4134\n",
      "Epoch 593/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3861 - val_loss: 0.4126\n",
      "Epoch 594/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3903 - val_loss: 0.4127\n",
      "Epoch 595/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4038 - val_loss: 0.4129\n",
      "Epoch 596/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4161 - val_loss: 0.4127\n",
      "Epoch 597/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3770 - val_loss: 0.4124\n",
      "Epoch 598/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3918 - val_loss: 0.4126\n",
      "Epoch 599/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3957 - val_loss: 0.4134\n",
      "Epoch 600/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3861 - val_loss: 0.4125\n",
      "Epoch 601/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3467 - val_loss: 0.4124\n",
      "Epoch 602/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3975 - val_loss: 0.4129\n",
      "Epoch 603/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3784 - val_loss: 0.4125\n",
      "Epoch 604/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3950 - val_loss: 0.4126\n",
      "Epoch 605/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3914 - val_loss: 0.4148\n",
      "Epoch 606/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4290 - val_loss: 0.4139\n",
      "\n",
      "Epoch 00606: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 607/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3780 - val_loss: 0.4122\n",
      "Epoch 608/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4000 - val_loss: 0.4123\n",
      "Epoch 609/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3760 - val_loss: 0.4124\n",
      "Epoch 610/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3937 - val_loss: 0.4122\n",
      "Epoch 611/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3654 - val_loss: 0.4123\n",
      "Epoch 612/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3819 - val_loss: 0.4121\n",
      "Epoch 613/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3847 - val_loss: 0.4120\n",
      "Epoch 614/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3944 - val_loss: 0.4127\n",
      "Epoch 615/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3629 - val_loss: 0.4122\n",
      "Epoch 616/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4124 - val_loss: 0.4120\n",
      "Epoch 617/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3979 - val_loss: 0.4120\n",
      "Epoch 618/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3597 - val_loss: 0.4123\n",
      "Epoch 619/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3720 - val_loss: 0.4123\n",
      "Epoch 620/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3870 - val_loss: 0.4122\n",
      "Epoch 621/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3654 - val_loss: 0.4121\n",
      "Epoch 622/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3901 - val_loss: 0.4123\n",
      "Epoch 623/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4038 - val_loss: 0.4122\n",
      "Epoch 624/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3496 - val_loss: 0.4123\n",
      "Epoch 625/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3813 - val_loss: 0.4122\n",
      "Epoch 626/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4023 - val_loss: 0.4127\n",
      "Epoch 627/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4119 - val_loss: 0.4127\n",
      "Epoch 628/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3473 - val_loss: 0.4123\n",
      "Epoch 629/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3723 - val_loss: 0.4122\n",
      "Epoch 630/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4069 - val_loss: 0.4124\n",
      "Epoch 631/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3763 - val_loss: 0.4124\n",
      "Epoch 632/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4030 - val_loss: 0.4124\n",
      "Epoch 633/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3458 - val_loss: 0.4124\n",
      "Epoch 634/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3739 - val_loss: 0.4122\n",
      "Epoch 635/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3793 - val_loss: 0.4119\n",
      "Epoch 636/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3390 - val_loss: 0.4120\n",
      "Epoch 637/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3906 - val_loss: 0.4121\n",
      "Epoch 638/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3951 - val_loss: 0.4122\n",
      "Epoch 639/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3877 - val_loss: 0.4119\n",
      "Epoch 640/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4398 - val_loss: 0.4120\n",
      "Epoch 641/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3592 - val_loss: 0.4121\n",
      "Epoch 642/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3934 - val_loss: 0.4121\n",
      "Epoch 643/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3806 - val_loss: 0.4119\n",
      "Epoch 644/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4145 - val_loss: 0.4121\n",
      "Epoch 645/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4060 - val_loss: 0.4122\n",
      "Epoch 646/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4004 - val_loss: 0.4121\n",
      "Epoch 647/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3995 - val_loss: 0.4127\n",
      "Epoch 648/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3624 - val_loss: 0.4124\n",
      "Epoch 649/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3571 - val_loss: 0.4126\n",
      "Epoch 650/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4133 - val_loss: 0.4120\n",
      "Epoch 651/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3467 - val_loss: 0.4119\n",
      "Epoch 652/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4237 - val_loss: 0.4121\n",
      "Epoch 653/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4162 - val_loss: 0.4122\n",
      "Epoch 654/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4012 - val_loss: 0.4120\n",
      "Epoch 655/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3355 - val_loss: 0.4119\n",
      "Epoch 656/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3832 - val_loss: 0.4119\n",
      "Epoch 657/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3882 - val_loss: 0.4121\n",
      "Epoch 658/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3849 - val_loss: 0.4118\n",
      "Epoch 659/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3442 - val_loss: 0.4119\n",
      "Epoch 660/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3685 - val_loss: 0.4117\n",
      "Epoch 661/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3819 - val_loss: 0.4117\n",
      "Epoch 662/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3877 - val_loss: 0.4121\n",
      "Epoch 663/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4111 - val_loss: 0.4118\n",
      "Epoch 664/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3667 - val_loss: 0.4118\n",
      "Epoch 665/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4301 - val_loss: 0.4123\n",
      "Epoch 666/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4209 - val_loss: 0.4127\n",
      "Epoch 667/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4118 - val_loss: 0.4129\n",
      "Epoch 668/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3860 - val_loss: 0.4117\n",
      "Epoch 669/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3858 - val_loss: 0.4117\n",
      "Epoch 670/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3562 - val_loss: 0.4127\n",
      "Epoch 671/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4152 - val_loss: 0.4121\n",
      "Epoch 672/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3738 - val_loss: 0.4117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3545 - val_loss: 0.4125\n",
      "Epoch 674/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4137 - val_loss: 0.4121\n",
      "Epoch 675/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4015 - val_loss: 0.4119\n",
      "Epoch 676/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3877 - val_loss: 0.4120\n",
      "Epoch 677/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4027 - val_loss: 0.4123\n",
      "Epoch 678/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3645 - val_loss: 0.4122\n",
      "Epoch 679/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3881 - val_loss: 0.4128\n",
      "Epoch 680/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3778 - val_loss: 0.4119\n",
      "Epoch 681/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4315 - val_loss: 0.4123\n",
      "Epoch 682/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3550 - val_loss: 0.4122\n",
      "Epoch 683/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4702 - val_loss: 0.4125\n",
      "Epoch 684/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3926 - val_loss: 0.4124\n",
      "Epoch 685/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3443 - val_loss: 0.4122\n",
      "Epoch 686/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4381 - val_loss: 0.4122\n",
      "Epoch 687/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3802 - val_loss: 0.4119\n",
      "Epoch 688/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3850 - val_loss: 0.4120\n",
      "\n",
      "Epoch 00688: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 689/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3600 - val_loss: 0.4120\n",
      "Epoch 690/2000\n",
      "34764/34764 [==============================] - 0s 8us/step - loss: 0.4135 - val_loss: 0.4120\n",
      "Epoch 691/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3758 - val_loss: 0.4120\n",
      "Epoch 692/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3874 - val_loss: 0.4120\n",
      "Epoch 693/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3848 - val_loss: 0.4120\n",
      "Epoch 694/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3440 - val_loss: 0.4121\n",
      "Epoch 695/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3578 - val_loss: 0.4121\n",
      "Epoch 696/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4078 - val_loss: 0.4121\n",
      "Epoch 697/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3730 - val_loss: 0.4119\n",
      "Epoch 698/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.4037 - val_loss: 0.4120\n",
      "Epoch 699/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3877 - val_loss: 0.4120\n",
      "Epoch 700/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3742 - val_loss: 0.4120\n",
      "Epoch 701/2000\n",
      "34764/34764 [==============================] - 0s 9us/step - loss: 0.3559 - val_loss: 0.4119\n",
      "Epoch 702/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.4136 - val_loss: 0.4119\n",
      "Epoch 703/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3628 - val_loss: 0.4119\n",
      "Epoch 704/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3514 - val_loss: 0.4119\n",
      "Epoch 705/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3734 - val_loss: 0.4119\n",
      "Epoch 706/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3708 - val_loss: 0.4119\n",
      "Epoch 707/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3835 - val_loss: 0.4119\n",
      "Epoch 708/2000\n",
      "34764/34764 [==============================] - 0s 10us/step - loss: 0.3614 - val_loss: 0.4119\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00708: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8872641645568449\n",
      "Training 2JHN out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 95787 samples, validate on 23466 samples\n",
      "Epoch 1/2000\n",
      "95787/95787 [==============================] - 1s 15us/step - loss: 1.8865 - val_loss: 0.9997\n",
      "Epoch 2/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.5512 - val_loss: 0.7251\n",
      "Epoch 3/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.4375 - val_loss: 0.4644\n",
      "Epoch 4/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.3853 - val_loss: 0.4089\n",
      "Epoch 5/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.3677 - val_loss: 0.3858\n",
      "Epoch 6/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.3408 - val_loss: 0.3372\n",
      "Epoch 7/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.3296 - val_loss: 0.3748\n",
      "Epoch 8/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.3208 - val_loss: 0.3529\n",
      "Epoch 9/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.3059 - val_loss: 0.2879\n",
      "Epoch 10/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2978 - val_loss: 0.3717\n",
      "Epoch 11/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2900 - val_loss: 0.2738\n",
      "Epoch 12/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2900 - val_loss: 0.3314\n",
      "Epoch 13/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2793 - val_loss: 0.2491\n",
      "Epoch 14/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2796 - val_loss: 0.2596\n",
      "Epoch 15/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2787 - val_loss: 0.2656\n",
      "Epoch 16/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2748 - val_loss: 0.2572\n",
      "Epoch 17/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2662 - val_loss: 0.2606\n",
      "Epoch 18/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2740 - val_loss: 0.2581\n",
      "Epoch 19/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2709 - val_loss: 0.2433\n",
      "Epoch 20/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2626 - val_loss: 0.2598\n",
      "Epoch 21/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2487 - val_loss: 0.2306\n",
      "Epoch 22/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2487 - val_loss: 0.2393\n",
      "Epoch 23/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2520 - val_loss: 0.2520\n",
      "Epoch 24/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2419 - val_loss: 0.2370\n",
      "Epoch 25/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2481 - val_loss: 0.2332\n",
      "Epoch 26/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2512 - val_loss: 0.2589\n",
      "Epoch 27/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2451 - val_loss: 0.2776\n",
      "Epoch 28/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2395 - val_loss: 0.2254\n",
      "Epoch 29/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2391 - val_loss: 0.2408\n",
      "Epoch 30/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2414 - val_loss: 0.2215\n",
      "Epoch 31/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2380 - val_loss: 0.2143\n",
      "Epoch 32/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2341 - val_loss: 0.2218\n",
      "Epoch 33/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2319 - val_loss: 0.2311\n",
      "Epoch 34/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2260 - val_loss: 0.2265\n",
      "Epoch 35/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2295 - val_loss: 0.2077\n",
      "Epoch 36/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2361 - val_loss: 0.2481\n",
      "Epoch 37/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2331 - val_loss: 0.2461\n",
      "Epoch 38/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2229 - val_loss: 0.2281\n",
      "Epoch 39/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2278 - val_loss: 0.2516\n",
      "Epoch 40/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2223 - val_loss: 0.2086\n",
      "Epoch 41/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2268 - val_loss: 0.2478\n",
      "Epoch 42/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2239 - val_loss: 0.2038\n",
      "Epoch 43/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2162 - val_loss: 0.2082\n",
      "Epoch 44/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2261 - val_loss: 0.2229\n",
      "Epoch 45/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2189 - val_loss: 0.2150\n",
      "Epoch 46/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2187 - val_loss: 0.2582\n",
      "Epoch 47/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2234 - val_loss: 0.2241\n",
      "Epoch 48/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2156 - val_loss: 0.2237\n",
      "Epoch 49/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2167 - val_loss: 0.2106\n",
      "Epoch 50/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2145 - val_loss: 0.2014\n",
      "Epoch 51/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2145 - val_loss: 0.2194\n",
      "Epoch 52/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2125 - val_loss: 0.2154\n",
      "Epoch 53/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2113 - val_loss: 0.2150\n",
      "Epoch 54/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2071 - val_loss: 0.2083\n",
      "Epoch 55/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2133 - val_loss: 0.1995\n",
      "Epoch 56/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2134 - val_loss: 0.2195\n",
      "Epoch 57/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2110 - val_loss: 0.2350\n",
      "Epoch 58/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2112 - val_loss: 0.2137\n",
      "Epoch 59/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2017 - val_loss: 0.2013\n",
      "Epoch 60/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2053 - val_loss: 0.2471\n",
      "Epoch 61/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2063 - val_loss: 0.2000\n",
      "Epoch 62/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2137 - val_loss: 0.2270\n",
      "Epoch 63/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1998 - val_loss: 0.2013\n",
      "Epoch 64/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2032 - val_loss: 0.2101\n",
      "Epoch 65/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2070 - val_loss: 0.2033\n",
      "Epoch 66/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2096 - val_loss: 0.1964\n",
      "Epoch 67/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2022 - val_loss: 0.2098\n",
      "Epoch 68/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2073 - val_loss: 0.2138\n",
      "Epoch 69/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2045 - val_loss: 0.2025\n",
      "Epoch 70/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2021 - val_loss: 0.2060\n",
      "Epoch 71/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2044 - val_loss: 0.2507\n",
      "Epoch 72/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2040 - val_loss: 0.1942\n",
      "Epoch 73/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2021 - val_loss: 0.1873\n",
      "Epoch 74/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2018 - val_loss: 0.1879\n",
      "Epoch 75/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1917 - val_loss: 0.2120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.2004 - val_loss: 0.2102\n",
      "Epoch 77/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2049 - val_loss: 0.2067\n",
      "Epoch 78/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1981 - val_loss: 0.1890\n",
      "Epoch 79/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1891 - val_loss: 0.1973\n",
      "Epoch 80/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1939 - val_loss: 0.1952\n",
      "Epoch 81/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1985 - val_loss: 0.2035\n",
      "Epoch 82/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2027 - val_loss: 0.2034\n",
      "Epoch 83/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1991 - val_loss: 0.1881\n",
      "Epoch 84/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1975 - val_loss: 0.2181\n",
      "Epoch 85/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1977 - val_loss: 0.1905\n",
      "Epoch 86/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1938 - val_loss: 0.2346\n",
      "Epoch 87/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1934 - val_loss: 0.1966\n",
      "Epoch 88/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1891 - val_loss: 0.1830\n",
      "Epoch 89/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1896 - val_loss: 0.1891\n",
      "Epoch 90/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.2012 - val_loss: 0.2030\n",
      "Epoch 91/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1934 - val_loss: 0.1857\n",
      "Epoch 92/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1940 - val_loss: 0.2087\n",
      "Epoch 93/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1950 - val_loss: 0.1876\n",
      "Epoch 94/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1880 - val_loss: 0.1990\n",
      "Epoch 95/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1952 - val_loss: 0.1860\n",
      "Epoch 96/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1882 - val_loss: 0.1858\n",
      "Epoch 97/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1880 - val_loss: 0.1840\n",
      "Epoch 98/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1904 - val_loss: 0.2026\n",
      "Epoch 99/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1851 - val_loss: 0.1973\n",
      "Epoch 100/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1963 - val_loss: 0.1834\n",
      "Epoch 101/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1961 - val_loss: 0.1884\n",
      "Epoch 102/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1947 - val_loss: 0.1858\n",
      "Epoch 103/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1799 - val_loss: 0.2010\n",
      "Epoch 104/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1964 - val_loss: 0.1931\n",
      "Epoch 105/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1878 - val_loss: 0.1881\n",
      "Epoch 106/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1831 - val_loss: 0.1795\n",
      "Epoch 107/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1851 - val_loss: 0.1862\n",
      "Epoch 108/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1850 - val_loss: 0.1876\n",
      "Epoch 109/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1882 - val_loss: 0.1795\n",
      "Epoch 110/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1827 - val_loss: 0.1930\n",
      "Epoch 111/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1843 - val_loss: 0.1928\n",
      "Epoch 112/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1821 - val_loss: 0.1871\n",
      "Epoch 113/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1855 - val_loss: 0.1927\n",
      "Epoch 114/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1884 - val_loss: 0.2225\n",
      "Epoch 115/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1742 - val_loss: 0.1885\n",
      "Epoch 116/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1852 - val_loss: 0.1956\n",
      "Epoch 117/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1872 - val_loss: 0.1956\n",
      "Epoch 118/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1777 - val_loss: 0.1865\n",
      "Epoch 119/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1834 - val_loss: 0.2050\n",
      "Epoch 120/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1950 - val_loss: 0.1858\n",
      "Epoch 121/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1824 - val_loss: 0.2070\n",
      "Epoch 122/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1836 - val_loss: 0.1790\n",
      "Epoch 123/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1708 - val_loss: 0.1763\n",
      "Epoch 124/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1861 - val_loss: 0.1957\n",
      "Epoch 125/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1822 - val_loss: 0.1844\n",
      "Epoch 126/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1764 - val_loss: 0.1872\n",
      "Epoch 127/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1727 - val_loss: 0.1925\n",
      "Epoch 128/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1874 - val_loss: 0.1735\n",
      "Epoch 129/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1848 - val_loss: 0.1751\n",
      "Epoch 130/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1838 - val_loss: 0.1759\n",
      "Epoch 131/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1790 - val_loss: 0.1777\n",
      "Epoch 132/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1841 - val_loss: 0.2001\n",
      "Epoch 133/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1799 - val_loss: 0.1739\n",
      "Epoch 134/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1894 - val_loss: 0.1923\n",
      "Epoch 135/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1847 - val_loss: 0.2004\n",
      "Epoch 136/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1726 - val_loss: 0.1854\n",
      "Epoch 137/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1760 - val_loss: 0.1741\n",
      "Epoch 138/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1802 - val_loss: 0.1739\n",
      "Epoch 139/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1753 - val_loss: 0.1923\n",
      "Epoch 140/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1740 - val_loss: 0.1713\n",
      "Epoch 141/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1804 - val_loss: 0.1781\n",
      "Epoch 142/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1741 - val_loss: 0.2059\n",
      "Epoch 143/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1780 - val_loss: 0.1892\n",
      "Epoch 144/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1768 - val_loss: 0.2017\n",
      "Epoch 145/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1920 - val_loss: 0.2012\n",
      "Epoch 146/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1737 - val_loss: 0.1785\n",
      "Epoch 147/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1721 - val_loss: 0.1805\n",
      "Epoch 148/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1779 - val_loss: 0.1904\n",
      "Epoch 149/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1807 - val_loss: 0.1785\n",
      "Epoch 150/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1678 - val_loss: 0.1683\n",
      "Epoch 151/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1732 - val_loss: 0.1853\n",
      "Epoch 152/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1773 - val_loss: 0.1931\n",
      "Epoch 153/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1738 - val_loss: 0.1855\n",
      "Epoch 154/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1730 - val_loss: 0.1818\n",
      "Epoch 155/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1764 - val_loss: 0.1743\n",
      "Epoch 156/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1805 - val_loss: 0.1814\n",
      "Epoch 157/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1733 - val_loss: 0.1863\n",
      "Epoch 158/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1756 - val_loss: 0.1992\n",
      "Epoch 159/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1744 - val_loss: 0.1790\n",
      "Epoch 160/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1709 - val_loss: 0.1682\n",
      "Epoch 161/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1718 - val_loss: 0.1734\n",
      "Epoch 162/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1756 - val_loss: 0.1867\n",
      "Epoch 163/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1753 - val_loss: 0.2064\n",
      "Epoch 164/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1741 - val_loss: 0.1980\n",
      "Epoch 165/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1673 - val_loss: 0.1688\n",
      "Epoch 166/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1678 - val_loss: 0.1739\n",
      "Epoch 167/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1675 - val_loss: 0.1747\n",
      "Epoch 168/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1779 - val_loss: 0.1800\n",
      "Epoch 169/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1749 - val_loss: 0.1725\n",
      "Epoch 170/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1736 - val_loss: 0.1651\n",
      "Epoch 171/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1733 - val_loss: 0.1794\n",
      "Epoch 172/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1731 - val_loss: 0.1679\n",
      "Epoch 173/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1736 - val_loss: 0.1668\n",
      "Epoch 174/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1746 - val_loss: 0.1726\n",
      "Epoch 175/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1719 - val_loss: 0.1709\n",
      "Epoch 176/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1699 - val_loss: 0.1890\n",
      "Epoch 177/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1729 - val_loss: 0.1897\n",
      "Epoch 178/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1683 - val_loss: 0.1939\n",
      "Epoch 179/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1654 - val_loss: 0.1699\n",
      "Epoch 180/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1677 - val_loss: 0.1811\n",
      "Epoch 181/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1698 - val_loss: 0.1891\n",
      "Epoch 182/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1728 - val_loss: 0.1825\n",
      "Epoch 183/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1685 - val_loss: 0.1873\n",
      "Epoch 184/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1691 - val_loss: 0.1804\n",
      "Epoch 185/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1650 - val_loss: 0.1723\n",
      "Epoch 186/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1671 - val_loss: 0.1699\n",
      "Epoch 187/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1770 - val_loss: 0.1798\n",
      "Epoch 188/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1643 - val_loss: 0.1834\n",
      "Epoch 189/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1681 - val_loss: 0.1721\n",
      "Epoch 190/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1633 - val_loss: 0.1839\n",
      "Epoch 191/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1695 - val_loss: 0.1684\n",
      "Epoch 192/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1761 - val_loss: 0.1666\n",
      "Epoch 193/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1701 - val_loss: 0.1816\n",
      "Epoch 194/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1703 - val_loss: 0.1701\n",
      "Epoch 195/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1728 - val_loss: 0.1741\n",
      "Epoch 196/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1583 - val_loss: 0.1709\n",
      "Epoch 197/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1714 - val_loss: 0.1700\n",
      "Epoch 198/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1665 - val_loss: 0.1635\n",
      "Epoch 199/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1701 - val_loss: 0.1724\n",
      "Epoch 200/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1678 - val_loss: 0.1733\n",
      "Epoch 201/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1614 - val_loss: 0.1701\n",
      "Epoch 202/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1607 - val_loss: 0.1644\n",
      "Epoch 203/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1759 - val_loss: 0.1976\n",
      "Epoch 204/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1691 - val_loss: 0.1811\n",
      "Epoch 205/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1680 - val_loss: 0.1819\n",
      "Epoch 206/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1659 - val_loss: 0.1647\n",
      "Epoch 207/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1603 - val_loss: 0.1637\n",
      "Epoch 208/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1722 - val_loss: 0.1806\n",
      "Epoch 209/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1629 - val_loss: 0.1627\n",
      "Epoch 210/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1622 - val_loss: 0.2055\n",
      "Epoch 211/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1679 - val_loss: 0.1904\n",
      "Epoch 212/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1689 - val_loss: 0.1672\n",
      "Epoch 213/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1699 - val_loss: 0.1707\n",
      "Epoch 214/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1631 - val_loss: 0.1634\n",
      "Epoch 215/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1672 - val_loss: 0.1706\n",
      "Epoch 216/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1663 - val_loss: 0.1776\n",
      "Epoch 217/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1620 - val_loss: 0.1686\n",
      "Epoch 218/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1684 - val_loss: 0.1625\n",
      "Epoch 219/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1586 - val_loss: 0.1879\n",
      "Epoch 220/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1656 - val_loss: 0.1770\n",
      "Epoch 221/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1586 - val_loss: 0.1707\n",
      "Epoch 222/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1677 - val_loss: 0.1734\n",
      "Epoch 223/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1674 - val_loss: 0.1683\n",
      "Epoch 224/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1654 - val_loss: 0.1672\n",
      "Epoch 225/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1609 - val_loss: 0.1653\n",
      "Epoch 226/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1703 - val_loss: 0.1832\n",
      "Epoch 227/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1712 - val_loss: 0.1673\n",
      "Epoch 228/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1667 - val_loss: 0.1724\n",
      "Epoch 229/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1688 - val_loss: 0.1679\n",
      "Epoch 230/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1636 - val_loss: 0.1718\n",
      "Epoch 231/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1761 - val_loss: 0.1763\n",
      "Epoch 232/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1637 - val_loss: 0.1941\n",
      "Epoch 233/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1747 - val_loss: 0.1668\n",
      "Epoch 234/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1589 - val_loss: 0.1712\n",
      "Epoch 235/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1690 - val_loss: 0.1616\n",
      "Epoch 236/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1614 - val_loss: 0.1667\n",
      "Epoch 237/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1618 - val_loss: 0.1646\n",
      "Epoch 238/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1550 - val_loss: 0.1638\n",
      "Epoch 239/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1646 - val_loss: 0.1590\n",
      "Epoch 240/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1576 - val_loss: 0.1703\n",
      "Epoch 241/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1603 - val_loss: 0.1621\n",
      "Epoch 242/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1650 - val_loss: 0.1727\n",
      "Epoch 243/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1723 - val_loss: 0.1761\n",
      "Epoch 244/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1691 - val_loss: 0.1804\n",
      "Epoch 245/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1608 - val_loss: 0.1629\n",
      "Epoch 246/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1597 - val_loss: 0.1948\n",
      "Epoch 247/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1578 - val_loss: 0.1643\n",
      "Epoch 248/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1525 - val_loss: 0.1788\n",
      "Epoch 249/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1592 - val_loss: 0.1729\n",
      "Epoch 250/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1642 - val_loss: 0.1700\n",
      "Epoch 251/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1638 - val_loss: 0.1711\n",
      "Epoch 252/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1564 - val_loss: 0.1609\n",
      "Epoch 253/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1628 - val_loss: 0.1907\n",
      "Epoch 254/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1624 - val_loss: 0.1557\n",
      "Epoch 255/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1623 - val_loss: 0.1754\n",
      "Epoch 256/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1646 - val_loss: 0.1716\n",
      "Epoch 257/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1647 - val_loss: 0.1678\n",
      "Epoch 258/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1672 - val_loss: 0.1673\n",
      "Epoch 259/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1621 - val_loss: 0.1938\n",
      "Epoch 260/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1582 - val_loss: 0.1697\n",
      "Epoch 261/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1545 - val_loss: 0.1642\n",
      "Epoch 262/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1537 - val_loss: 0.1599\n",
      "Epoch 263/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1645 - val_loss: 0.1562\n",
      "Epoch 264/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1698 - val_loss: 0.1674\n",
      "Epoch 265/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1553 - val_loss: 0.1603\n",
      "Epoch 266/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1586 - val_loss: 0.1774\n",
      "Epoch 267/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1637 - val_loss: 0.1666\n",
      "Epoch 268/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1663 - val_loss: 0.1765\n",
      "Epoch 269/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1558 - val_loss: 0.1631\n",
      "Epoch 270/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1578 - val_loss: 0.1664\n",
      "Epoch 271/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1574 - val_loss: 0.1720\n",
      "Epoch 272/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1626 - val_loss: 0.1608\n",
      "Epoch 273/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1622 - val_loss: 0.1743\n",
      "Epoch 274/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1549 - val_loss: 0.1612\n",
      "Epoch 275/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1599 - val_loss: 0.1638\n",
      "Epoch 276/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1563 - val_loss: 0.1698\n",
      "Epoch 277/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1624 - val_loss: 0.1577\n",
      "Epoch 278/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1594 - val_loss: 0.1583\n",
      "Epoch 279/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1643 - val_loss: 0.1723\n",
      "Epoch 280/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1566 - val_loss: 0.1665\n",
      "Epoch 281/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1667 - val_loss: 0.1616\n",
      "Epoch 282/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1648 - val_loss: 0.1705\n",
      "Epoch 283/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1594 - val_loss: 0.1544\n",
      "Epoch 284/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1550 - val_loss: 0.1723\n",
      "Epoch 285/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1636 - val_loss: 0.1621\n",
      "Epoch 286/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1511 - val_loss: 0.1580\n",
      "Epoch 287/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1623 - val_loss: 0.1666\n",
      "Epoch 288/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1554 - val_loss: 0.1675\n",
      "Epoch 289/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1541 - val_loss: 0.1571\n",
      "Epoch 290/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1609 - val_loss: 0.1734\n",
      "Epoch 291/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1634 - val_loss: 0.1640\n",
      "Epoch 292/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1563 - val_loss: 0.1581\n",
      "Epoch 293/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1617 - val_loss: 0.1811\n",
      "Epoch 294/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1540 - val_loss: 0.1581\n",
      "Epoch 295/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1501 - val_loss: 0.1598\n",
      "Epoch 296/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1578 - val_loss: 0.1626\n",
      "Epoch 297/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1560 - val_loss: 0.1813\n",
      "Epoch 298/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1557 - val_loss: 0.1625\n",
      "Epoch 299/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1558 - val_loss: 0.1735\n",
      "Epoch 300/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1747 - val_loss: 0.1770\n",
      "Epoch 301/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1550 - val_loss: 0.1678\n",
      "Epoch 302/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1598 - val_loss: 0.1677\n",
      "Epoch 303/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1580 - val_loss: 0.1608\n",
      "Epoch 304/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1560 - val_loss: 0.1638\n",
      "Epoch 305/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1655 - val_loss: 0.2016\n",
      "Epoch 306/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1727 - val_loss: 0.1548\n",
      "Epoch 307/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1603 - val_loss: 0.1589\n",
      "Epoch 308/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1690 - val_loss: 0.1742\n",
      "Epoch 309/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1590 - val_loss: 0.1632\n",
      "Epoch 310/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1647 - val_loss: 0.1836\n",
      "Epoch 311/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1542 - val_loss: 0.1706\n",
      "Epoch 312/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1511 - val_loss: 0.1600\n",
      "Epoch 313/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1564 - val_loss: 0.1719\n",
      "\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 314/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1425 - val_loss: 0.1538\n",
      "Epoch 315/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1441 - val_loss: 0.1525\n",
      "Epoch 316/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1405 - val_loss: 0.1496\n",
      "Epoch 317/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1429 - val_loss: 0.1462\n",
      "Epoch 318/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1436 - val_loss: 0.1536\n",
      "Epoch 319/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1480 - val_loss: 0.1532\n",
      "Epoch 320/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1409 - val_loss: 0.1489\n",
      "Epoch 321/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1453 - val_loss: 0.1446\n",
      "Epoch 322/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1483 - val_loss: 0.1450\n",
      "Epoch 323/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1428 - val_loss: 0.1529\n",
      "Epoch 324/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1464 - val_loss: 0.1563\n",
      "Epoch 325/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1442 - val_loss: 0.1443\n",
      "Epoch 326/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1469 - val_loss: 0.1462\n",
      "Epoch 327/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1359 - val_loss: 0.1477\n",
      "Epoch 328/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1359 - val_loss: 0.1472\n",
      "Epoch 329/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1526 - val_loss: 0.1450\n",
      "Epoch 330/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1411 - val_loss: 0.1435\n",
      "Epoch 331/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1450 - val_loss: 0.1490\n",
      "Epoch 332/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1446 - val_loss: 0.1466\n",
      "Epoch 333/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1398 - val_loss: 0.1479\n",
      "Epoch 334/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1416 - val_loss: 0.1442\n",
      "Epoch 335/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1410 - val_loss: 0.1458\n",
      "Epoch 336/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1410 - val_loss: 0.1566\n",
      "Epoch 337/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1482 - val_loss: 0.1542\n",
      "Epoch 338/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1410 - val_loss: 0.1447\n",
      "Epoch 339/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1523 - val_loss: 0.1523\n",
      "Epoch 340/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1367 - val_loss: 0.1595\n",
      "Epoch 341/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1453 - val_loss: 0.1495\n",
      "Epoch 342/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1458 - val_loss: 0.1486\n",
      "Epoch 343/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1433 - val_loss: 0.1430\n",
      "Epoch 344/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1377 - val_loss: 0.1507\n",
      "Epoch 345/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1429 - val_loss: 0.1436\n",
      "Epoch 346/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1384 - val_loss: 0.1525\n",
      "Epoch 347/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1431 - val_loss: 0.1573\n",
      "Epoch 348/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1450 - val_loss: 0.1456\n",
      "Epoch 349/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1361 - val_loss: 0.1492\n",
      "Epoch 350/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1356 - val_loss: 0.1482\n",
      "Epoch 351/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1418 - val_loss: 0.1642\n",
      "Epoch 352/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1426 - val_loss: 0.1428\n",
      "Epoch 353/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1366 - val_loss: 0.1530\n",
      "Epoch 354/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1342 - val_loss: 0.1437\n",
      "Epoch 355/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1384 - val_loss: 0.1436\n",
      "Epoch 356/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1334 - val_loss: 0.1500\n",
      "Epoch 357/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1370 - val_loss: 0.1635\n",
      "Epoch 358/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1463 - val_loss: 0.1478\n",
      "Epoch 359/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1338 - val_loss: 0.1521\n",
      "Epoch 360/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1397 - val_loss: 0.1771\n",
      "Epoch 361/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1396 - val_loss: 0.1692\n",
      "Epoch 362/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1425 - val_loss: 0.1461\n",
      "Epoch 363/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1461 - val_loss: 0.1589\n",
      "Epoch 364/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1495 - val_loss: 0.1447\n",
      "Epoch 365/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1375 - val_loss: 0.1492\n",
      "Epoch 366/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1383 - val_loss: 0.1552\n",
      "Epoch 367/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1388 - val_loss: 0.1443\n",
      "Epoch 368/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1357 - val_loss: 0.1438\n",
      "Epoch 369/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1424 - val_loss: 0.1419\n",
      "Epoch 370/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1374 - val_loss: 0.1650\n",
      "Epoch 371/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1376 - val_loss: 0.1444\n",
      "Epoch 372/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1366 - val_loss: 0.1457\n",
      "Epoch 373/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1462 - val_loss: 0.1533\n",
      "Epoch 374/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1390 - val_loss: 0.1443\n",
      "Epoch 375/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1472 - val_loss: 0.1569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1380 - val_loss: 0.1568\n",
      "Epoch 377/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1405 - val_loss: 0.1444\n",
      "Epoch 378/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1380 - val_loss: 0.1412\n",
      "Epoch 379/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1345 - val_loss: 0.1444\n",
      "Epoch 380/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1423 - val_loss: 0.1421\n",
      "Epoch 381/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1453 - val_loss: 0.1516\n",
      "Epoch 382/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1575 - val_loss: 0.1624\n",
      "Epoch 383/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1355 - val_loss: 0.1537\n",
      "Epoch 384/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1404 - val_loss: 0.1442\n",
      "Epoch 385/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1363 - val_loss: 0.1499\n",
      "Epoch 386/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1460 - val_loss: 0.1446\n",
      "Epoch 387/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1410 - val_loss: 0.1476\n",
      "Epoch 388/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1402 - val_loss: 0.1430\n",
      "Epoch 389/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1372 - val_loss: 0.1555\n",
      "Epoch 390/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1454 - val_loss: 0.1460\n",
      "Epoch 391/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1314 - val_loss: 0.1467\n",
      "Epoch 392/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1358 - val_loss: 0.1453\n",
      "Epoch 393/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1382 - val_loss: 0.1428\n",
      "Epoch 394/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1441 - val_loss: 0.1422\n",
      "Epoch 395/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1335 - val_loss: 0.1466\n",
      "Epoch 396/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1472 - val_loss: 0.1458\n",
      "Epoch 397/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1348 - val_loss: 0.1437\n",
      "Epoch 398/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1368 - val_loss: 0.1592\n",
      "Epoch 399/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1475 - val_loss: 0.1447\n",
      "Epoch 400/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1309 - val_loss: 0.1437\n",
      "Epoch 401/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1353 - val_loss: 0.1520\n",
      "Epoch 402/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1431 - val_loss: 0.1492\n",
      "Epoch 403/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1424 - val_loss: 0.1409\n",
      "Epoch 404/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1418 - val_loss: 0.1441\n",
      "Epoch 405/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1470 - val_loss: 0.1437\n",
      "Epoch 406/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1412 - val_loss: 0.1453\n",
      "Epoch 407/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1447 - val_loss: 0.1590\n",
      "Epoch 408/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1488 - val_loss: 0.1589\n",
      "Epoch 409/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1329 - val_loss: 0.1593\n",
      "Epoch 410/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1526 - val_loss: 0.1443\n",
      "Epoch 411/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1404 - val_loss: 0.1447\n",
      "Epoch 412/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1421 - val_loss: 0.1439\n",
      "Epoch 413/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1406 - val_loss: 0.1481\n",
      "Epoch 414/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1337 - val_loss: 0.1538\n",
      "Epoch 415/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1434 - val_loss: 0.1577\n",
      "Epoch 416/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1356 - val_loss: 0.1483\n",
      "Epoch 417/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1480 - val_loss: 0.1439\n",
      "Epoch 418/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1376 - val_loss: 0.1490\n",
      "Epoch 419/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1376 - val_loss: 0.1442\n",
      "Epoch 420/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1437 - val_loss: 0.1526\n",
      "Epoch 421/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1362 - val_loss: 0.1434\n",
      "Epoch 422/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1356 - val_loss: 0.1448\n",
      "Epoch 423/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1400 - val_loss: 0.1415\n",
      "Epoch 424/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1407 - val_loss: 0.1520\n",
      "Epoch 425/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1354 - val_loss: 0.1423\n",
      "Epoch 426/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1440 - val_loss: 0.1415\n",
      "Epoch 427/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1431 - val_loss: 0.1527\n",
      "Epoch 428/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1378 - val_loss: 0.1484\n",
      "Epoch 429/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1368 - val_loss: 0.1493\n",
      "Epoch 430/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1365 - val_loss: 0.1471\n",
      "Epoch 431/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1410 - val_loss: 0.1547\n",
      "Epoch 432/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1494 - val_loss: 0.1531\n",
      "Epoch 433/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1365 - val_loss: 0.1406\n",
      "Epoch 434/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1347 - val_loss: 0.1434\n",
      "Epoch 435/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1340 - val_loss: 0.1518\n",
      "Epoch 436/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1446 - val_loss: 0.1431\n",
      "Epoch 437/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1339 - val_loss: 0.1443\n",
      "Epoch 438/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1358 - val_loss: 0.1486\n",
      "Epoch 439/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1417 - val_loss: 0.1507\n",
      "Epoch 440/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1424 - val_loss: 0.1503\n",
      "Epoch 441/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1377 - val_loss: 0.1408\n",
      "Epoch 442/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1391 - val_loss: 0.1431\n",
      "Epoch 443/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1389 - val_loss: 0.1507\n",
      "Epoch 444/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1433 - val_loss: 0.1412\n",
      "Epoch 445/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1460 - val_loss: 0.1476\n",
      "Epoch 446/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1394 - val_loss: 0.1420\n",
      "Epoch 447/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1434 - val_loss: 0.1534\n",
      "Epoch 448/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1407 - val_loss: 0.1524\n",
      "Epoch 449/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1454 - val_loss: 0.1486\n",
      "Epoch 450/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1325 - val_loss: 0.1463\n",
      "Epoch 451/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1371 - val_loss: 0.1434\n",
      "Epoch 452/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1362 - val_loss: 0.1439\n",
      "Epoch 453/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1356 - val_loss: 0.1441\n",
      "Epoch 454/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1314 - val_loss: 0.1505\n",
      "Epoch 455/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1457 - val_loss: 0.1404\n",
      "Epoch 456/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1306 - val_loss: 0.1411\n",
      "Epoch 457/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1422 - val_loss: 0.1435\n",
      "Epoch 458/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1344 - val_loss: 0.1458\n",
      "Epoch 459/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1347 - val_loss: 0.1417\n",
      "Epoch 460/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1382 - val_loss: 0.1525\n",
      "Epoch 461/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1411 - val_loss: 0.1464\n",
      "Epoch 462/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1308 - val_loss: 0.1458\n",
      "Epoch 463/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1332 - val_loss: 0.1448\n",
      "Epoch 464/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1532 - val_loss: 0.1448\n",
      "Epoch 465/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1537 - val_loss: 0.1398\n",
      "Epoch 466/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1376 - val_loss: 0.1411\n",
      "Epoch 467/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1358 - val_loss: 0.1450\n",
      "Epoch 468/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1441 - val_loss: 0.1432\n",
      "Epoch 469/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1475 - val_loss: 0.1539\n",
      "Epoch 470/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1450 - val_loss: 0.1380\n",
      "Epoch 471/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1439 - val_loss: 0.1428\n",
      "Epoch 472/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1299 - val_loss: 0.1431\n",
      "Epoch 473/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1344 - val_loss: 0.1397\n",
      "Epoch 474/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1347 - val_loss: 0.1511\n",
      "Epoch 475/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1359 - val_loss: 0.1432\n",
      "Epoch 476/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1346 - val_loss: 0.1436\n",
      "Epoch 477/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1454 - val_loss: 0.1448\n",
      "Epoch 478/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1307 - val_loss: 0.1464\n",
      "Epoch 479/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1376 - val_loss: 0.1478\n",
      "Epoch 480/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1420 - val_loss: 0.1463\n",
      "Epoch 481/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1440 - val_loss: 0.1455\n",
      "Epoch 482/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1417 - val_loss: 0.1462\n",
      "Epoch 483/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1306 - val_loss: 0.1467\n",
      "Epoch 484/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1340 - val_loss: 0.1502\n",
      "Epoch 485/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1363 - val_loss: 0.1487\n",
      "Epoch 486/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1342 - val_loss: 0.1379\n",
      "Epoch 487/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1320 - val_loss: 0.1426\n",
      "Epoch 488/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1380 - val_loss: 0.1496\n",
      "Epoch 489/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1378 - val_loss: 0.1484\n",
      "Epoch 490/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1344 - val_loss: 0.1414\n",
      "Epoch 491/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1431 - val_loss: 0.1484\n",
      "Epoch 492/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1284 - val_loss: 0.1451\n",
      "Epoch 493/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1351 - val_loss: 0.1499\n",
      "Epoch 494/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1304 - val_loss: 0.1448\n",
      "Epoch 495/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1428 - val_loss: 0.1414\n",
      "Epoch 496/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1399 - val_loss: 0.1380\n",
      "Epoch 497/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1346 - val_loss: 0.1500\n",
      "Epoch 498/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1430 - val_loss: 0.1418\n",
      "Epoch 499/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1415 - val_loss: 0.1409\n",
      "Epoch 500/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1417 - val_loss: 0.1537\n",
      "Epoch 501/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1429 - val_loss: 0.1515\n",
      "Epoch 502/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1286 - val_loss: 0.1445\n",
      "Epoch 503/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1405 - val_loss: 0.1566\n",
      "Epoch 504/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1414 - val_loss: 0.1402\n",
      "Epoch 505/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1417 - val_loss: 0.1398\n",
      "Epoch 506/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1354 - val_loss: 0.1383\n",
      "Epoch 507/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1403 - val_loss: 0.1415\n",
      "Epoch 508/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1351 - val_loss: 0.1460\n",
      "Epoch 509/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1244 - val_loss: 0.1463\n",
      "Epoch 510/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1447 - val_loss: 0.1417\n",
      "Epoch 511/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1498 - val_loss: 0.1406\n",
      "Epoch 512/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1392 - val_loss: 0.1447\n",
      "Epoch 513/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1451 - val_loss: 0.1404\n",
      "Epoch 514/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1330 - val_loss: 0.1490\n",
      "Epoch 515/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1440 - val_loss: 0.1485\n",
      "Epoch 516/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1386 - val_loss: 0.1377\n",
      "Epoch 517/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1387 - val_loss: 0.1406\n",
      "Epoch 518/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1451 - val_loss: 0.1446\n",
      "Epoch 519/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1258 - val_loss: 0.1409\n",
      "Epoch 520/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1366 - val_loss: 0.1415\n",
      "Epoch 521/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1389 - val_loss: 0.1468\n",
      "Epoch 522/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1273 - val_loss: 0.1394\n",
      "Epoch 523/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1476 - val_loss: 0.1537\n",
      "Epoch 524/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1397 - val_loss: 0.1392\n",
      "Epoch 525/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1351 - val_loss: 0.1423\n",
      "Epoch 526/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1363 - val_loss: 0.1468\n",
      "Epoch 527/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1350 - val_loss: 0.1422\n",
      "Epoch 528/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1397 - val_loss: 0.1451\n",
      "Epoch 529/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1391 - val_loss: 0.1426\n",
      "Epoch 530/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1389 - val_loss: 0.1404\n",
      "Epoch 531/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1350 - val_loss: 0.1433\n",
      "Epoch 532/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1351 - val_loss: 0.1466\n",
      "Epoch 533/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1365 - val_loss: 0.1556\n",
      "Epoch 534/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1352 - val_loss: 0.1416\n",
      "Epoch 535/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1351 - val_loss: 0.1392\n",
      "Epoch 536/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1379 - val_loss: 0.1573\n",
      "Epoch 537/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1288 - val_loss: 0.1449\n",
      "Epoch 538/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1328 - val_loss: 0.1402\n",
      "Epoch 539/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1365 - val_loss: 0.1520\n",
      "Epoch 540/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1457 - val_loss: 0.1404\n",
      "Epoch 541/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1406 - val_loss: 0.1466\n",
      "Epoch 542/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1373 - val_loss: 0.1396\n",
      "Epoch 543/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1468 - val_loss: 0.1416\n",
      "Epoch 544/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1357 - val_loss: 0.1419\n",
      "Epoch 545/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1351 - val_loss: 0.1534\n",
      "Epoch 546/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1388 - val_loss: 0.1498\n",
      "\n",
      "Epoch 00546: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 547/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1276 - val_loss: 0.1355\n",
      "Epoch 548/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1267 - val_loss: 0.1385\n",
      "Epoch 549/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1288 - val_loss: 0.1355\n",
      "Epoch 550/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1318 - val_loss: 0.1340\n",
      "Epoch 551/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1400 - val_loss: 0.1334\n",
      "Epoch 552/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1399 - val_loss: 0.1370\n",
      "Epoch 553/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1327 - val_loss: 0.1344\n",
      "Epoch 554/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1337 - val_loss: 0.1343\n",
      "Epoch 555/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1333 - val_loss: 0.1336\n",
      "Epoch 556/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1184 - val_loss: 0.1403\n",
      "Epoch 557/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1291 - val_loss: 0.1341\n",
      "Epoch 558/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1216 - val_loss: 0.1355\n",
      "Epoch 559/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1328 - val_loss: 0.1377\n",
      "Epoch 560/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1280 - val_loss: 0.1371\n",
      "Epoch 561/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1388 - val_loss: 0.1355\n",
      "Epoch 562/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1332 - val_loss: 0.1327\n",
      "Epoch 563/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1226 - val_loss: 0.1347\n",
      "Epoch 564/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1275 - val_loss: 0.1335\n",
      "Epoch 565/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1284 - val_loss: 0.1379\n",
      "Epoch 566/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1177 - val_loss: 0.1338\n",
      "Epoch 567/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1254 - val_loss: 0.1352\n",
      "Epoch 568/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1264 - val_loss: 0.1373\n",
      "Epoch 569/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1341 - val_loss: 0.1344\n",
      "Epoch 570/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1219 - val_loss: 0.1339\n",
      "Epoch 571/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1375 - val_loss: 0.1328\n",
      "Epoch 572/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1273 - val_loss: 0.1400\n",
      "Epoch 573/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1247 - val_loss: 0.1418\n",
      "Epoch 574/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1317 - val_loss: 0.1355\n",
      "Epoch 575/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1340 - val_loss: 0.1374\n",
      "Epoch 576/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1213 - val_loss: 0.1381\n",
      "Epoch 577/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1260 - val_loss: 0.1333\n",
      "Epoch 578/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1358 - val_loss: 0.1343\n",
      "Epoch 579/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1310 - val_loss: 0.1362\n",
      "Epoch 580/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1273 - val_loss: 0.1349\n",
      "Epoch 581/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1308 - val_loss: 0.1359\n",
      "Epoch 582/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1324 - val_loss: 0.1347\n",
      "Epoch 583/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1234 - val_loss: 0.1361\n",
      "Epoch 584/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1277 - val_loss: 0.1468\n",
      "Epoch 585/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1331 - val_loss: 0.1338\n",
      "Epoch 586/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1301 - val_loss: 0.1384\n",
      "Epoch 587/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1306 - val_loss: 0.1384\n",
      "Epoch 588/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1300 - val_loss: 0.1422\n",
      "Epoch 589/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1286 - val_loss: 0.1343\n",
      "Epoch 590/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1184 - val_loss: 0.1334\n",
      "Epoch 591/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1230 - val_loss: 0.1339\n",
      "Epoch 592/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1310 - val_loss: 0.1348\n",
      "\n",
      "Epoch 00592: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 593/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1234 - val_loss: 0.1312\n",
      "Epoch 594/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1281 - val_loss: 0.1324\n",
      "Epoch 595/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1234 - val_loss: 0.1333\n",
      "Epoch 596/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1228 - val_loss: 0.1308\n",
      "Epoch 597/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1197 - val_loss: 0.1317\n",
      "Epoch 598/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1308 - val_loss: 0.1309\n",
      "Epoch 599/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1337 - val_loss: 0.1315\n",
      "Epoch 600/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1249 - val_loss: 0.1309\n",
      "Epoch 601/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1342 - val_loss: 0.1313\n",
      "Epoch 602/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1174 - val_loss: 0.1325\n",
      "Epoch 603/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1242 - val_loss: 0.1323\n",
      "Epoch 604/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1306 - val_loss: 0.1301\n",
      "Epoch 605/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1250 - val_loss: 0.1307\n",
      "Epoch 606/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1230 - val_loss: 0.1304\n",
      "Epoch 607/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1275 - val_loss: 0.1353\n",
      "Epoch 608/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1242 - val_loss: 0.1305\n",
      "Epoch 609/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1295 - val_loss: 0.1306\n",
      "Epoch 610/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1210 - val_loss: 0.1303\n",
      "Epoch 611/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1245 - val_loss: 0.1314\n",
      "Epoch 612/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1186 - val_loss: 0.1303\n",
      "Epoch 613/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1307 - val_loss: 0.1314\n",
      "Epoch 614/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1323 - val_loss: 0.1317\n",
      "Epoch 615/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1366 - val_loss: 0.1318\n",
      "Epoch 616/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1200 - val_loss: 0.1305\n",
      "Epoch 617/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1181 - val_loss: 0.1328\n",
      "Epoch 618/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1265 - val_loss: 0.1314\n",
      "Epoch 619/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1220 - val_loss: 0.1319\n",
      "Epoch 620/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1267 - val_loss: 0.1314\n",
      "Epoch 621/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1170 - val_loss: 0.1325\n",
      "Epoch 622/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1223 - val_loss: 0.1334\n",
      "Epoch 623/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1268 - val_loss: 0.1309\n",
      "Epoch 624/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1236 - val_loss: 0.1307\n",
      "Epoch 625/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1282 - val_loss: 0.1303\n",
      "Epoch 626/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1232 - val_loss: 0.1309\n",
      "Epoch 627/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1248 - val_loss: 0.1310\n",
      "Epoch 628/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1221 - val_loss: 0.1315\n",
      "Epoch 629/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1274 - val_loss: 0.1300\n",
      "Epoch 630/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1280 - val_loss: 0.1309\n",
      "Epoch 631/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1126 - val_loss: 0.1305\n",
      "Epoch 632/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1415 - val_loss: 0.1302\n",
      "Epoch 633/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1209 - val_loss: 0.1304\n",
      "Epoch 634/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1267 - val_loss: 0.1326\n",
      "\n",
      "Epoch 00634: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 635/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1165 - val_loss: 0.1297\n",
      "Epoch 636/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1218 - val_loss: 0.1292\n",
      "Epoch 637/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1236 - val_loss: 0.1292\n",
      "Epoch 638/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1281 - val_loss: 0.1302\n",
      "Epoch 639/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1182 - val_loss: 0.1294\n",
      "Epoch 640/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1264 - val_loss: 0.1301\n",
      "Epoch 641/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1213 - val_loss: 0.1306\n",
      "Epoch 642/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1289 - val_loss: 0.1298\n",
      "Epoch 643/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1222 - val_loss: 0.1292\n",
      "Epoch 644/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1254 - val_loss: 0.1293\n",
      "Epoch 645/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1229 - val_loss: 0.1291\n",
      "Epoch 646/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1216 - val_loss: 0.1292\n",
      "Epoch 647/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1257 - val_loss: 0.1298\n",
      "Epoch 648/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1335 - val_loss: 0.1296\n",
      "Epoch 649/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1166 - val_loss: 0.1296\n",
      "Epoch 650/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1245 - val_loss: 0.1291\n",
      "Epoch 651/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1282 - val_loss: 0.1295\n",
      "Epoch 652/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1355 - val_loss: 0.1290\n",
      "Epoch 653/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1243 - val_loss: 0.1305\n",
      "Epoch 654/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1212 - val_loss: 0.1294\n",
      "Epoch 655/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1292 - val_loss: 0.1294\n",
      "Epoch 656/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1235 - val_loss: 0.1301\n",
      "Epoch 657/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1245 - val_loss: 0.1299\n",
      "Epoch 658/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1192 - val_loss: 0.1297\n",
      "Epoch 659/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1227 - val_loss: 0.1291\n",
      "Epoch 660/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1182 - val_loss: 0.1293\n",
      "Epoch 661/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1253 - val_loss: 0.1295\n",
      "Epoch 662/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1244 - val_loss: 0.1290\n",
      "Epoch 663/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1179 - val_loss: 0.1297\n",
      "Epoch 664/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1191 - val_loss: 0.1293\n",
      "Epoch 665/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1348 - val_loss: 0.1294\n",
      "Epoch 666/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1221 - val_loss: 0.1304\n",
      "Epoch 667/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1355 - val_loss: 0.1293\n",
      "Epoch 668/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1269 - val_loss: 0.1292\n",
      "Epoch 669/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1241 - val_loss: 0.1292\n",
      "Epoch 670/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1169 - val_loss: 0.1289\n",
      "Epoch 671/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1277 - val_loss: 0.1296\n",
      "Epoch 672/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1224 - val_loss: 0.1290\n",
      "Epoch 673/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1241 - val_loss: 0.1296\n",
      "Epoch 674/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1250 - val_loss: 0.1290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 675/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1290 - val_loss: 0.1303\n",
      "Epoch 676/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1212 - val_loss: 0.1293\n",
      "Epoch 677/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1111 - val_loss: 0.1294\n",
      "Epoch 678/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1395 - val_loss: 0.1289\n",
      "Epoch 679/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1188 - val_loss: 0.1294\n",
      "Epoch 680/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1216 - val_loss: 0.1291\n",
      "Epoch 681/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1242 - val_loss: 0.1288\n",
      "Epoch 682/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1199 - val_loss: 0.1286\n",
      "Epoch 683/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1127 - val_loss: 0.1294\n",
      "Epoch 684/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1270 - val_loss: 0.1293\n",
      "Epoch 685/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1278 - val_loss: 0.1294\n",
      "Epoch 686/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1221 - val_loss: 0.1295\n",
      "Epoch 687/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1247 - val_loss: 0.1290\n",
      "Epoch 688/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1151 - val_loss: 0.1294\n",
      "Epoch 689/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1259 - val_loss: 0.1291\n",
      "Epoch 690/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1297 - val_loss: 0.1301\n",
      "Epoch 691/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1121 - val_loss: 0.1302\n",
      "Epoch 692/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1275 - val_loss: 0.1296\n",
      "Epoch 693/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1228 - val_loss: 0.1288\n",
      "Epoch 694/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1190 - val_loss: 0.1288\n",
      "Epoch 695/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1210 - val_loss: 0.1288\n",
      "Epoch 696/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1174 - val_loss: 0.1287\n",
      "Epoch 697/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1220 - val_loss: 0.1290\n",
      "Epoch 698/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1194 - val_loss: 0.1300\n",
      "Epoch 699/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1223 - val_loss: 0.1296\n",
      "Epoch 700/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1219 - val_loss: 0.1294\n",
      "Epoch 701/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1276 - val_loss: 0.1293\n",
      "Epoch 702/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1264 - val_loss: 0.1295\n",
      "Epoch 703/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1301 - val_loss: 0.1296\n",
      "Epoch 704/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1174 - val_loss: 0.1291\n",
      "Epoch 705/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1292 - val_loss: 0.1313\n",
      "Epoch 706/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1192 - val_loss: 0.1287\n",
      "Epoch 707/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1226 - val_loss: 0.1290\n",
      "Epoch 708/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1224 - val_loss: 0.1292\n",
      "Epoch 709/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1265 - val_loss: 0.1290\n",
      "Epoch 710/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1225 - val_loss: 0.1294\n",
      "Epoch 711/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1259 - val_loss: 0.1300\n",
      "Epoch 712/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1258 - val_loss: 0.1294\n",
      "\n",
      "Epoch 00712: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 713/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1159 - val_loss: 0.1282\n",
      "Epoch 714/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1255 - val_loss: 0.1284\n",
      "Epoch 715/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1206 - val_loss: 0.1288\n",
      "Epoch 716/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1273 - val_loss: 0.1286\n",
      "Epoch 717/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1233 - val_loss: 0.1285\n",
      "Epoch 718/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1192 - val_loss: 0.1283\n",
      "Epoch 719/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1211 - val_loss: 0.1283\n",
      "Epoch 720/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1180 - val_loss: 0.1286\n",
      "Epoch 721/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1183 - val_loss: 0.1284\n",
      "Epoch 722/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1171 - val_loss: 0.1286\n",
      "Epoch 723/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1206 - val_loss: 0.1285\n",
      "Epoch 724/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1185 - val_loss: 0.1280\n",
      "Epoch 725/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1163 - val_loss: 0.1281\n",
      "Epoch 726/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1190 - val_loss: 0.1283\n",
      "Epoch 727/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1302 - val_loss: 0.1282\n",
      "Epoch 728/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1261 - val_loss: 0.1282\n",
      "Epoch 729/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1279 - val_loss: 0.1280\n",
      "Epoch 730/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1179 - val_loss: 0.1285\n",
      "Epoch 731/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1134 - val_loss: 0.1283\n",
      "Epoch 732/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1202 - val_loss: 0.1283\n",
      "Epoch 733/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1231 - val_loss: 0.1283\n",
      "Epoch 734/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1223 - val_loss: 0.1282\n",
      "Epoch 735/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1207 - val_loss: 0.1281\n",
      "Epoch 736/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1284 - val_loss: 0.1289\n",
      "Epoch 737/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1136 - val_loss: 0.1285\n",
      "Epoch 738/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1255 - val_loss: 0.1291\n",
      "Epoch 739/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1191 - val_loss: 0.1287\n",
      "Epoch 740/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1202 - val_loss: 0.1287\n",
      "Epoch 741/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1176 - val_loss: 0.1282\n",
      "Epoch 742/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1175 - val_loss: 0.1284\n",
      "Epoch 743/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1253 - val_loss: 0.1282\n",
      "Epoch 744/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1226 - val_loss: 0.1287\n",
      "Epoch 745/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1307 - val_loss: 0.1284\n",
      "Epoch 746/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1195 - val_loss: 0.1290\n",
      "Epoch 747/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1182 - val_loss: 0.1284\n",
      "Epoch 748/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1141 - val_loss: 0.1282\n",
      "Epoch 749/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1175 - val_loss: 0.1290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 750/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1313 - val_loss: 0.1287\n",
      "Epoch 751/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1238 - val_loss: 0.1283\n",
      "Epoch 752/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1232 - val_loss: 0.1284\n",
      "Epoch 753/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1295 - val_loss: 0.1285\n",
      "Epoch 754/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1229 - val_loss: 0.1285\n",
      "\n",
      "Epoch 00754: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 755/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1191 - val_loss: 0.1279\n",
      "Epoch 756/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1249 - val_loss: 0.1281\n",
      "Epoch 757/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1220 - val_loss: 0.1279\n",
      "Epoch 758/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1198 - val_loss: 0.1279\n",
      "Epoch 759/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1233 - val_loss: 0.1280\n",
      "Epoch 760/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1213 - val_loss: 0.1281\n",
      "Epoch 761/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1212 - val_loss: 0.1279\n",
      "Epoch 762/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1151 - val_loss: 0.1284\n",
      "Epoch 763/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1203 - val_loss: 0.1282\n",
      "Epoch 764/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1336 - val_loss: 0.1281\n",
      "Epoch 765/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1204 - val_loss: 0.1281\n",
      "Epoch 766/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1292 - val_loss: 0.1280\n",
      "Epoch 767/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1236 - val_loss: 0.1282\n",
      "Epoch 768/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1175 - val_loss: 0.1280\n",
      "Epoch 769/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1138 - val_loss: 0.1279\n",
      "Epoch 770/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1250 - val_loss: 0.1279\n",
      "Epoch 771/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1223 - val_loss: 0.1280\n",
      "Epoch 772/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1299 - val_loss: 0.1280\n",
      "Epoch 773/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1204 - val_loss: 0.1278\n",
      "Epoch 774/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1258 - val_loss: 0.1277\n",
      "Epoch 775/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1244 - val_loss: 0.1278\n",
      "Epoch 776/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1185 - val_loss: 0.1281\n",
      "Epoch 777/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1222 - val_loss: 0.1280\n",
      "Epoch 778/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1169 - val_loss: 0.1278\n",
      "Epoch 779/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1243 - val_loss: 0.1279\n",
      "Epoch 780/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1331 - val_loss: 0.1278\n",
      "Epoch 781/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1230 - val_loss: 0.1278\n",
      "Epoch 782/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1185 - val_loss: 0.1278\n",
      "Epoch 783/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1196 - val_loss: 0.1283\n",
      "Epoch 784/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1293 - val_loss: 0.1285\n",
      "Epoch 785/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1190 - val_loss: 0.1283\n",
      "Epoch 786/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1212 - val_loss: 0.1279\n",
      "Epoch 787/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1273 - val_loss: 0.1278\n",
      "Epoch 788/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1129 - val_loss: 0.1278\n",
      "Epoch 789/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1189 - val_loss: 0.1278\n",
      "Epoch 790/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1182 - val_loss: 0.1277\n",
      "Epoch 791/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1229 - val_loss: 0.1277\n",
      "Epoch 792/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1170 - val_loss: 0.1278\n",
      "Epoch 793/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1253 - val_loss: 0.1278\n",
      "Epoch 794/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1347 - val_loss: 0.1278\n",
      "Epoch 795/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1241 - val_loss: 0.1279\n",
      "Epoch 796/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1322 - val_loss: 0.1278\n",
      "Epoch 797/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1177 - val_loss: 0.1278\n",
      "Epoch 798/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1290 - val_loss: 0.1279\n",
      "Epoch 799/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1258 - val_loss: 0.1280\n",
      "Epoch 800/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1160 - val_loss: 0.1279\n",
      "Epoch 801/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1142 - val_loss: 0.1280\n",
      "Epoch 802/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1164 - val_loss: 0.1278\n",
      "Epoch 803/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1194 - val_loss: 0.1279\n",
      "\n",
      "Epoch 00803: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 804/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1234 - val_loss: 0.1277\n",
      "Epoch 805/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1242 - val_loss: 0.1279\n",
      "Epoch 806/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1239 - val_loss: 0.1277\n",
      "Epoch 807/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1300 - val_loss: 0.1276\n",
      "Epoch 808/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1246 - val_loss: 0.1277\n",
      "Epoch 809/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1175 - val_loss: 0.1277\n",
      "Epoch 810/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1297 - val_loss: 0.1278\n",
      "Epoch 811/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1199 - val_loss: 0.1276\n",
      "Epoch 812/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1169 - val_loss: 0.1278\n",
      "Epoch 813/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1173 - val_loss: 0.1279\n",
      "Epoch 814/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1114 - val_loss: 0.1278\n",
      "Epoch 815/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1292 - val_loss: 0.1276\n",
      "Epoch 816/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1289 - val_loss: 0.1276\n",
      "Epoch 817/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1219 - val_loss: 0.1276\n",
      "Epoch 818/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1173 - val_loss: 0.1276\n",
      "Epoch 819/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1250 - val_loss: 0.1280\n",
      "Epoch 820/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1283 - val_loss: 0.1278\n",
      "Epoch 821/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1279 - val_loss: 0.1278\n",
      "Epoch 822/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1355 - val_loss: 0.1279\n",
      "Epoch 823/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1245 - val_loss: 0.1277\n",
      "Epoch 824/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1151 - val_loss: 0.1277\n",
      "Epoch 825/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1276 - val_loss: 0.1277\n",
      "Epoch 826/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1220 - val_loss: 0.1277\n",
      "Epoch 827/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1190 - val_loss: 0.1279\n",
      "Epoch 828/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1342 - val_loss: 0.1279\n",
      "Epoch 829/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1208 - val_loss: 0.1277\n",
      "Epoch 830/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1323 - val_loss: 0.1278\n",
      "Epoch 831/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1215 - val_loss: 0.1277\n",
      "Epoch 832/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1375 - val_loss: 0.1277\n",
      "Epoch 833/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1233 - val_loss: 0.1277\n",
      "Epoch 834/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1217 - val_loss: 0.1278\n",
      "Epoch 835/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1196 - val_loss: 0.1277\n",
      "Epoch 836/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1278 - val_loss: 0.1277\n",
      "Epoch 837/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1192 - val_loss: 0.1277\n",
      "\n",
      "Epoch 00837: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 838/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1269 - val_loss: 0.1277\n",
      "Epoch 839/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1121 - val_loss: 0.1276\n",
      "Epoch 840/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1140 - val_loss: 0.1277\n",
      "Epoch 841/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1196 - val_loss: 0.1277\n",
      "Epoch 842/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1119 - val_loss: 0.1276\n",
      "Epoch 843/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1172 - val_loss: 0.1276\n",
      "Epoch 844/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1326 - val_loss: 0.1276\n",
      "Epoch 845/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1288 - val_loss: 0.1278\n",
      "Epoch 846/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1150 - val_loss: 0.1277\n",
      "Epoch 847/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1257 - val_loss: 0.1276\n",
      "Epoch 848/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1213 - val_loss: 0.1276\n",
      "Epoch 849/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1255 - val_loss: 0.1278\n",
      "Epoch 850/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1215 - val_loss: 0.1278\n",
      "Epoch 851/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1208 - val_loss: 0.1276\n",
      "Epoch 852/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1139 - val_loss: 0.1276\n",
      "Epoch 853/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1192 - val_loss: 0.1276\n",
      "Epoch 854/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1281 - val_loss: 0.1277\n",
      "Epoch 855/2000\n",
      "95787/95787 [==============================] - 1s 10us/step - loss: 0.1322 - val_loss: 0.1276\n",
      "Epoch 856/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1210 - val_loss: 0.1277\n",
      "Epoch 857/2000\n",
      "95787/95787 [==============================] - 1s 9us/step - loss: 0.1158 - val_loss: 0.1276\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00857: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxddZnH8c9zb9YmaZO26ZpCS4HS\nhS4hFlAEyg4ji9gBKiggyoALo6gj6MyguOEyiKgj4gioo+0gCCJW9gIqULoAhW50oaVpSpOma5r9\n3mf+OCfpvclNmra5TZt836/XffWe3/mdc557uNwnv+WcY+6OiIhIW5GeDkBERA5NShAiIpKSEoSI\niKSkBCEiIikpQYiISEpKECIikpIShEg3MLNvmdkWM3uvp2MR6S5KENJrmNk6MzurB447CvgiMMHd\nh3XD/oaY2WwzqzCzHWb2DzM7MWH96WZWnrD8vJl9ss0+UtWpD2NtKTvLzNYdaLzSeylBiBy4I4Fq\nd6/c1w3NLCNFcT6wADgBGAj8GviLmeUfUJSwG/iPA9yH9CFKENInmNmnzGy1mW01s8fMbERYbmb2\nIzOrDP9aX2Jmk8J1F5jZMjPbZWYbzexLKfZ7FvA0MMLMaszsgbD8IjNbambbw7/exydss87MvmJm\nS4DdbZOEu6919zvdfZO7x9z9XiALGHeAp+FuYJaZHX2A+5E+QglCej0zOwP4LnAZMBxYD8wJV58D\nnAocCxQClwPV4bpfAf/i7gXAJOC5tvt292eA84EKd89392vM7FhgNvB5oBiYC/zZzLISNp0F/BNQ\n6O7Ne4l/KkGCWL2PH72tjcAvga8f4H6kj1CCkL7gSuA+d1/s7g3ArcDJZjYaaAIKgOMAc/fl7r4p\n3K4JmGBm/d19m7sv7uLxLgf+4u5Pu3sT8EMgF3h/Qp273X2Du9d1tiMz6w/8FviGu+/opOrdYWtl\nu5ltBx7voN53gQvNbGIXP4v0YUoQ0heMIGg1AODuNQSthJHu/hzwU+BnwGYzuzf8UQb4CHABsN7M\nXjCzk/fzeHFgAzAyoc6Gve3EzHKBPwOvuPt391L9JncvbHkBH0pVyd2rCD7v7Xs7vogShPQFFQQD\nyQCYWR4wiKDLBXe/291PACYSdDV9OSxf4O4XA0OAR4EH9/N4BoxqOV6o09som1l2eMyNwL908bhd\n9QNgBsEguEiHlCCkt8k0s5yEVwbwe+BaM5sa/vB+B5jv7uvM7H1mdqKZZRLM8qkHYmaWZWZXmtmA\nsJtoJxDrYgwPAv9kZmeG+/0i0AC81JWNw20eAuqAj4ctkG7j7tuB/wL+rTv3K72PEoT0NnMJflhb\nXl9392cJpnc+DGwCxgJXhPX7EwzcbiPoFqomGDMA+Biwzsx2AjcAV3UlAHdfGdb9CbAFuBC40N0b\nu/gZ3k/QRXQOsD2cHVVjZh9MPEwX99WRH9P1hCd9lOmBQSKHFzO7CLjd3af2dCzSu6kFIXIYCbvM\nPgIs7OlYpPdLdRWniByCzGwAweynRcDHezgc6QPUxSQiIimpi0lERFLqVV1MgwcP9tGjR/d0GCIi\nh41FixZtcffiVOt6VYIYPXo0Cxdq7E5EpKvMbH1H69TFJCIiKSlBiIhISkoQIiKSUq8ag0ilqamJ\n8vJy6uvrezqUXiMnJ4eSkhIyMzN7OhQRSaNenyDKy8spKChg9OjRBDfVlAPh7lRXV1NeXs6YMWN6\nOhwRSaNe38VUX1/PoEGDlBy6iZkxaNAgtchE+oBenyAAJYdupvMp0jf0iQSxN5t31rOrvqmnwxAR\nOaQoQQBVuxqoaej0ufH7pbq6mqlTpzJ16lSGDRvGyJEjW5cbG7v2aIBrr72WlStXdntsIiJ70+sH\nqbssDfcsHDRoEK+//joAX//618nPz+dLX/pS8mHdcXcikdS5+v777+/+wEREukAtiNDBvKft6tWr\nmTRpEjfccAOlpaVs2rSJ66+/nrKyMiZOnMjtt+95nvwpp5zC66+/TnNzM4WFhdxyyy1MmTKFk08+\nmcrKyoMYtYj0NX2qBfGNPy9lWcXOduW7G5vJjETIytj3fDlhRH9uu3DiPm+3bNky7r//fu655x4A\n7rjjDgYOHEhzczMzZsxg5syZTJgwIWmbHTt2cNppp3HHHXdw8803c99993HLLbfs87FFRLpCLYge\nMnbsWN73vve1Ls+ePZvS0lJKS0tZvnw5y5Yta7dNbm4u559/PgAnnHAC69atO1jhikgf1KdaEB39\npb904w6K8rIYUZh70GLJy8trfb9q1Sp+/OMf8+qrr1JYWMhVV12V8jqDrKys1vfRaJTm5u4fWBcR\naaEWxCFg586dFBQU0L9/fzZt2sSTTz7Z0yGJiPStFsShqrS0lAkTJjBp0iSOOuooPvCBD/R0SCIi\nveuZ1GVlZd72gUHLly9n/PjxnW63tGIHRf0ObhfT4a4r51VEDn1mtsjdy1KtUxeTiIikpAQhIiIp\nKUGIiEhKaRukNrP7gA8Ble4+KcX6LwNXJsQxHih2961mtg7YBcSA5o76x7pT7xmJERHpHulsQTwA\nnNfRSnf/gbtPdfepwK3AC+6+NaHKjHB92pODiIi0l7YE4e4vAlv3WjEwC5idrlhERGTf9fgYhJn1\nI2hpPJxQ7MBTZrbIzK7fy/bXm9lCM1tYVVW1fzG0HDENTj/99HYXvt111118+tOf7nCb/Px8ACoq\nKpg5c2aH+207pbetu+66i9ra2tblCy64gO3bt3c1dBHp43o8QQAXAv9o0730AXcvBc4HPmNmp3a0\nsbvf6+5l7l5WXFy8nyGk7wlps2bNYs6cOUllc+bMYdasWXvddsSIETz00EP7fey2CWLu3LkUFhbu\n9/5EpG85FBLEFbTpXnL3ivDfSuARYHr6w0hPE2LmzJk8/vjjNDQ0ALBu3ToqKiqYOnUqZ555JqWl\npRx//PH86U9/arftunXrmDQpGN+vq6vjiiuuYPLkyVx++eXU1dW11rvxxhtbbxV+2223AXD33XdT\nUVHBjBkzmDFjBgCjR49my5YtANx5551MmjSJSZMmcdddd7Ueb/z48XzqU59i4sSJnHPOOUnHEZG+\npUdvtWFmA4DTgKsSyvKAiLvvCt+fA9zewS72zV9vgffebFd8ZGMzGRGDjOi+73PY8XD+HR2uHjRo\nENOnT+eJJ57g4osvZs6cOVx++eXk5ubyyCOP0L9/f7Zs2cJJJ53ERRdd1OHznn/+85/Tr18/lixZ\nwpIlSygtLW1d9+1vf5uBAwcSi8U488wzWbJkCTfddBN33nkn8+bNY/DgwUn7WrRoEffffz/z58/H\n3TnxxBM57bTTKCoqYtWqVcyePZtf/vKXXHbZZTz88MNcddVVbcMRkT4gbS0IM5sNvAyMM7NyM7vO\nzG4wsxsSqn0YeMrddyeUDQX+bmZvAK8Cf3H3J9IV58GQ2M3U0r3k7nz1q19l8uTJnHXWWWzcuJHN\nmzd3uI8XX3yx9Yd68uTJTJ48uXXdgw8+SGlpKdOmTWPp0qUpbxWe6O9//zsf/vCHycvLIz8/n0sv\nvZS//e1vAIwZM4apU6cCuqW4SF+XthaEu++1k93dHyCYDptYthaYkpagOvhLf33FTvrnZlBS1C8t\nh73kkku4+eabWbx4MXV1dZSWlvLAAw9QVVXFokWLyMzMZPTo0Slv8Z0oVevinXfe4Yc//CELFiyg\nqKiIa665Zq/76ez+W9nZ2a3vo9GouphE+rBDYQyi18vPz+f000/nE5/4ROvg9I4dOxgyZAiZmZnM\nmzeP9evXd7qPU089ld/97ncAvPXWWyxZsgQIbhWel5fHgAED2Lx5M3/9619btykoKGDXrl0p9/Xo\no49SW1vL7t27eeSRR/jgBz/YXR9XRHoJ3e77IJk1axaXXnppa1fTlVdeyYUXXkhZWRlTp07luOOO\n63T7G2+8kWuvvZbJkyczdepUpk8Pxu2nTJnCtGnTmDhxYrtbhV9//fWcf/75DB8+nHnz5rWWl5aW\ncs0117Tu45Of/CTTpk1Td5KIJNHtvoFlm3bSPyd9XUy9kW73LdI76HbfIiKyz5QgSOdlciIih68+\nkSB6UzfaoUDnU6Rv6PUJIicnh+rq6r3/qOk3r0vcnerqanJycno6FBFJs14/i6mkpITy8nI6u5Hf\nph31bMuMsGtz1kGM7PCVk5NDSUlJT4chImnW6xNEZmYmY8aM6bTOtd95llOPHcz3Z2pWjohIi17f\nxdQVZqBudRGRZEoQaBaTiEgqShAhNSBERJIpQRDcBE9dTCIiyZQgREQkJSWIkKuTSUQkiRIEwSwm\nERFJpgTRQg0IEZEkShCE10H0dBAiIocYJQjAdCWEiEg7aUsQZnafmVWa2VsdrD/dzHaY2evh6z8T\n1p1nZivNbLWZ3ZKuGBPpDqUiIsnS2YJ4ADhvL3X+5u5Tw9ftAGYWBX4GnA9MAGaZ2YQ0xqlBahGR\nFNKWINz9RWDrfmw6HVjt7mvdvRGYA1zcrcGloPaDiEiynh6DONnM3jCzv5rZxLBsJLAhoU55WJaS\nmV1vZgvNbGFnt/TujKGb9YmItNWTCWIxcKS7TwF+Ajwalqfq8Onw59vd73X3MncvKy4u3q9ATH1M\nIiLt9FiCcPed7l4Tvp8LZJrZYIIWw6iEqiVARdrjSfcBREQOMz2WIMxsmIV/upvZ9DCWamABcIyZ\njTGzLOAK4LG0xpLOnYuIHKbS9kQ5M5sNnA4MNrNy4DYgE8Dd7wFmAjeaWTNQB1zhwVzTZjP7LPAk\nEAXuc/el6Yqzhaa5iogkS1uCcPdZe1n/U+CnHaybC8xNR1wp6UpqEZF2enoW0yFBXUwiIu0pQbRQ\nE0JEJIkSBJrmKiKSihJESA8MEhFJpgSBrqQWEUlFCQLdrE9EJBUliJBaECIiyZQgCB4YpDEIEZFk\nShCoi0lEJBUliJC6mEREkilBiIhISkoQITUgRESSKUEQXEmtLiYRkWRKEOhmfSIiqShBtFITQkQk\nkRIEmuYqIpKKEkRIYxAiIsmUIAhaEMoPIiLJlCAIbrUhIiLJ0pYgzOw+M6s0s7c6WH+lmS0JXy+Z\n2ZSEdevM7E0ze93MFqYrxkSuPiYRkSTpbEE8AJzXyfp3gNPcfTLwTeDeNutnuPtUdy9LU3ytNEgt\nItJeRrp27O4vmtnoTta/lLD4ClCSrli6Qu0HEZFkh8oYxHXAXxOWHXjKzBaZ2fWdbWhm15vZQjNb\nWFVVtV8H1xPlRETaS1sLoqvMbAZBgjglofgD7l5hZkOAp81shbu/mGp7d7+XsHuqrKxs/37m1cck\nItJOj7YgzGwy8D/Axe5e3VLu7hXhv5XAI8D0dMeiBoSISLIeSxBmdgTwR+Bj7v52QnmemRW0vAfO\nAVLOhOq2WNK5cxGRw1TaupjMbDZwOjDYzMqB24BMAHe/B/hPYBDw3xZ08TSHM5aGAo+EZRnA7939\niXTF2ULTXEVEkqVzFtOsvaz/JPDJFOVrgSntt0gfDUGIiLR3qMxi6lHKDyIi7SlBhNTDJCKSTAmC\n8IlymsckIpJECQJ1MYmIpKIEEVIXk4hIMiUINItJRCQVJYiQWhAiIsmUIAgeGKRBahGRZEoQoFFq\nEZEUlCBC6mISEUmmBIEaECIiqShBhNSAEBFJpgRBOM1VGUJEJIkSBMEsJhERSdalBGFmY80sO3x/\nupndZGaF6Q3t4NI0VxGRZF1tQTwMxMzsaOBXwBjg92mL6iDTldQiIu11NUHE3b0Z+DBwl7t/ARie\nvrAOPk1zFRFJ1tUE0WRms4CrgcfDssz0hHTwmWmMWkSkra4miGuBk4Fvu/s7ZjYG+N/0hXVwaZBa\nRKS9LiUId1/m7je5+2wzKwIK3P2OvW1nZveZWaWZvdXBejOzu81stZktMbPShHVXm9mq8HV1lz/R\nfnL1MYmIJOnqLKbnzay/mQ0E3gDuN7M7u7DpA8B5naw/HzgmfF0P/Dw83kDgNuBEYDpwW5iY0kKD\n1CIi7XW1i2mAu+8ELgXud/cTgLP2tpG7vwhs7aTKxcBvPPAKUGhmw4Fzgafdfau7bwOepvNEc0Bu\nrfwy59c+lq7di4gclrqaIDLCH+7L2DNI3R1GAhsSlsvDso7K2zGz681soZktrKqq2q8gjmpcyZDY\n5v3aVkSkt+pqgrgdeBJY4+4LzOwoYFU3HD9V5453Ut6+0P1edy9z97Li4uL9CiJOhAjx/dpWRKS3\nyuhKJXf/A/CHhOW1wEe64fjlwKiE5RKgIiw/vU35891wvJTiFsVcCUJEJFFXB6lLzOyRcEbSZjN7\n2MxKuuH4jwEfD2cznQTscPdNBK2Vc8ysKBycPicsS4ugBRFL1+5FRA5LXWpBAPcT3Frjn8Plq8Ky\nszvbyMxmE7QEBptZOcHMpEwAd78HmAtcAKwGagmut8Ddt5rZN4EF4a5ud/fOBrsPiKuLSUSkna4m\niGJ3vz9h+QEz+/zeNnL3WXtZ78BnOlh3H3BfF+M7IHGLEFEXk4hIkq4OUm8xs6vMLBq+rgKq0xnY\nwaRBahGR9rqaID5BMMX1PWATMJOwO6g3iBNVC0JEpI2u3mrjXXe/yN2L3X2Iu19CcNFcrxA3DVKL\niLR1IE+Uu7nbouhhGqQWEWnvQBJEr7mDkQapRUTaO5AE0Wtuf6pBahGR9jqd5mpmu0idCAzITUtE\nPUBXUouItNdpgnD3goMVSE+KEyXqGqQWEUl0IF1MvUYwi0ktCBGRREoQaAxCRCQVJQjA1YIQEWlH\nCYKWK6k1BiEikkgJArUgRERSUYJAYxAiIqkoQRB2MSlBiIgkUYIg7GLShXIiIkmUIAiug4iqBSEi\nkkQJAt3NVUQkFSUIgnsxKUGIiCRLa4Iws/PMbKWZrTazW1Ks/5GZvR6+3jaz7QnrYgnrHktnnHEi\nug5CRKSNTm/WdyDMLAr8DDgbKAcWmNlj7r6spY67fyGh/ueAaQm7qHP3qemKL5GugxARaS+dLYjp\nwGp3X+vujcAc4OJO6s8CZqcxng7pOggRkfbSmSBGAhsSlsvDsnbM7EhgDPBcQnGOmS00s1fM7JKO\nDmJm14f1FlZVVe1XoK4xCBGRdtKZIFI9krSjp9BdATzknjQQcIS7lwEfBe4ys7GpNnT3e929zN3L\niouL9ytQTXMVEWkvnQmiHBiVsFwCVHRQ9wradC+5e0X471rgeZLHJ7qVrqQWEWkvnQliAXCMmY0x\nsyyCJNBuNpKZjQOKgJcTyorMLDt8Pxj4ALCs7bbdRVdSi4i0l7ZZTO7ebGafBZ4EosB97r7UzG4H\nFrp7S7KYBcxx98Tup/HAL8wsTpDE7kic/dTtsWqQWkSknbQlCAB3nwvMbVP2n22Wv55iu5eA49MZ\nW6JgDELXQYiIJNKV1OhKahGRVJQgANcgtYhIO0oQaJqriEgqShCoBSEikooSBC33YnLwjq7jExHp\ne5QgCLqYgjeaySQi0kIJguBmfQDolt8iIq2UIAhu1geoBSEikkAJgmAMInijBCEi0kIJguBmfcEb\nJQgRkRZKECS2IDTVVUSkhRIECQlCLQgRkVZKECQMUmsMQkSklRIEwe2+AbUgREQSKEEQ3M0VUAtC\nRCSBEgQagxARSUUJAiDS0oLQLCYRkRZKEEA0qusgRETaSmuCMLPzzGylma02s1tSrL/GzKrM7PXw\n9cmEdVeb2arwdXU644xGM4M3GoMQEWmVtmdSm1kU+BlwNlAOLDCzx9x9WZuq/+fun22z7UDgNqAM\ncGBRuO22dMQajQanwWNNWDoOICJyGEpnC2I6sNrd17p7IzAHuLiL254LPO3uW8Ok8DRwXpriJJoR\ndDE1NTen6xAiIoeddCaIkcCGhOXysKytj5jZEjN7yMxG7eO23aKli6mpuSldhxAROeykM0Gk6q1p\n+8i2PwOj3X0y8Azw633YNqhodr2ZLTSzhVVVVfsVaEsXU3OTWhAiIi3SmSDKgVEJyyVARWIFd692\n94Zw8ZfACV3dNmEf97p7mbuXFRcX71eg0YwgQaiLSURkj3QmiAXAMWY2xsyygCuAxxIrmNnwhMWL\ngOXh+yeBc8ysyMyKgHPCsrTIaEkQakGIiLRK2ywmd282s88S/LBHgfvcfamZ3Q4sdPfHgJvM7CKg\nGdgKXBNuu9XMvkmQZABud/et6Yq1pQXRrDEIEZFWaUsQAO4+F5jbpuw/E97fCtzawbb3AfelM74W\nlp0PQLx+18E4nIjIYUFXUgORnAEAxOt39nAkIiKHDiUIIJIbJAiv39HDkYiIHDqUIIB+BYUAxOqU\nIEREWihBAAV5+TR4JvE6dTGJiLRQggD652awk37qYhIRSaAEAeRnZ7DJB5Kze2NPhyIicshQggAy\nohE22jAK6sp7OhQRkUOGEkRoa+YwBjWUQ1NdT4ciInJIUIIIZRUMCt58exg8/72eDUZE5BCgBBHK\nLxy8Z+H57/RcICIihwgliFB+/4E9HYKIyCFFCSI0MC+zp0MQETmkKEGEBg0cnFzQuLtnAhEROUQo\nQYSGln6IVyLT9hQ8emPPBSMicghQgghZJMKa0VfsKShf2HPBiIgcApQgEkwaPWzPQiTac4GIiBwC\nlCASjB9euGfBlCBEpG9TgkiQlblnJlNDc6wHIxER6XlKEImGTmh9W7WzlvXVCTOZtm+ArWt7ICgR\nkZ6R1gRhZueZ2UozW21mt6RYf7OZLTOzJWb2rJkdmbAuZmavh6/H0hlnq9wiGHcBADGPcNoPnuf2\nPy8L1t01Ce6e1snGIiK9S9oShJlFgZ8B5wMTgFlmNqFNtdeAMnefDDwEfD9hXZ27Tw1fF6UrznYu\n/hkAR0Yq+WvWLWx8+UHeqdzLg4Rqt0Jz40EITkTk4ElnC2I6sNrd17p7IzAHuDixgrvPc/facPEV\noCSN8XRNvz233BgfeZdfZP2IjHtOSq7jHiSFFX8Jlr8/Bv5wdef7LV8IjbWd1xEROYSkM0GMBDYk\nLJeHZR25DvhrwnKOmS00s1fM7JKONjKz68N6C6uqqg4s4hb9k8McFd/zIKFVGyth8a+DpDDno/CD\no4MVK+cm7+N7o+Hl/w7e73oP/udM+PO/dk98IiIHQUYa920pyjxlRbOrgDLgtITiI9y9wsyOAp4z\nszfdfU27HbrfC9wLUFZWlnL/++xfXoTmhuBHf+6XklZV/GImkaJcxrYU7G6TlNzhhe9D3TZ48lY4\n+dNQtz3ceHFy3Xf+BvFmGDujW8IWEelO6WxBlAOjEpZLgIq2lczsLOBrwEXu3tBS7u4V4b9rgeeB\ngzdCnDcYBoyE6Z9qt+q0yBuM3fFK6u3iMdj2TvLtwut3QEM4hhFpk49//SH4bYeNoz3qtgX7ERE5\niNLZglgAHGNmY4CNwBXARxMrmNk04BfAee5emVBeBNS6e4OZDQY+QPIA9sHzmQWQ0z+Y5vqrszqv\ne/tA6sd9mJzEskW/Bo8H7xMTxHtvdT2G742GzDz4Wrv8KiKSNmlrQbh7M/BZ4ElgOfCguy81s9vN\nrGVW0g+AfOAPbaazjgcWmtkbwDzgDndflq5YO1V8LBQMg1Hvg5n37bV6zspHkgue/g945rbg/ea3\niH+nhMaNS/D7zttTZ+0L8NJPg/feQS9Z026oXA7zfwHxeOo6lStg+7sdbF8Hy//c8f67oqYSqtv1\n8olIL2V+ID8Yh5iysjJfuDDNN9nbvQUe/zyMOhGe+veg7MK74c83HfCu66ZeS9aKR6k45kqyaGbo\nR74HsWb45qDkimf8O5z6Zdj6Djz+BcjKg0v+G+44Ilh/2ldgxleD97EmeOF7QTfVgv+By34LExJm\nDcfjENnL3wlv/RGOPQ9+Ugq7NsHX96G7q6k+uK9VtBuftzHnSigpg1O+0H37FOmjzGyRu5elXKcE\ncQCa6mHTGzBqOlQug3dfgbpt1NfuombpE2TWVzOgqYqdWUPI8Cb6NW3bp92/NvbTTFvz3x2uj+cU\nEakP95nZD5oSptGe9BlY8TiUXQvPfH1P+ftvgnO+Gbxf+wL89sNw9Jkw7WMw/kKwNnML3nsT7jkF\nJl8BS+YEZTcvh+yCoJvsyJP31L1zAgyfArNm7yn7+gAoeV+QmP7xY5h2ZbC/TzwFR5y4T+cjaZ+w\nb4lKRFLqLEGkcwyi98vM2fMjN3Ri8AJygJzzvhGUN9TQP5oJS/4PHvscW7NGkN9YSSYxLPWkrlad\nJQdgT3KA5OQA8EpwwV9ScgDia19g59/upXDkuKDry2Ow6ilY9RTNY88mY83TeNFo7INfgtKPBdd7\nwJ7kAEFSfPVeWPMcfG5xMGtr3Hmwc2PwatEyzlK+AB77HKx+GqpXBWVv/gGGjIfmesgrbp+YWgOO\nwYs/gAGjICMbjk4YB/rVuXDG12DMqZ2ep065Q211MDFBRJKoBXGwuAfjA4VHBD+G8Xjw7/PfhYFj\naRx8HLF3X6Vi/WrWbN5BRVU112Q8RbUN5Iy6O9hJP06MrGBO1rc6PcxDsVOZGX2xW0JemH0iZQ3z\n93m7zTcsY2jVy/DwdR3WaZp2DZElc4jG6oOCrAI47zuwZRVk5gbXjpz6Jbh3BtRuad0uRpQobW6k\n+LnFMGgs++XtJ+H3l8HHHtV0Y+mT1MV0uHIHMzZsreXhxeVcOGUEz7y8mLnz3+LSYZu58IhGCiP1\nPLqxgEveu5sqG8wPxs1m3bvv8lDtJ3g+NoVNOWMZYLVc0PgE1V7AINuVdIjdnk0GcW5p+iQ/yvp5\nD33QAzR8SnDtSkficdhVAQPCC/XfnQ/LH4OTboQfBa0+jvsQXPE7eOkngMH7PxuUb1sH77wIpR9P\n3mdDTTC2kpkb3GZly9tBksrI6bg1JHIIUoLog1YsfY3hJWMYMKAwuOhvy9u860NZuHojZzOfgpp1\nNB1/Bc9uG8Lm7TVEMrN54vV1zF+3nR9PXM13lw/h89EHqSeLJo+SYzFmRZ/hhch0jvb1bIvl8ky8\nlJdjE7ksYx5rcybyRM3RPJn1FTIseZbV07ETmBxZw1ALLhj8afPFfDbjT/v1ud6Mj+b4yLr2K075\nApRMhwW/pOG0/+CprUP50OTh2NwvwsJw9tlVf4Snb4PNbwbLFtkzBRnwguHYrk179jn+QrZtWEFR\nzSpi1z1H9I3/hYmXwpgPwgACzxQAAA6qSURBVLeGBRc53vgPWPArePUXvHf0FQzzKug3CCZfDgVD\ng+nRjbuJb3oDio8jMrI0OGZO/2D7ZX+CKR8Nk00/qF7NroYmfrmqgJkjt3FE3Qo49twg1vwhQVw7\nNoZdYgaxBohmQ+VSmPfdoKvxY4/SRISmWJx+USCaEXQVvva/cPJngmP97U6Y920YUQoX/hiy82H9\nyzD5MiW4PkYJQvZZxfY6qmsaOW54AfVNMaIRIxZ3CnKC2Ug1Dc3MXbKJE0YXMXxADv2yguGs197d\nxmvrqli1+Hk+deVHqdrVwOdmv8bpxxRxXexBxp02i0c2D2bp3F+Qn5tN/XGX0tjUxNr3ttGv+k3O\nGp1FsW9hy4p/8GJsMiV5Mepra1gUP5Z3cieyvbaBm0dv4Kq8+dyw8+Oct+kXXJvx5EE9N+9mjeWI\nxu6b7lubPYR+DZVJZQ82n8ZlGS8klW3NPxaAgTVvd76/0k/x9tLXmNoQ/L+wesD7OXrHS63rfeyZ\n2JpnU2/80QdpPOpssiIEiSzaZpiyfmdw4WdLa2zxb2Db+iCxFI/byydNrb4pRm1jjIF5Wfu1vRwY\nJQjpUfG4E4ns21+ldY0x6pqCH42G5hjvVtdyzNCCpDruztX3L2DImoc4O7KIMbaJYyMbWRo/kuPs\nXaLm7PRcHGNx/BhmRN8AYF18KKMjmwF4I34Uj8dO4muZv+/aZ3EjYu3/n5kXm8LJkWW84WM5MbKi\n3fpU3Xsttnh/Btte7hgMLI+PooEspkbSey3K07ETmJq9keLm93g9u4xB2c6oyBaqskoorvwHAPWZ\nRWwtOYMR7zy8Z8MpH4Vjz6Gh6Bj+XmGM6tfMsUPzg0kKky/nyWWb2Vn5Lv98XHZw+5lhx4MZV/7m\nTVa/s5753/hw0JIJra6soa4xxvJNO7nsfXtuyvDK2mrmvrmJL549jgH9MllTVcOZ//UCc2/6IBNG\n9O/4g7kHrbb9nHLt7sQdognf5apdDdQ2NnPkoLykuuurd1NckN36hxPAvBWVDC/M4bhhHcfo7sx+\ndQOnjSumuqaB1zdsZ8V7u/j2JZOwsGW3YWstJUW5rctVuxooLsjer88EShDSizU0x3h5TTUjC3Np\nijkji3L5+K/mEzXnE6ccRRxj+aadrHxvF4OzmlhS2cyu+ma2bt9GMxk0hRP5vv3hSfxx8UYq310B\nA46gancTx0YryMswXq4ZCkAODTSRwSirZJ0PB4I7/n7z4+cx84HE6zgdMIZTzSDbQfbgMWyqz+TI\n3W/wWvxoosS54ZidjByYz+x1Bby2uZGB7OKEyNus9eGs9pGcGVlM/4FDOb3pb7xVU8DvY2ewk+DH\nc7Kt4dzoAh6PncxaH84no3PZRS5zYyfxwcgSxkXK+UvsRCZH1jKInTSSwS76McXW8GB8BoNznLX1\neUy35dR6NkdHKlgVH8k50YWcH12QdH53eS4FVtct/606Sq4tmiPZ1MUziHozNVmDqW9oJGJOf3bT\nQDYDcjNojPajtmYnWTSx1QvIy8mkpr4ZgIysbPIy4liskebmZuIOxcVDiNZvY2tdDJrqyIs0UZ85\ngIjHqI4WMyA3i4H5OWARVlfVsr2uifEjCqmpb6J+5xaGF+axsdao2b2b/rmZ7KhtJDO7H8MHF5Kb\nlcXi9VuIxeKMGdKf8u2NxIkwcXgei9ZvwzGKsuIUDejP4P55vLQmmGyRl53BiWMG8u7WWlZX1pCd\nEeWo4jwqtteTEY2wpaah3bnpn5tJv6woRw7K45W1W8mMGuOHD6C2Kc7bm3dx8sSxFF1xz379d1GC\nEElhZ30TETMyIkZOZupnkLt7619qTbE4z62o5KSjBpGTGeGZZZW8f+wgivKy2FLTQL+sKE0xp39O\nRus2iZpiceqbYuRlZbS2qOJxZ8V7u6hrivGP1Vu4+uTRLFy/lXHDCigp6sc7W3bzzLLNXDJtJH9b\nVUXZkQNZtmkHRf2yGJiXxedmv8YXzj6WcyYM5X/nv8ugvCy21zbx1Ufe5C83ncKQghz+7aE3mLey\nis/OOJovnTuu9XNt3F7H/f9Yx+fPOoaCnEy+/8QK/vH2Jv799GJu+v1itpGPY4yzDWzyQdSSzQir\nZpWXMJStVFKIEyGfWpqJcpxtYHJkDVHiRIkTI8Iw20o+9Wwh+Ks5gxhr4yOIYxTbdjKIkWXNjLIq\nMohR71nUk0mR1dBMFA/v+dnkUeJEKLBa6jybOEY/C35IW6aLZ9NEE1HqySLuESLmFFDLLnJxIgyg\nhgKro9yLcYdhthXHyIhARsSJNceJhONnEeLs8n7EiJBHPc1EMZxGMsmlgWxrIkqcOEacCEF0wcsx\nHMNwGsgkl0Yie5nSvrcp7+G3MaH+nm0MJ2dAMUd88YUOtuucEoRIH7OlpoHB+fvf7fCHhRu465lV\n/Oa66ayprOG/nnqbfy4r4fiRAyjsl8VbG3fwkRNKWPneLo4c1I/nV1byh4XlfPz9o3lu+WZKjyzi\n+0+s5MvnjqMoL4tbH17CjOOG8PbmXSxYt43B+Vl88+JJvFO9m+8/sbL1uLmZUcYNK+Cfjh/OE0vf\nY1nFTn4yaxp/X72FB15aB8DIwlyu/cBofvzMKs6eMJQ/vraxg0+xx7//03jueWEtW2oa+PK54/jB\nk8ExjxtWwIattexujDEgN5Oifpmsq27/3JYRA3Ko2FHPZWUlxOLw5sbtrK+upaE5zsjCXDZuD1pZ\nJx01kFfWbmVAbibFBdmsrqxp3ceUkgFkRCMsWh9cv3TDaWPZuruBBxeWJ9W5cMoIvvWX5QCccdwQ\nJo7oz0+eW90upoyI0Rx3sjMi5GVn8PevzEjq0uoqJQgROWQ9t2IzxwwpYGRhbruxqsQWXEdeWVvN\nyvd2cf7xw5i/ditnHDeEN8q3s6u+mXMnDku5TUNzjIgZmdFI63GaYk5zPM6S8h1MHz2QF1dVcdqx\nxZgZTbE4Syt2MqVkQFI8LYl4WcVOBudnMaR/0q06cXf+vGQTM8YVt07w2Lq7kfJttUwuKWyt99tX\n1jNuaAHTxwzE3dm0o54RhblJ+3rktXIWrtvGpaUllBTlUtgvk0Xrt3H0kHxicWf4gOT6XaUEISIi\nKXWWINL5PAgRETmMKUGIiEhKShAiIpKSEoSIiKSkBCEiIikpQYiISEpKECIikpIShIiIpNSrLpQz\nsypg/X5uPhjYstdafZPOTcd0bjqmc9OxQ+ncHOnuxalW9KoEcSDMbGFHVxP2dTo3HdO56ZjOTccO\nl3OjLiYREUlJCUJERFJSgtjj3p4O4BCmc9MxnZuO6dx07LA4NxqDEBGRlNSCEBGRlJQgREQkpT6f\nIMzsPDNbaWarzeyWno7nYDOzUWY2z8yWm9lSM/vXsHygmT1tZqvCf4vCcjOzu8PztcTMSnv2E6Sf\nmUXN7DUzezxcHmNm88Nz839mlhWWZ4fLq8P1o3sy7nQzs0Ize8jMVoTfn5P1vQmY2RfC/5/eMrPZ\nZpZzOH5v+nSCMLMo8DPgfGACMMvMJvRsVAddM/BFdx8PnAR8JjwHtwDPuvsxwLPhMgTn6pjwdT3w\n84Mf8kH3r8DyhOXvAT8Kz8024Lqw/Dpgm7sfDfworNeb/Rh4wt2PA6YQnKM+/70xs5HATUCZu08C\nosAVHI7fG3fvsy/gZODJhOVbgVt7Oq4ePid/As4GVgLDw7LhwMrw/S+AWQn1W+v1xhdQQvBDdwbw\nOGAEV8BmtP0OAU8CJ4fvM8J61tOfIU3npT/wTtvPp++NA4wENgADw+/B48C5h+P3pk+3INjzH7JF\neVjWJ4VN22nAfGCou28CCP8dElbra+fsLuDfgHi4PAjY7u7N4XLi5289N+H6HWH93ugooAq4P+x+\n+x8zy0PfG9x9I/BD4F1gE8H3YBGH4femrycIS1HWJ+f9mlk+8DDweXff2VnVFGW98pyZ2YeASndf\nlFicoqp3YV1vkwGUAj9392nAbvZ0J6XSZ85NOO5yMTAGGAHkEXSxtXXIf2/6eoIoB0YlLJcAFT0U\nS48xs0yC5PA7d/9jWLzZzIaH64cDlWF5XzpnHwAuMrN1wByCbqa7gEIzywjrJH7+1nMTrh8AbD2Y\nAR9E5UC5u88Plx8iSBj63sBZwDvuXuXuTcAfgfdzGH5v+nqCWAAcE84uyCIYSHqsh2M6qMzMgF8B\ny939zoRVjwFXh++vJhibaCn/eDgr5SRgR0uXQm/j7re6e4m7jyb4bjzn7lcC84CZYbW256blnM0M\n6x8Sfwl2N3d/D9hgZuPCojOBZeh7A0HX0klm1i/8/6vl3Bx+35ueHgTp6RdwAfA2sAb4Wk/H0wOf\n/xSC5uwS4PXwdQFBH+izwKrw34FhfSOY+bUGeJNgpkaPf46DcJ5OBx4P3x8FvAqsBv4AZIflOeHy\n6nD9UT0dd5rPyVRgYfjdeRQo0vem9dx8A1gBvAX8Fsg+HL83utWGiIik1Ne7mEREpANKECIikpIS\nhIiIpKQEISIiKSlBiIhISkoQIvvAzGJm9nrCq9vuAGxmo83sre7an8iByth7FRFJUOfuU3s6CJGD\nQS0IkW5gZuvM7Htm9mr4OjosP9LMng2fgfCsmR0Rlg81s0fM7I3w9f5wV1Ez+2X4LIGnzCy3xz6U\n9HlKECL7JrdNF9PlCet2uvt04KcE92wifP8bd58M/A64Oyy/G3jB3acQ3MNoaVh+DPAzd58IbAc+\nkubPI9IhXUktsg/MrMbd81OUrwPOcPe14c0P33P3QWa2heC5B01h+SZ3H2xmVUCJuzck7GM08LQH\nD5TBzL4CZLr7t9L/yUTaUwtCpPt4B+87qpNKQ8L7GBonlB6kBCHSfS5P+Pfl8P1LBHeCBbgS+Hv4\n/lngRmh95nX/gxWkSFfprxORfZNrZq8nLD/h7i1TXbPNbD7BH16zwrKbgPvM7MsET2C7Niz/V+Be\nM7uOoKVwI8HTx0QOGRqDEOkG4RhEmbtv6elYRLqLuphERCQltSBERCQltSBERCQlJQgREUlJCUJE\nRFJSghARkZSUIEREJKX/B+bnAm4TucsKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.058575705231312\n",
      "Training 2JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 911004 samples, validate on 229670 samples\n",
      "Epoch 1/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.8486 - val_loss: 0.6799\n",
      "Epoch 2/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.6438 - val_loss: 0.5817\n",
      "Epoch 3/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.5933 - val_loss: 0.5379\n",
      "Epoch 4/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.5628 - val_loss: 0.5238\n",
      "Epoch 5/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.5368 - val_loss: 0.5015\n",
      "Epoch 6/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.5193 - val_loss: 0.4818\n",
      "Epoch 7/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.5057 - val_loss: 0.4788\n",
      "Epoch 8/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4942 - val_loss: 0.4631\n",
      "Epoch 9/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4813 - val_loss: 0.4521\n",
      "Epoch 10/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4735 - val_loss: 0.4442\n",
      "Epoch 11/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4664 - val_loss: 0.4442\n",
      "Epoch 12/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4603 - val_loss: 0.4328\n",
      "Epoch 13/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4512 - val_loss: 0.4173\n",
      "Epoch 14/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.4455 - val_loss: 0.4200\n",
      "Epoch 15/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4419 - val_loss: 0.4140\n",
      "Epoch 16/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.4350 - val_loss: 0.4110\n",
      "Epoch 17/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4296 - val_loss: 0.4107\n",
      "Epoch 18/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4257 - val_loss: 0.4056\n",
      "Epoch 19/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4212 - val_loss: 0.4007\n",
      "Epoch 20/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4176 - val_loss: 0.3954\n",
      "Epoch 21/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4157 - val_loss: 0.3863\n",
      "Epoch 22/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4108 - val_loss: 0.3977\n",
      "Epoch 23/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4075 - val_loss: 0.3872\n",
      "Epoch 24/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4058 - val_loss: 0.3834\n",
      "Epoch 25/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4027 - val_loss: 0.3804\n",
      "Epoch 26/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.4005 - val_loss: 0.3809\n",
      "Epoch 27/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3967 - val_loss: 0.3694\n",
      "Epoch 28/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3951 - val_loss: 0.3844\n",
      "Epoch 29/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3928 - val_loss: 0.3683\n",
      "Epoch 30/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3915 - val_loss: 0.3693\n",
      "Epoch 31/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3875 - val_loss: 0.3674\n",
      "Epoch 32/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3867 - val_loss: 0.3655\n",
      "Epoch 33/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3869 - val_loss: 0.3626\n",
      "Epoch 34/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3833 - val_loss: 0.3579\n",
      "Epoch 35/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3822 - val_loss: 0.3615\n",
      "Epoch 36/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3792 - val_loss: 0.3679\n",
      "Epoch 37/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3787 - val_loss: 0.3614\n",
      "Epoch 38/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3772 - val_loss: 0.3570\n",
      "Epoch 39/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3755 - val_loss: 0.3631\n",
      "Epoch 40/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3741 - val_loss: 0.3553\n",
      "Epoch 41/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3723 - val_loss: 0.3597\n",
      "Epoch 42/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3736 - val_loss: 0.3526\n",
      "Epoch 43/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3691 - val_loss: 0.3478\n",
      "Epoch 44/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3692 - val_loss: 0.3502\n",
      "Epoch 45/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3674 - val_loss: 0.3478\n",
      "Epoch 46/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3675 - val_loss: 0.3464\n",
      "Epoch 47/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3656 - val_loss: 0.3508\n",
      "Epoch 48/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3635 - val_loss: 0.3432\n",
      "Epoch 49/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3618 - val_loss: 0.3455\n",
      "Epoch 50/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3615 - val_loss: 0.3411\n",
      "Epoch 51/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3608 - val_loss: 0.3412\n",
      "Epoch 52/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3614 - val_loss: 0.3413\n",
      "Epoch 53/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3576 - val_loss: 0.3420\n",
      "Epoch 54/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3560 - val_loss: 0.3447\n",
      "Epoch 55/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3548 - val_loss: 0.3398\n",
      "Epoch 56/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3567 - val_loss: 0.3415\n",
      "Epoch 57/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3539 - val_loss: 0.3350\n",
      "Epoch 58/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3550 - val_loss: 0.3364\n",
      "Epoch 59/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3534 - val_loss: 0.3374\n",
      "Epoch 60/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3517 - val_loss: 0.3372\n",
      "Epoch 61/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3528 - val_loss: 0.3333\n",
      "Epoch 62/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3508 - val_loss: 0.3388\n",
      "Epoch 63/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3491 - val_loss: 0.3343\n",
      "Epoch 64/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3487 - val_loss: 0.3369\n",
      "Epoch 65/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3473 - val_loss: 0.3327\n",
      "Epoch 66/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3461 - val_loss: 0.3321\n",
      "Epoch 67/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3475 - val_loss: 0.3412\n",
      "Epoch 68/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3483 - val_loss: 0.3265\n",
      "Epoch 69/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3452 - val_loss: 0.3301\n",
      "Epoch 70/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3432 - val_loss: 0.3279\n",
      "Epoch 71/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3452 - val_loss: 0.3260\n",
      "Epoch 72/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3449 - val_loss: 0.3298\n",
      "Epoch 73/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3447 - val_loss: 0.3297\n",
      "Epoch 74/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3437 - val_loss: 0.3267\n",
      "Epoch 75/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3419 - val_loss: 0.3262\n",
      "Epoch 76/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3420 - val_loss: 0.3257\n",
      "Epoch 77/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3399 - val_loss: 0.3223\n",
      "Epoch 78/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3387 - val_loss: 0.3315\n",
      "Epoch 79/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3392 - val_loss: 0.3216\n",
      "Epoch 80/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3372 - val_loss: 0.3236\n",
      "Epoch 81/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3372 - val_loss: 0.3306\n",
      "Epoch 82/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3373 - val_loss: 0.3276\n",
      "Epoch 83/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3369 - val_loss: 0.3315\n",
      "Epoch 84/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3374 - val_loss: 0.3191\n",
      "Epoch 85/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3359 - val_loss: 0.3218\n",
      "Epoch 86/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3374 - val_loss: 0.3169\n",
      "Epoch 87/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3334 - val_loss: 0.3204\n",
      "Epoch 88/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3339 - val_loss: 0.3196\n",
      "Epoch 89/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3327 - val_loss: 0.3343\n",
      "Epoch 90/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3330 - val_loss: 0.3170\n",
      "Epoch 91/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3324 - val_loss: 0.3319\n",
      "Epoch 92/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3324 - val_loss: 0.3208\n",
      "Epoch 93/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3325 - val_loss: 0.3293\n",
      "Epoch 94/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3322 - val_loss: 0.3167\n",
      "Epoch 95/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3300 - val_loss: 0.3226\n",
      "Epoch 96/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3333 - val_loss: 0.3228\n",
      "Epoch 97/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3315 - val_loss: 0.3203\n",
      "Epoch 98/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3303 - val_loss: 0.3141\n",
      "Epoch 99/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3297 - val_loss: 0.3180\n",
      "Epoch 100/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3279 - val_loss: 0.3149\n",
      "Epoch 101/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3287 - val_loss: 0.3124\n",
      "Epoch 102/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3275 - val_loss: 0.3156\n",
      "Epoch 103/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3293 - val_loss: 0.3200\n",
      "Epoch 104/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3283 - val_loss: 0.3164\n",
      "Epoch 105/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3274 - val_loss: 0.3141\n",
      "Epoch 106/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3255 - val_loss: 0.3152\n",
      "Epoch 107/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3258 - val_loss: 0.3144\n",
      "Epoch 108/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3256 - val_loss: 0.3126\n",
      "Epoch 109/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3265 - val_loss: 0.3115\n",
      "Epoch 110/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3266 - val_loss: 0.3207\n",
      "Epoch 111/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3254 - val_loss: 0.3133\n",
      "Epoch 112/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3246 - val_loss: 0.3132\n",
      "Epoch 113/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3246 - val_loss: 0.3139\n",
      "Epoch 114/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3220 - val_loss: 0.3146\n",
      "Epoch 115/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3250 - val_loss: 0.3115\n",
      "Epoch 116/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3239 - val_loss: 0.3092\n",
      "Epoch 117/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3214 - val_loss: 0.3124\n",
      "Epoch 118/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3233 - val_loss: 0.3191\n",
      "Epoch 119/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3229 - val_loss: 0.3086\n",
      "Epoch 120/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3218 - val_loss: 0.3106\n",
      "Epoch 121/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3231 - val_loss: 0.3115\n",
      "Epoch 122/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3218 - val_loss: 0.3112\n",
      "Epoch 123/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3253 - val_loss: 0.3097\n",
      "Epoch 124/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3216 - val_loss: 0.3078\n",
      "Epoch 125/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3220 - val_loss: 0.3060\n",
      "Epoch 126/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3204 - val_loss: 0.3080\n",
      "Epoch 127/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3201 - val_loss: 0.3133\n",
      "Epoch 128/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3193 - val_loss: 0.3126\n",
      "Epoch 129/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3180 - val_loss: 0.3055\n",
      "Epoch 130/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3200 - val_loss: 0.3068\n",
      "Epoch 131/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3188 - val_loss: 0.3130\n",
      "Epoch 132/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3206 - val_loss: 0.3091\n",
      "Epoch 133/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3191 - val_loss: 0.3074\n",
      "Epoch 134/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3207 - val_loss: 0.3109\n",
      "Epoch 135/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3192 - val_loss: 0.3051\n",
      "Epoch 136/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3175 - val_loss: 0.3097\n",
      "Epoch 137/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3166 - val_loss: 0.3048\n",
      "Epoch 138/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3176 - val_loss: 0.3068\n",
      "Epoch 139/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3176 - val_loss: 0.3105\n",
      "Epoch 140/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3160 - val_loss: 0.3035\n",
      "Epoch 141/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3195 - val_loss: 0.3100\n",
      "Epoch 142/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3169 - val_loss: 0.3040\n",
      "Epoch 143/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3163 - val_loss: 0.3058\n",
      "Epoch 144/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3142 - val_loss: 0.3047\n",
      "Epoch 145/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3150 - val_loss: 0.3024\n",
      "Epoch 146/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3153 - val_loss: 0.3082\n",
      "Epoch 147/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3122 - val_loss: 0.3054\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3171 - val_loss: 0.3040\n",
      "Epoch 149/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3138 - val_loss: 0.3051\n",
      "Epoch 150/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3149 - val_loss: 0.3035\n",
      "Epoch 151/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3152 - val_loss: 0.3017\n",
      "Epoch 152/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3130 - val_loss: 0.3055\n",
      "Epoch 153/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3127 - val_loss: 0.3001\n",
      "Epoch 154/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3110 - val_loss: 0.3044\n",
      "Epoch 155/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3132 - val_loss: 0.3004\n",
      "Epoch 156/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3098 - val_loss: 0.3012\n",
      "Epoch 157/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3130 - val_loss: 0.3030\n",
      "Epoch 158/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3130 - val_loss: 0.3028\n",
      "Epoch 159/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3126 - val_loss: 0.3135\n",
      "Epoch 160/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3139 - val_loss: 0.2996\n",
      "Epoch 161/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3120 - val_loss: 0.3082\n",
      "Epoch 162/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3116 - val_loss: 0.3030\n",
      "Epoch 163/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3130 - val_loss: 0.2994\n",
      "Epoch 164/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3084 - val_loss: 0.2995\n",
      "Epoch 165/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3132 - val_loss: 0.3033\n",
      "Epoch 166/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3096 - val_loss: 0.3016\n",
      "Epoch 167/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3138 - val_loss: 0.2985\n",
      "Epoch 168/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3100 - val_loss: 0.2980\n",
      "Epoch 169/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3109 - val_loss: 0.3007\n",
      "Epoch 170/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3092 - val_loss: 0.2989\n",
      "Epoch 171/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3116 - val_loss: 0.3040\n",
      "Epoch 172/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3109 - val_loss: 0.2988\n",
      "Epoch 173/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3090 - val_loss: 0.3038\n",
      "Epoch 174/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3086 - val_loss: 0.3023\n",
      "Epoch 175/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3107 - val_loss: 0.2994\n",
      "Epoch 176/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3082 - val_loss: 0.3059\n",
      "Epoch 177/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3084 - val_loss: 0.2975\n",
      "Epoch 178/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3093 - val_loss: 0.2998\n",
      "Epoch 179/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3086 - val_loss: 0.3005\n",
      "Epoch 180/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3084 - val_loss: 0.2985\n",
      "Epoch 181/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3077 - val_loss: 0.2973\n",
      "Epoch 182/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3082 - val_loss: 0.3024\n",
      "Epoch 183/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3069 - val_loss: 0.3006\n",
      "Epoch 184/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3106 - val_loss: 0.2978\n",
      "Epoch 185/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3082 - val_loss: 0.3016\n",
      "Epoch 186/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3098 - val_loss: 0.3119\n",
      "Epoch 187/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3054 - val_loss: 0.2962\n",
      "Epoch 188/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3087 - val_loss: 0.3023\n",
      "Epoch 189/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3086 - val_loss: 0.2955\n",
      "Epoch 190/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3070 - val_loss: 0.2978\n",
      "Epoch 191/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3080 - val_loss: 0.2938\n",
      "Epoch 192/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3078 - val_loss: 0.2994\n",
      "Epoch 193/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3072 - val_loss: 0.2966\n",
      "Epoch 194/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3077 - val_loss: 0.2978\n",
      "Epoch 195/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3042 - val_loss: 0.2953\n",
      "Epoch 196/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3046 - val_loss: 0.2986\n",
      "Epoch 197/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3054 - val_loss: 0.2960\n",
      "Epoch 198/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3074 - val_loss: 0.2969\n",
      "Epoch 199/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3067 - val_loss: 0.2998\n",
      "Epoch 200/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3063 - val_loss: 0.2979\n",
      "Epoch 201/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3042 - val_loss: 0.2962\n",
      "Epoch 202/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3048 - val_loss: 0.2963\n",
      "Epoch 203/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3053 - val_loss: 0.2950\n",
      "Epoch 204/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3034 - val_loss: 0.3016\n",
      "Epoch 205/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3032 - val_loss: 0.3055\n",
      "Epoch 206/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3049 - val_loss: 0.3041\n",
      "Epoch 207/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3029 - val_loss: 0.3012\n",
      "Epoch 208/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3039 - val_loss: 0.2928\n",
      "Epoch 209/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3063 - val_loss: 0.3000\n",
      "Epoch 210/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3024 - val_loss: 0.2943\n",
      "Epoch 211/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3033 - val_loss: 0.2951\n",
      "Epoch 212/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3039 - val_loss: 0.2923\n",
      "Epoch 213/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3044 - val_loss: 0.2958\n",
      "Epoch 214/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3037 - val_loss: 0.2958\n",
      "Epoch 215/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3040 - val_loss: 0.3010\n",
      "Epoch 216/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3021 - val_loss: 0.2931\n",
      "Epoch 217/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3010 - val_loss: 0.2957\n",
      "Epoch 218/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3019 - val_loss: 0.2943\n",
      "Epoch 219/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3021 - val_loss: 0.2912\n",
      "Epoch 220/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3006 - val_loss: 0.2935\n",
      "Epoch 221/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3028 - val_loss: 0.2910\n",
      "Epoch 222/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3017 - val_loss: 0.2921\n",
      "Epoch 223/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3054 - val_loss: 0.2926\n",
      "Epoch 224/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3009 - val_loss: 0.2928\n",
      "Epoch 225/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3032 - val_loss: 0.2943\n",
      "Epoch 226/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3012 - val_loss: 0.2942\n",
      "Epoch 227/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2996 - val_loss: 0.3043\n",
      "Epoch 228/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3012 - val_loss: 0.2963\n",
      "Epoch 229/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3013 - val_loss: 0.2912\n",
      "Epoch 230/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3015 - val_loss: 0.2972\n",
      "Epoch 231/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3025 - val_loss: 0.2933\n",
      "Epoch 232/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3011 - val_loss: 0.2918\n",
      "Epoch 233/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.3027 - val_loss: 0.2898\n",
      "Epoch 234/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3000 - val_loss: 0.2920\n",
      "Epoch 235/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.3015 - val_loss: 0.2944\n",
      "Epoch 236/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3000 - val_loss: 0.2949\n",
      "Epoch 237/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3006 - val_loss: 0.2944\n",
      "Epoch 238/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3017 - val_loss: 0.2953\n",
      "Epoch 239/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3006 - val_loss: 0.2945\n",
      "Epoch 240/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3002 - val_loss: 0.2950\n",
      "Epoch 241/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2999 - val_loss: 0.2916\n",
      "Epoch 242/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3005 - val_loss: 0.2959\n",
      "Epoch 243/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3005 - val_loss: 0.2901\n",
      "Epoch 244/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3002 - val_loss: 0.2953\n",
      "Epoch 245/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2990 - val_loss: 0.2915\n",
      "Epoch 246/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3003 - val_loss: 0.2943\n",
      "Epoch 247/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3014 - val_loss: 0.2912\n",
      "Epoch 248/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3002 - val_loss: 0.2890\n",
      "Epoch 249/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3003 - val_loss: 0.2918\n",
      "Epoch 250/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2958 - val_loss: 0.2887\n",
      "Epoch 251/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2993 - val_loss: 0.2956\n",
      "Epoch 252/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2989 - val_loss: 0.2904\n",
      "Epoch 253/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3002 - val_loss: 0.2890\n",
      "Epoch 254/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3001 - val_loss: 0.2907\n",
      "Epoch 255/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3001 - val_loss: 0.2850\n",
      "Epoch 256/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2991 - val_loss: 0.2924\n",
      "Epoch 257/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2959 - val_loss: 0.2903\n",
      "Epoch 258/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3005 - val_loss: 0.2905\n",
      "Epoch 259/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.3000 - val_loss: 0.2905\n",
      "Epoch 260/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2991 - val_loss: 0.2880\n",
      "Epoch 261/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2993 - val_loss: 0.2889\n",
      "Epoch 262/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2981 - val_loss: 0.2902\n",
      "Epoch 263/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2964 - val_loss: 0.2902\n",
      "Epoch 264/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2959 - val_loss: 0.2886\n",
      "Epoch 265/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2969 - val_loss: 0.2901\n",
      "Epoch 266/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2985 - val_loss: 0.2909\n",
      "Epoch 267/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2973 - val_loss: 0.2895\n",
      "Epoch 268/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2967 - val_loss: 0.2905\n",
      "Epoch 269/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2967 - val_loss: 0.2903\n",
      "Epoch 270/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2971 - val_loss: 0.2951\n",
      "Epoch 271/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2962 - val_loss: 0.2885\n",
      "Epoch 272/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2956 - val_loss: 0.2900\n",
      "Epoch 273/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2957 - val_loss: 0.2862\n",
      "Epoch 274/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2975 - val_loss: 0.2874\n",
      "Epoch 275/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2969 - val_loss: 0.2872\n",
      "Epoch 276/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2955 - val_loss: 0.2967\n",
      "Epoch 277/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2952 - val_loss: 0.2861\n",
      "Epoch 278/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2963 - val_loss: 0.2872\n",
      "Epoch 279/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2952 - val_loss: 0.2884\n",
      "Epoch 280/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2959 - val_loss: 0.2883\n",
      "Epoch 281/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2971 - val_loss: 0.2894\n",
      "Epoch 282/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2938 - val_loss: 0.2881\n",
      "Epoch 283/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2962 - val_loss: 0.2885\n",
      "Epoch 284/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2955 - val_loss: 0.2883\n",
      "Epoch 285/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2951 - val_loss: 0.2887\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 286/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2862 - val_loss: 0.2750\n",
      "Epoch 287/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2816 - val_loss: 0.2751\n",
      "Epoch 288/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2837 - val_loss: 0.2772\n",
      "Epoch 289/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2815 - val_loss: 0.2767\n",
      "Epoch 290/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2811 - val_loss: 0.2749\n",
      "Epoch 291/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2813 - val_loss: 0.2754\n",
      "Epoch 292/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2811 - val_loss: 0.2757\n",
      "Epoch 293/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2847 - val_loss: 0.2743\n",
      "Epoch 294/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2802 - val_loss: 0.2722\n",
      "Epoch 295/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2832 - val_loss: 0.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2813 - val_loss: 0.2746\n",
      "Epoch 297/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2841 - val_loss: 0.2731\n",
      "Epoch 298/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2814 - val_loss: 0.2737\n",
      "Epoch 299/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2811 - val_loss: 0.2750\n",
      "Epoch 300/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2799 - val_loss: 0.2748\n",
      "Epoch 301/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2807 - val_loss: 0.2751\n",
      "Epoch 302/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2826 - val_loss: 0.2713\n",
      "Epoch 303/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2797 - val_loss: 0.2723\n",
      "Epoch 304/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2804 - val_loss: 0.2740\n",
      "Epoch 305/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2802 - val_loss: 0.2736\n",
      "Epoch 306/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2823 - val_loss: 0.2751\n",
      "Epoch 307/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2808 - val_loss: 0.2722\n",
      "Epoch 308/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2789 - val_loss: 0.2724\n",
      "Epoch 309/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2809 - val_loss: 0.2752\n",
      "Epoch 310/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2836 - val_loss: 0.2714\n",
      "Epoch 311/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2801 - val_loss: 0.2712\n",
      "Epoch 312/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2807 - val_loss: 0.2768\n",
      "Epoch 313/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2812 - val_loss: 0.2726\n",
      "Epoch 314/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2813 - val_loss: 0.2722\n",
      "Epoch 315/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2793 - val_loss: 0.2725\n",
      "Epoch 316/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2803 - val_loss: 0.2718\n",
      "Epoch 317/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2809 - val_loss: 0.2723\n",
      "Epoch 318/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2794 - val_loss: 0.2723\n",
      "Epoch 319/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2805 - val_loss: 0.2730\n",
      "Epoch 320/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2802 - val_loss: 0.2755\n",
      "Epoch 321/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2832 - val_loss: 0.2742\n",
      "Epoch 322/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2799 - val_loss: 0.2727\n",
      "Epoch 323/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2804 - val_loss: 0.2764\n",
      "Epoch 324/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2806 - val_loss: 0.2703\n",
      "Epoch 325/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2797 - val_loss: 0.2724\n",
      "Epoch 326/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2787 - val_loss: 0.2708\n",
      "Epoch 327/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2791 - val_loss: 0.2710\n",
      "Epoch 328/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2787 - val_loss: 0.2749\n",
      "Epoch 329/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2776 - val_loss: 0.2726\n",
      "Epoch 330/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2799 - val_loss: 0.2746\n",
      "Epoch 331/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2806 - val_loss: 0.2745\n",
      "Epoch 332/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2807 - val_loss: 0.2732\n",
      "Epoch 333/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2794 - val_loss: 0.2713\n",
      "Epoch 334/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2789 - val_loss: 0.2732\n",
      "Epoch 335/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2792 - val_loss: 0.2722\n",
      "Epoch 336/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2794 - val_loss: 0.2724\n",
      "Epoch 337/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2802 - val_loss: 0.2743\n",
      "Epoch 338/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2787 - val_loss: 0.2757\n",
      "Epoch 339/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2794 - val_loss: 0.2716\n",
      "Epoch 340/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2776 - val_loss: 0.2771\n",
      "Epoch 341/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2787 - val_loss: 0.2718\n",
      "Epoch 342/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2785 - val_loss: 0.2720\n",
      "Epoch 343/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2794 - val_loss: 0.2734\n",
      "Epoch 344/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2797 - val_loss: 0.2721\n",
      "Epoch 345/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2815 - val_loss: 0.2733\n",
      "Epoch 346/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2775 - val_loss: 0.2713\n",
      "Epoch 347/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2795 - val_loss: 0.2719\n",
      "Epoch 348/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2786 - val_loss: 0.2730\n",
      "Epoch 349/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2805 - val_loss: 0.2723\n",
      "Epoch 350/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2794 - val_loss: 0.2722\n",
      "Epoch 351/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2781 - val_loss: 0.2731\n",
      "Epoch 352/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2797 - val_loss: 0.2728\n",
      "Epoch 353/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2794 - val_loss: 0.2701\n",
      "Epoch 354/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2791 - val_loss: 0.2704\n",
      "Epoch 355/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2768 - val_loss: 0.2703\n",
      "Epoch 356/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2799 - val_loss: 0.2712\n",
      "Epoch 357/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2781 - val_loss: 0.2711\n",
      "Epoch 358/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2826 - val_loss: 0.2690\n",
      "Epoch 359/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2767 - val_loss: 0.2756\n",
      "Epoch 360/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2770 - val_loss: 0.2720\n",
      "Epoch 361/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2807 - val_loss: 0.2714\n",
      "Epoch 362/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2770 - val_loss: 0.2708\n",
      "Epoch 363/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2771 - val_loss: 0.2728\n",
      "Epoch 364/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2781 - val_loss: 0.2701\n",
      "Epoch 365/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2782 - val_loss: 0.2751\n",
      "Epoch 366/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2807 - val_loss: 0.2710\n",
      "Epoch 367/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2790 - val_loss: 0.2707\n",
      "Epoch 368/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2773 - val_loss: 0.2733\n",
      "Epoch 369/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2784 - val_loss: 0.2702\n",
      "Epoch 370/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2780 - val_loss: 0.2712\n",
      "Epoch 371/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2775 - val_loss: 0.2719\n",
      "Epoch 372/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2787 - val_loss: 0.2691\n",
      "Epoch 373/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2765 - val_loss: 0.2711\n",
      "Epoch 374/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2786 - val_loss: 0.2726\n",
      "Epoch 375/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2763 - val_loss: 0.2699\n",
      "Epoch 376/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2790 - val_loss: 0.2721\n",
      "Epoch 377/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2796 - val_loss: 0.2695\n",
      "Epoch 378/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2788 - val_loss: 0.2694\n",
      "Epoch 379/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2769 - val_loss: 0.2720\n",
      "Epoch 380/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2754 - val_loss: 0.2698\n",
      "Epoch 381/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2795 - val_loss: 0.2693\n",
      "Epoch 382/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2763 - val_loss: 0.2707\n",
      "Epoch 383/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2785 - val_loss: 0.2707\n",
      "Epoch 384/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2758 - val_loss: 0.2700\n",
      "Epoch 385/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2771 - val_loss: 0.2704\n",
      "Epoch 386/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2785 - val_loss: 0.2701\n",
      "Epoch 387/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2770 - val_loss: 0.2711\n",
      "Epoch 388/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2770 - val_loss: 0.2726\n",
      "\n",
      "Epoch 00388: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 389/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2718 - val_loss: 0.2667\n",
      "Epoch 390/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2735 - val_loss: 0.2649\n",
      "Epoch 391/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2719 - val_loss: 0.2647\n",
      "Epoch 392/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2714 - val_loss: 0.2635\n",
      "Epoch 393/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2702 - val_loss: 0.2641\n",
      "Epoch 394/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2697 - val_loss: 0.2645\n",
      "Epoch 395/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2709 - val_loss: 0.2641\n",
      "Epoch 396/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2703 - val_loss: 0.2647\n",
      "Epoch 397/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2710 - val_loss: 0.2640\n",
      "Epoch 398/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2721 - val_loss: 0.2649\n",
      "Epoch 399/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2727 - val_loss: 0.2634\n",
      "Epoch 400/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2703 - val_loss: 0.2654\n",
      "Epoch 401/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2702 - val_loss: 0.2639\n",
      "Epoch 402/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2693 - val_loss: 0.2639\n",
      "Epoch 403/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2698 - val_loss: 0.2654\n",
      "Epoch 404/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2686 - val_loss: 0.2652\n",
      "Epoch 405/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2703 - val_loss: 0.2655\n",
      "Epoch 406/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2721 - val_loss: 0.2640\n",
      "Epoch 407/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2724 - val_loss: 0.2646\n",
      "Epoch 408/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2708 - val_loss: 0.2632\n",
      "Epoch 409/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2704 - val_loss: 0.2642\n",
      "Epoch 410/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2712 - val_loss: 0.2634\n",
      "Epoch 411/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2696 - val_loss: 0.2636\n",
      "Epoch 412/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2695 - val_loss: 0.2645\n",
      "Epoch 413/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2700 - val_loss: 0.2632\n",
      "Epoch 414/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2709 - val_loss: 0.2631\n",
      "Epoch 415/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2678 - val_loss: 0.2637\n",
      "Epoch 416/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2722 - val_loss: 0.2632\n",
      "Epoch 417/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2719 - val_loss: 0.2635\n",
      "Epoch 418/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2709 - val_loss: 0.2639\n",
      "Epoch 419/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2630\n",
      "Epoch 420/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2708 - val_loss: 0.2646\n",
      "Epoch 421/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2697 - val_loss: 0.2623\n",
      "Epoch 422/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2709 - val_loss: 0.2629\n",
      "Epoch 423/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2696 - val_loss: 0.2635\n",
      "Epoch 424/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2682 - val_loss: 0.2629\n",
      "Epoch 425/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2703 - val_loss: 0.2631\n",
      "Epoch 426/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2715 - val_loss: 0.2638\n",
      "Epoch 427/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2674 - val_loss: 0.2635\n",
      "Epoch 428/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2699 - val_loss: 0.2628\n",
      "Epoch 429/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2675 - val_loss: 0.2631\n",
      "Epoch 430/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2690 - val_loss: 0.2630\n",
      "Epoch 431/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2706 - val_loss: 0.2628\n",
      "Epoch 432/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2687 - val_loss: 0.2623\n",
      "Epoch 433/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2699 - val_loss: 0.2625\n",
      "Epoch 434/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2685 - val_loss: 0.2622\n",
      "Epoch 435/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2703 - val_loss: 0.2630\n",
      "Epoch 436/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2679 - val_loss: 0.2628\n",
      "Epoch 437/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2714 - val_loss: 0.2631\n",
      "Epoch 438/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2676 - val_loss: 0.2625\n",
      "Epoch 439/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2687 - val_loss: 0.2621\n",
      "Epoch 440/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2695 - val_loss: 0.2628\n",
      "Epoch 441/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2693 - val_loss: 0.2624\n",
      "Epoch 442/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2715 - val_loss: 0.2636\n",
      "Epoch 443/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2685 - val_loss: 0.2627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2634\n",
      "Epoch 445/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2709 - val_loss: 0.2627\n",
      "Epoch 446/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2631\n",
      "Epoch 447/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2705 - val_loss: 0.2628\n",
      "Epoch 448/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2698 - val_loss: 0.2629\n",
      "Epoch 449/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2723 - val_loss: 0.2628\n",
      "Epoch 450/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2691 - val_loss: 0.2641\n",
      "Epoch 451/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2687 - val_loss: 0.2628\n",
      "Epoch 452/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2679 - val_loss: 0.2638\n",
      "Epoch 453/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2679 - val_loss: 0.2626\n",
      "Epoch 454/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2685 - val_loss: 0.2647\n",
      "Epoch 455/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2697 - val_loss: 0.2646\n",
      "Epoch 456/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2634\n",
      "Epoch 457/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2683 - val_loss: 0.2622\n",
      "Epoch 458/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2708 - val_loss: 0.2642\n",
      "Epoch 459/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2689 - val_loss: 0.2622\n",
      "Epoch 460/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2685 - val_loss: 0.2615\n",
      "Epoch 461/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2690 - val_loss: 0.2631\n",
      "Epoch 462/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2681 - val_loss: 0.2618\n",
      "Epoch 463/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2687 - val_loss: 0.2621\n",
      "Epoch 464/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2694 - val_loss: 0.2622\n",
      "Epoch 465/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2698 - val_loss: 0.2635\n",
      "Epoch 466/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2706 - val_loss: 0.2617\n",
      "Epoch 467/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2677 - val_loss: 0.2616\n",
      "Epoch 468/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2703 - val_loss: 0.2618\n",
      "Epoch 469/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2695 - val_loss: 0.2620\n",
      "Epoch 470/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2710 - val_loss: 0.2627\n",
      "Epoch 471/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2664 - val_loss: 0.2622\n",
      "Epoch 472/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2690 - val_loss: 0.2623\n",
      "Epoch 473/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2699 - val_loss: 0.2628\n",
      "Epoch 474/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2693 - val_loss: 0.2631\n",
      "Epoch 475/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2683 - val_loss: 0.2628\n",
      "Epoch 476/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2693 - val_loss: 0.2631\n",
      "Epoch 477/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2696 - val_loss: 0.2635\n",
      "Epoch 478/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2680 - val_loss: 0.2618\n",
      "Epoch 479/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2689 - val_loss: 0.2620\n",
      "Epoch 480/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2681 - val_loss: 0.2642\n",
      "Epoch 481/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2680 - val_loss: 0.2620\n",
      "Epoch 482/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2711 - val_loss: 0.2631\n",
      "Epoch 483/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2680 - val_loss: 0.2633\n",
      "Epoch 484/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2673 - val_loss: 0.2611\n",
      "Epoch 485/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2685 - val_loss: 0.2631\n",
      "Epoch 486/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2622\n",
      "Epoch 487/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2691 - val_loss: 0.2624\n",
      "Epoch 488/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2626\n",
      "Epoch 489/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2669 - val_loss: 0.2629\n",
      "Epoch 490/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2690 - val_loss: 0.2620\n",
      "Epoch 491/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2691 - val_loss: 0.2629\n",
      "Epoch 492/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2675 - val_loss: 0.2621\n",
      "Epoch 493/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2670 - val_loss: 0.2620\n",
      "Epoch 494/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2694 - val_loss: 0.2642\n",
      "Epoch 495/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2665 - val_loss: 0.2626\n",
      "Epoch 496/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2682 - val_loss: 0.2617\n",
      "Epoch 497/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2680 - val_loss: 0.2624\n",
      "Epoch 498/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2701 - val_loss: 0.2614\n",
      "Epoch 499/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2686 - val_loss: 0.2635\n",
      "Epoch 500/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2669 - val_loss: 0.2626\n",
      "Epoch 501/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2684 - val_loss: 0.2617\n",
      "Epoch 502/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2697 - val_loss: 0.2632\n",
      "Epoch 503/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2678 - val_loss: 0.2629\n",
      "Epoch 504/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2698 - val_loss: 0.2609\n",
      "Epoch 505/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2683 - val_loss: 0.2614\n",
      "Epoch 506/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2687 - val_loss: 0.2624\n",
      "Epoch 507/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2675 - val_loss: 0.2633\n",
      "Epoch 508/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2714 - val_loss: 0.2619\n",
      "Epoch 509/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2675 - val_loss: 0.2615\n",
      "Epoch 510/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2684 - val_loss: 0.2625\n",
      "Epoch 511/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2670 - val_loss: 0.2623\n",
      "Epoch 512/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2667 - val_loss: 0.2604\n",
      "Epoch 513/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2683 - val_loss: 0.2656\n",
      "Epoch 514/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2690 - val_loss: 0.2609\n",
      "Epoch 515/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2702 - val_loss: 0.2615\n",
      "Epoch 516/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2681 - val_loss: 0.2622\n",
      "Epoch 517/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2695 - val_loss: 0.2619\n",
      "Epoch 518/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2716 - val_loss: 0.2615\n",
      "Epoch 519/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2692 - val_loss: 0.2628\n",
      "Epoch 520/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2673 - val_loss: 0.2611\n",
      "Epoch 521/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2680 - val_loss: 0.2620\n",
      "Epoch 522/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2658 - val_loss: 0.2621\n",
      "Epoch 523/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2683 - val_loss: 0.2626\n",
      "Epoch 524/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2678 - val_loss: 0.2613\n",
      "Epoch 525/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2683 - val_loss: 0.2614\n",
      "Epoch 526/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2680 - val_loss: 0.2615\n",
      "Epoch 527/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2684 - val_loss: 0.2618\n",
      "Epoch 528/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2673 - val_loss: 0.2615\n",
      "Epoch 529/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2696 - val_loss: 0.2610\n",
      "Epoch 530/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2704 - val_loss: 0.2629\n",
      "Epoch 531/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2677 - val_loss: 0.2607\n",
      "Epoch 532/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2666 - val_loss: 0.2609\n",
      "Epoch 533/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2676 - val_loss: 0.2631\n",
      "Epoch 534/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2659 - val_loss: 0.2621\n",
      "Epoch 535/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2688 - val_loss: 0.2615\n",
      "Epoch 536/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2691 - val_loss: 0.2617\n",
      "Epoch 537/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2682 - val_loss: 0.2609\n",
      "Epoch 538/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2700 - val_loss: 0.2615\n",
      "Epoch 539/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2680 - val_loss: 0.2609\n",
      "Epoch 540/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2661 - val_loss: 0.2610\n",
      "Epoch 541/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2667 - val_loss: 0.2627\n",
      "Epoch 542/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2669 - val_loss: 0.2620\n",
      "\n",
      "Epoch 00542: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 543/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2649 - val_loss: 0.2585\n",
      "Epoch 544/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2657 - val_loss: 0.2580\n",
      "Epoch 545/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2656 - val_loss: 0.2585\n",
      "Epoch 546/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2667 - val_loss: 0.2586\n",
      "Epoch 547/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2641 - val_loss: 0.2588\n",
      "Epoch 548/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2648 - val_loss: 0.2588\n",
      "Epoch 549/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2642 - val_loss: 0.2582\n",
      "Epoch 550/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2665 - val_loss: 0.2584\n",
      "Epoch 551/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2652 - val_loss: 0.2578\n",
      "Epoch 552/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2657 - val_loss: 0.2585\n",
      "Epoch 553/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2657 - val_loss: 0.2577\n",
      "Epoch 554/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2653 - val_loss: 0.2578\n",
      "Epoch 555/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2660 - val_loss: 0.2581\n",
      "Epoch 556/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2648 - val_loss: 0.2583\n",
      "Epoch 557/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2659 - val_loss: 0.2582\n",
      "Epoch 558/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2645 - val_loss: 0.2588\n",
      "Epoch 559/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2639 - val_loss: 0.2581\n",
      "Epoch 560/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2634 - val_loss: 0.2581\n",
      "Epoch 561/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2661 - val_loss: 0.2590\n",
      "Epoch 562/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2641 - val_loss: 0.2582\n",
      "Epoch 563/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2640 - val_loss: 0.2579\n",
      "Epoch 564/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2630 - val_loss: 0.2578\n",
      "Epoch 565/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2665 - val_loss: 0.2576\n",
      "Epoch 566/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2658 - val_loss: 0.2580\n",
      "Epoch 567/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2626 - val_loss: 0.2578\n",
      "Epoch 568/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2674 - val_loss: 0.2578\n",
      "Epoch 569/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2633 - val_loss: 0.2585\n",
      "Epoch 570/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2654 - val_loss: 0.2573\n",
      "Epoch 571/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2646 - val_loss: 0.2575\n",
      "Epoch 572/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2639 - val_loss: 0.2583\n",
      "Epoch 573/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2622 - val_loss: 0.2576\n",
      "Epoch 574/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2632 - val_loss: 0.2574\n",
      "Epoch 575/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2650 - val_loss: 0.2579\n",
      "Epoch 576/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2675 - val_loss: 0.2579\n",
      "Epoch 577/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2635 - val_loss: 0.2575\n",
      "Epoch 578/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2628 - val_loss: 0.2580\n",
      "Epoch 579/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2668 - val_loss: 0.2579\n",
      "Epoch 580/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2624 - val_loss: 0.2580\n",
      "Epoch 581/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2629 - val_loss: 0.2580\n",
      "Epoch 582/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2634 - val_loss: 0.2574\n",
      "Epoch 583/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2639 - val_loss: 0.2575\n",
      "Epoch 584/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2647 - val_loss: 0.2579\n",
      "Epoch 585/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2639 - val_loss: 0.2581\n",
      "Epoch 586/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2644 - val_loss: 0.2576\n",
      "Epoch 587/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2634 - val_loss: 0.2572\n",
      "Epoch 588/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2654 - val_loss: 0.2574\n",
      "Epoch 589/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2632 - val_loss: 0.2576\n",
      "Epoch 590/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2578\n",
      "Epoch 591/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2642 - val_loss: 0.2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2663 - val_loss: 0.2575\n",
      "Epoch 593/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2633 - val_loss: 0.2570\n",
      "Epoch 594/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2657 - val_loss: 0.2576\n",
      "Epoch 595/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2575\n",
      "Epoch 596/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2628 - val_loss: 0.2576\n",
      "Epoch 597/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2637 - val_loss: 0.2570\n",
      "Epoch 598/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2579\n",
      "Epoch 599/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2645 - val_loss: 0.2570\n",
      "Epoch 600/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2622 - val_loss: 0.2580\n",
      "Epoch 601/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2649 - val_loss: 0.2574\n",
      "Epoch 602/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2649 - val_loss: 0.2575\n",
      "Epoch 603/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2664 - val_loss: 0.2573\n",
      "Epoch 604/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2628 - val_loss: 0.2574\n",
      "Epoch 605/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2634 - val_loss: 0.2575\n",
      "Epoch 606/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2640 - val_loss: 0.2573\n",
      "Epoch 607/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2624 - val_loss: 0.2579\n",
      "Epoch 608/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2641 - val_loss: 0.2572\n",
      "Epoch 609/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2655 - val_loss: 0.2577\n",
      "Epoch 610/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2573\n",
      "Epoch 611/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2641 - val_loss: 0.2572\n",
      "Epoch 612/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2656 - val_loss: 0.2575\n",
      "Epoch 613/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2632 - val_loss: 0.2574\n",
      "Epoch 614/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2662 - val_loss: 0.2575\n",
      "Epoch 615/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2628 - val_loss: 0.2572\n",
      "Epoch 616/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2637 - val_loss: 0.2574\n",
      "Epoch 617/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2633 - val_loss: 0.2579\n",
      "Epoch 618/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2636 - val_loss: 0.2571\n",
      "Epoch 619/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2679 - val_loss: 0.2571\n",
      "Epoch 620/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2655 - val_loss: 0.2574\n",
      "Epoch 621/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2627 - val_loss: 0.2569\n",
      "Epoch 622/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2636 - val_loss: 0.2569\n",
      "Epoch 623/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2656 - val_loss: 0.2577\n",
      "Epoch 624/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2629 - val_loss: 0.2577\n",
      "Epoch 625/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2577\n",
      "Epoch 626/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2652 - val_loss: 0.2566\n",
      "Epoch 627/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2636 - val_loss: 0.2576\n",
      "Epoch 628/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2644 - val_loss: 0.2574\n",
      "Epoch 629/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2659 - val_loss: 0.2574\n",
      "Epoch 630/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2641 - val_loss: 0.2575\n",
      "Epoch 631/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2627 - val_loss: 0.2571\n",
      "Epoch 632/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2646 - val_loss: 0.2583\n",
      "Epoch 633/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2616 - val_loss: 0.2574\n",
      "Epoch 634/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2652 - val_loss: 0.2582\n",
      "Epoch 635/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2629 - val_loss: 0.2570\n",
      "Epoch 636/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2637 - val_loss: 0.2570\n",
      "Epoch 637/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2626 - val_loss: 0.2575\n",
      "Epoch 638/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2634 - val_loss: 0.2575\n",
      "Epoch 639/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2648 - val_loss: 0.2578\n",
      "Epoch 640/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2575\n",
      "Epoch 641/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2665 - val_loss: 0.2582\n",
      "Epoch 642/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2572\n",
      "Epoch 643/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2654 - val_loss: 0.2575\n",
      "Epoch 644/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2639 - val_loss: 0.2573\n",
      "Epoch 645/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2657 - val_loss: 0.2573\n",
      "Epoch 646/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2625 - val_loss: 0.2568\n",
      "Epoch 647/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2621 - val_loss: 0.2571\n",
      "Epoch 648/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2565\n",
      "Epoch 649/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2640 - val_loss: 0.2575\n",
      "Epoch 650/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2658 - val_loss: 0.2571\n",
      "Epoch 651/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2634 - val_loss: 0.2571\n",
      "Epoch 652/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2573\n",
      "Epoch 653/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2638 - val_loss: 0.2568\n",
      "Epoch 654/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2642 - val_loss: 0.2576\n",
      "Epoch 655/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2630 - val_loss: 0.2569\n",
      "Epoch 656/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2575\n",
      "Epoch 657/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2653 - val_loss: 0.2570\n",
      "Epoch 658/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2572\n",
      "Epoch 659/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2617 - val_loss: 0.2577\n",
      "Epoch 660/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2652 - val_loss: 0.2571\n",
      "Epoch 661/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2623 - val_loss: 0.2569\n",
      "Epoch 662/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2639 - val_loss: 0.2567\n",
      "Epoch 663/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2631 - val_loss: 0.2568\n",
      "Epoch 664/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2641 - val_loss: 0.2574\n",
      "Epoch 665/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2625 - val_loss: 0.2574\n",
      "Epoch 666/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2631 - val_loss: 0.2574\n",
      "Epoch 667/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2620 - val_loss: 0.2566\n",
      "Epoch 668/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2626 - val_loss: 0.2570\n",
      "Epoch 669/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2640 - val_loss: 0.2574\n",
      "Epoch 670/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2657 - val_loss: 0.2565\n",
      "Epoch 671/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2628 - val_loss: 0.2570\n",
      "Epoch 672/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2617 - val_loss: 0.2574\n",
      "Epoch 673/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2651 - val_loss: 0.2573\n",
      "Epoch 674/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2647 - val_loss: 0.2576\n",
      "Epoch 675/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2642 - val_loss: 0.2572\n",
      "Epoch 676/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2622 - val_loss: 0.2575\n",
      "Epoch 677/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2641 - val_loss: 0.2569\n",
      "Epoch 678/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2614 - val_loss: 0.2567\n",
      "\n",
      "Epoch 00678: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 679/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2613 - val_loss: 0.2560\n",
      "Epoch 680/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2621 - val_loss: 0.2554\n",
      "Epoch 681/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2620 - val_loss: 0.2557\n",
      "Epoch 682/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2640 - val_loss: 0.2557\n",
      "Epoch 683/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2557\n",
      "Epoch 684/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2609 - val_loss: 0.2555\n",
      "Epoch 685/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2556\n",
      "Epoch 686/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2632 - val_loss: 0.2558\n",
      "Epoch 687/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2619 - val_loss: 0.2555\n",
      "Epoch 688/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2618 - val_loss: 0.2558\n",
      "Epoch 689/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2553\n",
      "Epoch 690/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2631 - val_loss: 0.2554\n",
      "Epoch 691/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2553\n",
      "Epoch 692/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2623 - val_loss: 0.2559\n",
      "Epoch 693/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2610 - val_loss: 0.2560\n",
      "Epoch 694/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2619 - val_loss: 0.2558\n",
      "Epoch 695/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2594 - val_loss: 0.2559\n",
      "Epoch 696/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2600 - val_loss: 0.2556\n",
      "Epoch 697/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2636 - val_loss: 0.2551\n",
      "Epoch 698/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2553\n",
      "Epoch 699/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2588 - val_loss: 0.2553\n",
      "Epoch 700/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2555\n",
      "Epoch 701/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2614 - val_loss: 0.2558\n",
      "Epoch 702/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2640 - val_loss: 0.2549\n",
      "Epoch 703/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2607 - val_loss: 0.2553\n",
      "Epoch 704/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2615 - val_loss: 0.2551\n",
      "Epoch 705/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2617 - val_loss: 0.2552\n",
      "Epoch 706/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2590 - val_loss: 0.2552\n",
      "Epoch 707/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2613 - val_loss: 0.2557\n",
      "Epoch 708/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2615 - val_loss: 0.2556\n",
      "Epoch 709/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2616 - val_loss: 0.2553\n",
      "Epoch 710/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2622 - val_loss: 0.2553\n",
      "Epoch 711/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2621 - val_loss: 0.2552\n",
      "Epoch 712/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2604 - val_loss: 0.2553\n",
      "Epoch 713/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2619 - val_loss: 0.2554\n",
      "Epoch 714/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2621 - val_loss: 0.2555\n",
      "Epoch 715/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2606 - val_loss: 0.2551\n",
      "Epoch 716/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2601 - val_loss: 0.2550\n",
      "Epoch 717/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2608 - val_loss: 0.2551\n",
      "Epoch 718/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2591 - val_loss: 0.2553\n",
      "Epoch 719/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2613 - val_loss: 0.2553\n",
      "Epoch 720/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2622 - val_loss: 0.2552\n",
      "Epoch 721/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2610 - val_loss: 0.2552\n",
      "Epoch 722/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2555\n",
      "Epoch 723/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2551\n",
      "Epoch 724/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2627 - val_loss: 0.2552\n",
      "Epoch 725/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2594 - val_loss: 0.2551\n",
      "Epoch 726/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2552\n",
      "Epoch 727/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2550\n",
      "Epoch 728/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2550\n",
      "Epoch 729/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2592 - val_loss: 0.2550\n",
      "Epoch 730/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2615 - val_loss: 0.2550\n",
      "Epoch 731/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2643 - val_loss: 0.2549\n",
      "Epoch 732/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2552\n",
      "\n",
      "Epoch 00732: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 733/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2614 - val_loss: 0.2546\n",
      "Epoch 734/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2592 - val_loss: 0.2546\n",
      "Epoch 735/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2607 - val_loss: 0.2546\n",
      "Epoch 736/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2586 - val_loss: 0.2548\n",
      "Epoch 737/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2607 - val_loss: 0.2545\n",
      "Epoch 738/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2548\n",
      "Epoch 739/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2594 - val_loss: 0.2545\n",
      "Epoch 740/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2544\n",
      "Epoch 741/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2610 - val_loss: 0.2543\n",
      "Epoch 742/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2609 - val_loss: 0.2545\n",
      "Epoch 743/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2547\n",
      "Epoch 744/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2610 - val_loss: 0.2546\n",
      "Epoch 745/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2596 - val_loss: 0.2546\n",
      "Epoch 746/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2613 - val_loss: 0.2543\n",
      "Epoch 747/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2546\n",
      "Epoch 748/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2587 - val_loss: 0.2545\n",
      "Epoch 749/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2598 - val_loss: 0.2546\n",
      "Epoch 750/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2586 - val_loss: 0.2544\n",
      "Epoch 751/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2595 - val_loss: 0.2542\n",
      "Epoch 752/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2541\n",
      "Epoch 753/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2624 - val_loss: 0.2545\n",
      "Epoch 754/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2587 - val_loss: 0.2546\n",
      "Epoch 755/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2601 - val_loss: 0.2546\n",
      "Epoch 756/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2615 - val_loss: 0.2544\n",
      "Epoch 757/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2574 - val_loss: 0.2543\n",
      "Epoch 758/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2545\n",
      "Epoch 759/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2623 - val_loss: 0.2544\n",
      "Epoch 760/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2628 - val_loss: 0.2545\n",
      "Epoch 761/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2572 - val_loss: 0.2544\n",
      "Epoch 762/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2544\n",
      "Epoch 763/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2609 - val_loss: 0.2543\n",
      "Epoch 764/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2615 - val_loss: 0.2543\n",
      "Epoch 765/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2587 - val_loss: 0.2544\n",
      "Epoch 766/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2612 - val_loss: 0.2547\n",
      "Epoch 767/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2581 - val_loss: 0.2544\n",
      "Epoch 768/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2601 - val_loss: 0.2548\n",
      "Epoch 769/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2596 - val_loss: 0.2541\n",
      "Epoch 770/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2547\n",
      "Epoch 771/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2542\n",
      "Epoch 772/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2607 - val_loss: 0.2542\n",
      "Epoch 773/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2607 - val_loss: 0.2545\n",
      "Epoch 774/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2592 - val_loss: 0.2545\n",
      "Epoch 775/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2544\n",
      "Epoch 776/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2614 - val_loss: 0.2544\n",
      "Epoch 777/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2611 - val_loss: 0.2546\n",
      "Epoch 778/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2585 - val_loss: 0.2545\n",
      "Epoch 779/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2606 - val_loss: 0.2546\n",
      "Epoch 780/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2604 - val_loss: 0.2543\n",
      "Epoch 781/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2543\n",
      "\n",
      "Epoch 00781: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 782/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2578 - val_loss: 0.2543\n",
      "Epoch 783/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2543\n",
      "Epoch 784/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2601 - val_loss: 0.2543\n",
      "Epoch 785/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2542\n",
      "Epoch 786/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2609 - val_loss: 0.2540\n",
      "Epoch 787/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2606 - val_loss: 0.2542\n",
      "Epoch 788/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2603 - val_loss: 0.2540\n",
      "Epoch 789/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2605 - val_loss: 0.2540\n",
      "Epoch 790/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2541\n",
      "Epoch 791/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2598 - val_loss: 0.2541\n",
      "Epoch 792/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2595 - val_loss: 0.2542\n",
      "Epoch 793/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2542\n",
      "Epoch 794/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2606 - val_loss: 0.2543\n",
      "Epoch 795/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2591 - val_loss: 0.2542\n",
      "Epoch 796/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2590 - val_loss: 0.2540\n",
      "Epoch 797/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2542\n",
      "Epoch 798/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2541\n",
      "Epoch 799/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2584 - val_loss: 0.2542\n",
      "Epoch 800/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2613 - val_loss: 0.2542\n",
      "Epoch 801/2000\n",
      "911004/911004 [==============================] - 8s 9us/step - loss: 0.2616 - val_loss: 0.2539\n",
      "Epoch 802/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2589 - val_loss: 0.2540\n",
      "Epoch 803/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2589 - val_loss: 0.2540\n",
      "Epoch 804/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2613 - val_loss: 0.2540\n",
      "Epoch 805/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2591 - val_loss: 0.2542\n",
      "Epoch 806/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2542\n",
      "Epoch 807/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2619 - val_loss: 0.2542\n",
      "Epoch 808/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2540\n",
      "Epoch 809/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2582 - val_loss: 0.2542\n",
      "Epoch 810/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2588 - val_loss: 0.2540\n",
      "Epoch 811/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2593 - val_loss: 0.2545\n",
      "Epoch 812/2000\n",
      "911004/911004 [==============================] - 9s 10us/step - loss: 0.2580 - val_loss: 0.2541\n",
      "Epoch 813/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2591 - val_loss: 0.2540\n",
      "Epoch 814/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2595 - val_loss: 0.2540\n",
      "Epoch 815/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2639 - val_loss: 0.2540\n",
      "Epoch 816/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2588 - val_loss: 0.2541\n",
      "\n",
      "Epoch 00816: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 817/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2627 - val_loss: 0.2540\n",
      "Epoch 818/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2600 - val_loss: 0.2540\n",
      "Epoch 819/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2598 - val_loss: 0.2538\n",
      "Epoch 820/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2540\n",
      "Epoch 821/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2593 - val_loss: 0.2538\n",
      "Epoch 822/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2571 - val_loss: 0.2539\n",
      "Epoch 823/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2604 - val_loss: 0.2538\n",
      "Epoch 824/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2598 - val_loss: 0.2540\n",
      "Epoch 825/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2541\n",
      "Epoch 826/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2620 - val_loss: 0.2538\n",
      "Epoch 827/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2612 - val_loss: 0.2538\n",
      "Epoch 828/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2601 - val_loss: 0.2538\n",
      "Epoch 829/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2609 - val_loss: 0.2539\n",
      "Epoch 830/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2538\n",
      "Epoch 831/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2596 - val_loss: 0.2540\n",
      "Epoch 832/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2611 - val_loss: 0.2541\n",
      "Epoch 833/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2595 - val_loss: 0.2540\n",
      "Epoch 834/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2604 - val_loss: 0.2540\n",
      "Epoch 835/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2621 - val_loss: 0.2538\n",
      "Epoch 836/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2590 - val_loss: 0.2538\n",
      "Epoch 837/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2590 - val_loss: 0.2539\n",
      "Epoch 838/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2580 - val_loss: 0.2540\n",
      "Epoch 839/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2584 - val_loss: 0.2541\n",
      "Epoch 840/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2573 - val_loss: 0.2541\n",
      "Epoch 841/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2592 - val_loss: 0.2539\n",
      "Epoch 842/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2613 - val_loss: 0.2539\n",
      "Epoch 843/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2593 - val_loss: 0.2539\n",
      "Epoch 844/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2540\n",
      "Epoch 845/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2598 - val_loss: 0.2538\n",
      "Epoch 846/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2595 - val_loss: 0.2540\n",
      "Epoch 847/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2580 - val_loss: 0.2542\n",
      "Epoch 848/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2566 - val_loss: 0.2538\n",
      "Epoch 849/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2540\n",
      "\n",
      "Epoch 00849: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 850/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2583 - val_loss: 0.2541\n",
      "Epoch 851/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2619 - val_loss: 0.2539\n",
      "Epoch 852/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2593 - val_loss: 0.2539\n",
      "Epoch 853/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2597 - val_loss: 0.2539\n",
      "Epoch 854/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2595 - val_loss: 0.2538\n",
      "Epoch 855/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2605 - val_loss: 0.2538\n",
      "Epoch 856/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2594 - val_loss: 0.2538\n",
      "Epoch 857/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2585 - val_loss: 0.2540\n",
      "Epoch 858/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2586 - val_loss: 0.2538\n",
      "Epoch 859/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2615 - val_loss: 0.2539\n",
      "Epoch 860/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2608 - val_loss: 0.2538\n",
      "Epoch 861/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2580 - val_loss: 0.2538\n",
      "Epoch 862/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2582 - val_loss: 0.2540\n",
      "Epoch 863/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2593 - val_loss: 0.2537\n",
      "Epoch 864/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2602 - val_loss: 0.2541\n",
      "Epoch 865/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2576 - val_loss: 0.2538\n",
      "Epoch 866/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2538\n",
      "Epoch 867/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2603 - val_loss: 0.2538\n",
      "Epoch 868/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2604 - val_loss: 0.2538\n",
      "Epoch 869/2000\n",
      "911004/911004 [==============================] - 9s 9us/step - loss: 0.2585 - val_loss: 0.2538\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00869: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.371110119090838\n",
      "Training 3JHH out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 472508 samples, validate on 118103 samples\n",
      "Epoch 1/2000\n",
      "472508/472508 [==============================] - 5s 11us/step - loss: 1.2117 - val_loss: 0.5720\n",
      "Epoch 2/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.4758 - val_loss: 0.4682\n",
      "Epoch 3/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.4294 - val_loss: 0.3844\n",
      "Epoch 4/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.3979 - val_loss: 0.4094\n",
      "Epoch 5/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3843 - val_loss: 0.3678\n",
      "Epoch 6/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3662 - val_loss: 0.3472\n",
      "Epoch 7/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3526 - val_loss: 0.3358\n",
      "Epoch 8/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.3521 - val_loss: 0.3827\n",
      "Epoch 9/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3376 - val_loss: 0.3237\n",
      "Epoch 10/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3365 - val_loss: 0.2989\n",
      "Epoch 11/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3270 - val_loss: 0.3109\n",
      "Epoch 12/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3190 - val_loss: 0.2905\n",
      "Epoch 13/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3162 - val_loss: 0.2978\n",
      "Epoch 14/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3125 - val_loss: 0.2862\n",
      "Epoch 15/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3110 - val_loss: 0.2846\n",
      "Epoch 16/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3035 - val_loss: 0.2891\n",
      "Epoch 17/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.3026 - val_loss: 0.2841\n",
      "Epoch 18/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2994 - val_loss: 0.2943\n",
      "Epoch 19/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2933 - val_loss: 0.2819\n",
      "Epoch 20/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2930 - val_loss: 0.2819\n",
      "Epoch 21/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2921 - val_loss: 0.2763\n",
      "Epoch 22/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2878 - val_loss: 0.2769\n",
      "Epoch 23/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2813 - val_loss: 0.2660\n",
      "Epoch 24/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2823 - val_loss: 0.2593\n",
      "Epoch 25/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2807 - val_loss: 0.2591\n",
      "Epoch 26/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2766 - val_loss: 0.3164\n",
      "Epoch 27/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2784 - val_loss: 0.2627\n",
      "Epoch 28/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2754 - val_loss: 0.2613\n",
      "Epoch 29/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2737 - val_loss: 0.2617\n",
      "Epoch 30/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2654 - val_loss: 0.3233\n",
      "Epoch 31/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2688 - val_loss: 0.2550\n",
      "Epoch 32/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2640 - val_loss: 0.2841\n",
      "Epoch 33/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2623 - val_loss: 0.2359\n",
      "Epoch 34/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2656 - val_loss: 0.2417\n",
      "Epoch 35/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2648 - val_loss: 0.2501\n",
      "Epoch 36/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2602 - val_loss: 0.2380\n",
      "Epoch 37/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2604 - val_loss: 0.2425\n",
      "Epoch 38/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2609 - val_loss: 0.2398\n",
      "Epoch 39/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2589 - val_loss: 0.2404\n",
      "Epoch 40/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2569 - val_loss: 0.2421\n",
      "Epoch 41/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2533 - val_loss: 0.2365\n",
      "Epoch 42/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2502 - val_loss: 0.2480\n",
      "Epoch 43/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2534 - val_loss: 0.2266\n",
      "Epoch 44/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2492 - val_loss: 0.2291\n",
      "Epoch 45/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2477 - val_loss: 0.2356\n",
      "Epoch 46/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2475 - val_loss: 0.2227\n",
      "Epoch 47/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2482 - val_loss: 0.2330\n",
      "Epoch 48/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2471 - val_loss: 0.2254\n",
      "Epoch 49/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2440 - val_loss: 0.2336\n",
      "Epoch 50/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2451 - val_loss: 0.2221\n",
      "Epoch 51/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2465 - val_loss: 0.2657\n",
      "Epoch 52/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2441 - val_loss: 0.2305\n",
      "Epoch 53/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2424 - val_loss: 0.2349\n",
      "Epoch 54/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2394 - val_loss: 0.2224\n",
      "Epoch 55/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2422 - val_loss: 0.2178\n",
      "Epoch 56/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2400 - val_loss: 0.2343\n",
      "Epoch 57/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2404 - val_loss: 0.2161\n",
      "Epoch 58/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2358 - val_loss: 0.2240\n",
      "Epoch 59/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2403 - val_loss: 0.2262\n",
      "Epoch 60/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2419 - val_loss: 0.2217\n",
      "Epoch 61/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2359 - val_loss: 0.2216\n",
      "Epoch 62/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2366 - val_loss: 0.2154\n",
      "Epoch 63/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2389 - val_loss: 0.2111\n",
      "Epoch 64/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2326 - val_loss: 0.2162\n",
      "Epoch 65/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2371 - val_loss: 0.2119\n",
      "Epoch 66/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2368 - val_loss: 0.2143\n",
      "Epoch 67/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2353 - val_loss: 0.2205\n",
      "Epoch 68/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2353 - val_loss: 0.2235\n",
      "Epoch 69/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2315 - val_loss: 0.2098\n",
      "Epoch 70/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2318 - val_loss: 0.2159\n",
      "Epoch 71/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2305 - val_loss: 0.2154\n",
      "Epoch 72/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2283 - val_loss: 0.2216\n",
      "Epoch 73/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2280 - val_loss: 0.2192\n",
      "Epoch 74/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2298 - val_loss: 0.2114\n",
      "Epoch 75/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2319 - val_loss: 0.2055\n",
      "Epoch 76/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2281 - val_loss: 0.2112\n",
      "Epoch 77/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2343 - val_loss: 0.2121\n",
      "Epoch 78/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2316 - val_loss: 0.2153\n",
      "Epoch 79/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2251 - val_loss: 0.2093\n",
      "Epoch 80/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2254 - val_loss: 0.2067\n",
      "Epoch 81/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2262 - val_loss: 0.2155\n",
      "Epoch 82/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2237 - val_loss: 0.2089\n",
      "Epoch 83/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2239 - val_loss: 0.1985\n",
      "Epoch 84/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2232 - val_loss: 0.2229\n",
      "Epoch 85/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2234 - val_loss: 0.2062\n",
      "Epoch 86/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2248 - val_loss: 0.2200\n",
      "Epoch 87/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2253 - val_loss: 0.2403\n",
      "Epoch 88/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2242 - val_loss: 0.2092\n",
      "Epoch 89/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2210 - val_loss: 0.2273\n",
      "Epoch 90/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2233 - val_loss: 0.2046\n",
      "Epoch 91/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2225 - val_loss: 0.2200\n",
      "Epoch 92/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2230 - val_loss: 0.2054\n",
      "Epoch 93/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2211 - val_loss: 0.2178\n",
      "Epoch 94/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2202 - val_loss: 0.2044\n",
      "Epoch 95/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2197 - val_loss: 0.2168\n",
      "Epoch 96/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2182 - val_loss: 0.2103\n",
      "Epoch 97/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2194 - val_loss: 0.2097\n",
      "Epoch 98/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2204 - val_loss: 0.2324\n",
      "Epoch 99/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2234 - val_loss: 0.2125\n",
      "Epoch 100/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2180 - val_loss: 0.2092\n",
      "Epoch 101/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2197 - val_loss: 0.1965\n",
      "Epoch 102/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2171 - val_loss: 0.2034\n",
      "Epoch 103/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2146 - val_loss: 0.2036\n",
      "Epoch 104/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2172 - val_loss: 0.2048\n",
      "Epoch 105/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2154 - val_loss: 0.2000\n",
      "Epoch 106/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2160 - val_loss: 0.2035\n",
      "Epoch 107/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2166 - val_loss: 0.2035\n",
      "Epoch 108/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2169 - val_loss: 0.2061\n",
      "Epoch 109/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2144 - val_loss: 0.1984\n",
      "Epoch 110/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2117 - val_loss: 0.1973\n",
      "Epoch 111/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2145 - val_loss: 0.1984\n",
      "Epoch 112/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2187 - val_loss: 0.1986\n",
      "Epoch 113/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2144 - val_loss: 0.1974\n",
      "Epoch 114/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2107 - val_loss: 0.2037\n",
      "Epoch 115/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2129 - val_loss: 0.2160\n",
      "Epoch 116/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2159 - val_loss: 0.1989\n",
      "Epoch 117/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2136 - val_loss: 0.2009\n",
      "Epoch 118/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2128 - val_loss: 0.2129\n",
      "Epoch 119/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2106 - val_loss: 0.2098\n",
      "Epoch 120/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2114 - val_loss: 0.1985\n",
      "Epoch 121/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2100 - val_loss: 0.2155\n",
      "Epoch 122/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2121 - val_loss: 0.2182\n",
      "Epoch 123/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2105 - val_loss: 0.1973\n",
      "Epoch 124/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2101 - val_loss: 0.1919\n",
      "Epoch 125/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2160 - val_loss: 0.2170\n",
      "Epoch 126/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2122 - val_loss: 0.2023\n",
      "Epoch 127/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2105 - val_loss: 0.1944\n",
      "Epoch 128/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2117 - val_loss: 0.2119\n",
      "Epoch 129/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2089 - val_loss: 0.1950\n",
      "Epoch 130/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2121 - val_loss: 0.2044\n",
      "Epoch 131/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2112 - val_loss: 0.1914\n",
      "Epoch 132/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2082 - val_loss: 0.1966\n",
      "Epoch 133/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2110 - val_loss: 0.2061\n",
      "Epoch 134/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2059 - val_loss: 0.1912\n",
      "Epoch 135/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2050 - val_loss: 0.1956\n",
      "Epoch 136/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2089 - val_loss: 0.1988\n",
      "Epoch 137/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2052 - val_loss: 0.1896\n",
      "Epoch 138/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2077 - val_loss: 0.2006\n",
      "Epoch 139/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2076 - val_loss: 0.1943\n",
      "Epoch 140/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2068 - val_loss: 0.1863\n",
      "Epoch 141/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2056 - val_loss: 0.1896\n",
      "Epoch 142/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2067 - val_loss: 0.1911\n",
      "Epoch 143/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2043 - val_loss: 0.1916\n",
      "Epoch 144/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2045 - val_loss: 0.1883\n",
      "Epoch 145/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2047 - val_loss: 0.2062\n",
      "Epoch 146/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2080 - val_loss: 0.1992\n",
      "Epoch 147/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2080 - val_loss: 0.2017\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2057 - val_loss: 0.1953\n",
      "Epoch 149/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2045 - val_loss: 0.1891\n",
      "Epoch 150/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2063 - val_loss: 0.1988\n",
      "Epoch 151/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2056 - val_loss: 0.1893\n",
      "Epoch 152/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2071 - val_loss: 0.1945\n",
      "Epoch 153/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2072 - val_loss: 0.2089\n",
      "Epoch 154/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2041 - val_loss: 0.1885\n",
      "Epoch 155/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2067 - val_loss: 0.1940\n",
      "Epoch 156/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2040 - val_loss: 0.1898\n",
      "Epoch 157/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2024 - val_loss: 0.1894\n",
      "Epoch 158/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2050 - val_loss: 0.1914\n",
      "Epoch 159/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.2040 - val_loss: 0.1968\n",
      "Epoch 160/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1995 - val_loss: 0.1974\n",
      "Epoch 161/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2028 - val_loss: 0.1839\n",
      "Epoch 162/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1998 - val_loss: 0.1926\n",
      "Epoch 163/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2006 - val_loss: 0.1820\n",
      "Epoch 164/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2049 - val_loss: 0.1958\n",
      "Epoch 165/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2001 - val_loss: 0.1902\n",
      "Epoch 166/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2012 - val_loss: 0.1907\n",
      "Epoch 167/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2025 - val_loss: 0.2057\n",
      "Epoch 168/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2005 - val_loss: 0.1894\n",
      "Epoch 169/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2048 - val_loss: 0.1856\n",
      "Epoch 170/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2018 - val_loss: 0.1930\n",
      "Epoch 171/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2026 - val_loss: 0.1870\n",
      "Epoch 172/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2014 - val_loss: 0.1914\n",
      "Epoch 173/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2031 - val_loss: 0.1879\n",
      "Epoch 174/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2024 - val_loss: 0.2008\n",
      "Epoch 175/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2103 - val_loss: 0.1875\n",
      "Epoch 176/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1978 - val_loss: 0.1862\n",
      "Epoch 177/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2029 - val_loss: 0.2049\n",
      "Epoch 178/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2013 - val_loss: 0.1825\n",
      "Epoch 179/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2015 - val_loss: 0.1901\n",
      "Epoch 180/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2003 - val_loss: 0.1923\n",
      "Epoch 181/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2034 - val_loss: 0.1848\n",
      "Epoch 182/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2018 - val_loss: 0.1781\n",
      "Epoch 183/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2000 - val_loss: 0.1895\n",
      "Epoch 184/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1990 - val_loss: 0.1867\n",
      "Epoch 185/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1996 - val_loss: 0.1872\n",
      "Epoch 186/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1972 - val_loss: 0.1841\n",
      "Epoch 187/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2002 - val_loss: 0.1846\n",
      "Epoch 188/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2006 - val_loss: 0.1805\n",
      "Epoch 189/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.2011 - val_loss: 0.1886\n",
      "Epoch 190/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2016 - val_loss: 0.1819\n",
      "Epoch 191/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1977 - val_loss: 0.1818\n",
      "Epoch 192/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1990 - val_loss: 0.1791\n",
      "Epoch 193/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1993 - val_loss: 0.1941\n",
      "Epoch 194/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2005 - val_loss: 0.1854\n",
      "Epoch 195/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2022 - val_loss: 0.1836\n",
      "Epoch 196/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1946 - val_loss: 0.1821\n",
      "Epoch 197/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1960 - val_loss: 0.1836\n",
      "Epoch 198/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1963 - val_loss: 0.1937\n",
      "Epoch 199/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1952 - val_loss: 0.1833\n",
      "Epoch 200/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1947 - val_loss: 0.1869\n",
      "Epoch 201/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1981 - val_loss: 0.1966\n",
      "Epoch 202/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.2004 - val_loss: 0.1849\n",
      "Epoch 203/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1947 - val_loss: 0.1968\n",
      "Epoch 204/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1986 - val_loss: 0.2049\n",
      "Epoch 205/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1964 - val_loss: 0.1910\n",
      "Epoch 206/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1978 - val_loss: 0.1947\n",
      "Epoch 207/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1966 - val_loss: 0.1791\n",
      "Epoch 208/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1908 - val_loss: 0.1878\n",
      "Epoch 209/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1963 - val_loss: 0.1767\n",
      "Epoch 210/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1994 - val_loss: 0.1998\n",
      "Epoch 211/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1992 - val_loss: 0.1858\n",
      "Epoch 212/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1953 - val_loss: 0.1888\n",
      "Epoch 213/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1940 - val_loss: 0.2031\n",
      "Epoch 214/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1947 - val_loss: 0.1827\n",
      "Epoch 215/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1969 - val_loss: 0.2169\n",
      "Epoch 216/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1932 - val_loss: 0.1765\n",
      "Epoch 217/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1942 - val_loss: 0.1813\n",
      "Epoch 218/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1938 - val_loss: 0.1747\n",
      "Epoch 219/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1960 - val_loss: 0.1789\n",
      "Epoch 220/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1950 - val_loss: 0.1886\n",
      "Epoch 221/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1966 - val_loss: 0.1828\n",
      "Epoch 222/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1977 - val_loss: 0.1807\n",
      "Epoch 223/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1956 - val_loss: 0.1816\n",
      "Epoch 224/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1969 - val_loss: 0.1833\n",
      "Epoch 225/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1968 - val_loss: 0.1848\n",
      "Epoch 226/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1930 - val_loss: 0.1779\n",
      "Epoch 227/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1949 - val_loss: 0.1764\n",
      "Epoch 228/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1934 - val_loss: 0.1876\n",
      "Epoch 229/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1939 - val_loss: 0.1840\n",
      "Epoch 230/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1980 - val_loss: 0.1785\n",
      "Epoch 231/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1916 - val_loss: 0.1825\n",
      "Epoch 232/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1957 - val_loss: 0.1959\n",
      "Epoch 233/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1948 - val_loss: 0.1779\n",
      "Epoch 234/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1909 - val_loss: 0.1840\n",
      "Epoch 235/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1894 - val_loss: 0.1852\n",
      "Epoch 236/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1960 - val_loss: 0.1814\n",
      "Epoch 237/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1895 - val_loss: 0.1811\n",
      "Epoch 238/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1983 - val_loss: 0.1753\n",
      "Epoch 239/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1943 - val_loss: 0.1771\n",
      "Epoch 240/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1936 - val_loss: 0.1862\n",
      "Epoch 241/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1919 - val_loss: 0.1833\n",
      "Epoch 242/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1915 - val_loss: 0.1892\n",
      "Epoch 243/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1917 - val_loss: 0.1954\n",
      "Epoch 244/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1923 - val_loss: 0.1928\n",
      "Epoch 245/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1889 - val_loss: 0.1757\n",
      "Epoch 246/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1945 - val_loss: 0.1792\n",
      "Epoch 247/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1901 - val_loss: 0.2012\n",
      "Epoch 248/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1924 - val_loss: 0.1843\n",
      "\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 249/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1827 - val_loss: 0.1734\n",
      "Epoch 250/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1813 - val_loss: 0.1697\n",
      "Epoch 251/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1812 - val_loss: 0.1791\n",
      "Epoch 252/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1820 - val_loss: 0.1659\n",
      "Epoch 253/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1834 - val_loss: 0.1624\n",
      "Epoch 254/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1784 - val_loss: 0.1827\n",
      "Epoch 255/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1775 - val_loss: 0.1635\n",
      "Epoch 256/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1827 - val_loss: 0.1645\n",
      "Epoch 257/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1784 - val_loss: 0.1733\n",
      "Epoch 258/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1788 - val_loss: 0.1644\n",
      "Epoch 259/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1795 - val_loss: 0.1693\n",
      "Epoch 260/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1802 - val_loss: 0.1635\n",
      "Epoch 261/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1810 - val_loss: 0.1649\n",
      "Epoch 262/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1793 - val_loss: 0.1841\n",
      "Epoch 263/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1827 - val_loss: 0.1618\n",
      "Epoch 264/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1812 - val_loss: 0.1658\n",
      "Epoch 265/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1810 - val_loss: 0.1687\n",
      "Epoch 266/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1813 - val_loss: 0.1766\n",
      "Epoch 267/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1770 - val_loss: 0.1639\n",
      "Epoch 268/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1785 - val_loss: 0.1632\n",
      "Epoch 269/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1796 - val_loss: 0.1662\n",
      "Epoch 270/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1795 - val_loss: 0.1628\n",
      "Epoch 271/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1803 - val_loss: 0.1713\n",
      "Epoch 272/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1789 - val_loss: 0.1639\n",
      "Epoch 273/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1779 - val_loss: 0.1644\n",
      "Epoch 274/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1804 - val_loss: 0.1671\n",
      "Epoch 275/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1800 - val_loss: 0.1668\n",
      "Epoch 276/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1781 - val_loss: 0.1739\n",
      "Epoch 277/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1765 - val_loss: 0.1617\n",
      "Epoch 278/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1805 - val_loss: 0.1632\n",
      "Epoch 279/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1793 - val_loss: 0.1659\n",
      "Epoch 280/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1775 - val_loss: 0.1612\n",
      "Epoch 281/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1810 - val_loss: 0.1714\n",
      "Epoch 282/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1814 - val_loss: 0.1614\n",
      "Epoch 283/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1781 - val_loss: 0.1630\n",
      "Epoch 284/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1776 - val_loss: 0.1682\n",
      "Epoch 285/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1797 - val_loss: 0.1606\n",
      "Epoch 286/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1786 - val_loss: 0.1636\n",
      "Epoch 287/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1793 - val_loss: 0.1635\n",
      "Epoch 288/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1757 - val_loss: 0.1630\n",
      "Epoch 289/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1782 - val_loss: 0.1708\n",
      "Epoch 290/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1733 - val_loss: 0.1610\n",
      "Epoch 291/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1747 - val_loss: 0.1600\n",
      "Epoch 292/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1774 - val_loss: 0.1637\n",
      "Epoch 293/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1741 - val_loss: 0.1610\n",
      "Epoch 294/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1789 - val_loss: 0.1695\n",
      "Epoch 295/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1735 - val_loss: 0.1680\n",
      "Epoch 296/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1772 - val_loss: 0.1679\n",
      "Epoch 297/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1800 - val_loss: 0.1601\n",
      "Epoch 298/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1761 - val_loss: 0.1616\n",
      "Epoch 299/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1791 - val_loss: 0.1694\n",
      "Epoch 300/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1734 - val_loss: 0.1746\n",
      "Epoch 301/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1814 - val_loss: 0.1594\n",
      "Epoch 302/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1782 - val_loss: 0.1649\n",
      "Epoch 303/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1785 - val_loss: 0.1625\n",
      "Epoch 304/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1778 - val_loss: 0.1646\n",
      "Epoch 305/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1745 - val_loss: 0.1655\n",
      "Epoch 306/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1763 - val_loss: 0.1660\n",
      "Epoch 307/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1769 - val_loss: 0.1605\n",
      "Epoch 308/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1776 - val_loss: 0.1715\n",
      "Epoch 309/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1794 - val_loss: 0.1618\n",
      "Epoch 310/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1731 - val_loss: 0.1751\n",
      "Epoch 311/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1751 - val_loss: 0.1606\n",
      "Epoch 312/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1724 - val_loss: 0.1629\n",
      "Epoch 313/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1763 - val_loss: 0.1611\n",
      "Epoch 314/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1703 - val_loss: 0.1602\n",
      "Epoch 315/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1781 - val_loss: 0.1601\n",
      "Epoch 316/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1775 - val_loss: 0.1630\n",
      "Epoch 317/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1767 - val_loss: 0.1729\n",
      "Epoch 318/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1780 - val_loss: 0.1645\n",
      "Epoch 319/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1771 - val_loss: 0.1646\n",
      "Epoch 320/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1761 - val_loss: 0.1623\n",
      "Epoch 321/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1743 - val_loss: 0.1615\n",
      "Epoch 322/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1771 - val_loss: 0.1686\n",
      "Epoch 323/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1781 - val_loss: 0.1594\n",
      "Epoch 324/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1753 - val_loss: 0.1603\n",
      "Epoch 325/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1768 - val_loss: 0.1653\n",
      "Epoch 326/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1779 - val_loss: 0.1620\n",
      "Epoch 327/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1747 - val_loss: 0.1620\n",
      "Epoch 328/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1747 - val_loss: 0.1641\n",
      "Epoch 329/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1727 - val_loss: 0.1667\n",
      "Epoch 330/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1723 - val_loss: 0.1591\n",
      "Epoch 331/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1786 - val_loss: 0.1743\n",
      "Epoch 332/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1759 - val_loss: 0.1744\n",
      "Epoch 333/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1723 - val_loss: 0.1628\n",
      "Epoch 334/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1738 - val_loss: 0.1617\n",
      "Epoch 335/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1759 - val_loss: 0.1784\n",
      "Epoch 336/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1789 - val_loss: 0.1674\n",
      "Epoch 337/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1786 - val_loss: 0.1595\n",
      "Epoch 338/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1769 - val_loss: 0.1585\n",
      "Epoch 339/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1784 - val_loss: 0.1657\n",
      "Epoch 340/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1762 - val_loss: 0.1669\n",
      "Epoch 341/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1750 - val_loss: 0.1637\n",
      "Epoch 342/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1764 - val_loss: 0.1663\n",
      "Epoch 343/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1767 - val_loss: 0.1636\n",
      "Epoch 344/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1738 - val_loss: 0.1685\n",
      "Epoch 345/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1799 - val_loss: 0.1641\n",
      "Epoch 346/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1754 - val_loss: 0.1681\n",
      "Epoch 347/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1740 - val_loss: 0.1630\n",
      "Epoch 348/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1727 - val_loss: 0.1619\n",
      "Epoch 349/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1738 - val_loss: 0.1601\n",
      "Epoch 350/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1724 - val_loss: 0.1679\n",
      "Epoch 351/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1744 - val_loss: 0.1594\n",
      "Epoch 352/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1781 - val_loss: 0.1650\n",
      "Epoch 353/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1767 - val_loss: 0.1611\n",
      "Epoch 354/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1770 - val_loss: 0.1718\n",
      "Epoch 355/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1768 - val_loss: 0.1590\n",
      "Epoch 356/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1730 - val_loss: 0.1659\n",
      "Epoch 357/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1762 - val_loss: 0.1582\n",
      "Epoch 358/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1732 - val_loss: 0.1581\n",
      "Epoch 359/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1717 - val_loss: 0.1601\n",
      "Epoch 360/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1751 - val_loss: 0.1606\n",
      "Epoch 361/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1751 - val_loss: 0.1607\n",
      "Epoch 362/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1737 - val_loss: 0.1665\n",
      "Epoch 363/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1751 - val_loss: 0.1697\n",
      "Epoch 364/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1729 - val_loss: 0.1612\n",
      "Epoch 365/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1761 - val_loss: 0.1588\n",
      "Epoch 366/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1764 - val_loss: 0.1634\n",
      "Epoch 367/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1765 - val_loss: 0.1597\n",
      "Epoch 368/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1750 - val_loss: 0.1646\n",
      "Epoch 369/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1755 - val_loss: 0.1598\n",
      "Epoch 370/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1755 - val_loss: 0.1604\n",
      "Epoch 371/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1736 - val_loss: 0.1667\n",
      "Epoch 372/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1762 - val_loss: 0.1599\n",
      "Epoch 373/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1775 - val_loss: 0.1642\n",
      "Epoch 374/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1769 - val_loss: 0.1616\n",
      "Epoch 375/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1748 - val_loss: 0.1620\n",
      "Epoch 376/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1736 - val_loss: 0.1608\n",
      "Epoch 377/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1775 - val_loss: 0.1624\n",
      "Epoch 378/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1777 - val_loss: 0.1601\n",
      "Epoch 379/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1743 - val_loss: 0.1691\n",
      "Epoch 380/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1748 - val_loss: 0.1575\n",
      "Epoch 381/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1741 - val_loss: 0.1611\n",
      "Epoch 382/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1724 - val_loss: 0.1631\n",
      "Epoch 383/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1725 - val_loss: 0.1600\n",
      "Epoch 384/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1706 - val_loss: 0.1602\n",
      "Epoch 385/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1696 - val_loss: 0.1619\n",
      "Epoch 386/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1759 - val_loss: 0.1579\n",
      "Epoch 387/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1721 - val_loss: 0.1690\n",
      "Epoch 388/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1747 - val_loss: 0.1590\n",
      "Epoch 389/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1740 - val_loss: 0.1635\n",
      "Epoch 390/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1741 - val_loss: 0.1569\n",
      "Epoch 391/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1724 - val_loss: 0.1579\n",
      "Epoch 392/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1734 - val_loss: 0.1581\n",
      "Epoch 393/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1724 - val_loss: 0.1668\n",
      "Epoch 394/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1747 - val_loss: 0.1592\n",
      "Epoch 395/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1752 - val_loss: 0.1588\n",
      "Epoch 396/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1732 - val_loss: 0.1763\n",
      "Epoch 397/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1744 - val_loss: 0.1593\n",
      "Epoch 398/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1786 - val_loss: 0.1622\n",
      "Epoch 399/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1776 - val_loss: 0.1576\n",
      "Epoch 400/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1748 - val_loss: 0.1627\n",
      "Epoch 401/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1719 - val_loss: 0.1576\n",
      "Epoch 402/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1737 - val_loss: 0.1683\n",
      "Epoch 403/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1733 - val_loss: 0.1637\n",
      "Epoch 404/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1746 - val_loss: 0.1586\n",
      "Epoch 405/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1735 - val_loss: 0.1573\n",
      "Epoch 406/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1708 - val_loss: 0.1568\n",
      "Epoch 407/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1749 - val_loss: 0.1763\n",
      "Epoch 408/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1756 - val_loss: 0.1586\n",
      "Epoch 409/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1763 - val_loss: 0.1579\n",
      "Epoch 410/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1731 - val_loss: 0.1894\n",
      "Epoch 411/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1759 - val_loss: 0.1628\n",
      "Epoch 412/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1692 - val_loss: 0.1574\n",
      "Epoch 413/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1726 - val_loss: 0.1729\n",
      "Epoch 414/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1731 - val_loss: 0.1564\n",
      "Epoch 415/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1699 - val_loss: 0.1577\n",
      "Epoch 416/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1716 - val_loss: 0.1605\n",
      "Epoch 417/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1737 - val_loss: 0.1671\n",
      "Epoch 418/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1713 - val_loss: 0.1590\n",
      "Epoch 419/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1736 - val_loss: 0.1581\n",
      "Epoch 420/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1741 - val_loss: 0.1632\n",
      "Epoch 421/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1721 - val_loss: 0.1577\n",
      "Epoch 422/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1746 - val_loss: 0.1583\n",
      "Epoch 423/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1725 - val_loss: 0.1612\n",
      "Epoch 424/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1701 - val_loss: 0.1726\n",
      "Epoch 425/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1781 - val_loss: 0.1605\n",
      "Epoch 426/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1703 - val_loss: 0.1577\n",
      "Epoch 427/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1742 - val_loss: 0.1707\n",
      "Epoch 428/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1742 - val_loss: 0.1577\n",
      "Epoch 429/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1698 - val_loss: 0.1573\n",
      "Epoch 430/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1741 - val_loss: 0.1575\n",
      "Epoch 431/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1740 - val_loss: 0.1567\n",
      "Epoch 432/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1714 - val_loss: 0.1589\n",
      "Epoch 433/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1759 - val_loss: 0.1665\n",
      "Epoch 434/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1700 - val_loss: 0.1737\n",
      "Epoch 435/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1725 - val_loss: 0.1572\n",
      "Epoch 436/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1740 - val_loss: 0.1621\n",
      "Epoch 437/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1732 - val_loss: 0.1683\n",
      "Epoch 438/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1719 - val_loss: 0.1614\n",
      "Epoch 439/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1707 - val_loss: 0.1634\n",
      "Epoch 440/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1702 - val_loss: 0.1591\n",
      "Epoch 441/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1720 - val_loss: 0.1591\n",
      "Epoch 442/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1713 - val_loss: 0.1593\n",
      "Epoch 443/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1735 - val_loss: 0.1592\n",
      "Epoch 444/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1731 - val_loss: 0.1580\n",
      "\n",
      "Epoch 00444: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 445/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1663 - val_loss: 0.1540\n",
      "Epoch 446/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1670 - val_loss: 0.1536\n",
      "Epoch 447/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1692 - val_loss: 0.1524\n",
      "Epoch 448/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1645 - val_loss: 0.1525\n",
      "Epoch 449/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1639 - val_loss: 0.1545\n",
      "Epoch 450/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1651 - val_loss: 0.1541\n",
      "Epoch 451/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1677 - val_loss: 0.1574\n",
      "Epoch 452/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1666 - val_loss: 0.1555\n",
      "Epoch 453/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1654 - val_loss: 0.1574\n",
      "Epoch 454/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1647 - val_loss: 0.1530\n",
      "Epoch 455/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1635 - val_loss: 0.1536\n",
      "Epoch 456/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1658 - val_loss: 0.1556\n",
      "Epoch 457/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1697 - val_loss: 0.1521\n",
      "Epoch 458/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1648 - val_loss: 0.1517\n",
      "Epoch 459/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1640 - val_loss: 0.1523\n",
      "Epoch 460/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1673 - val_loss: 0.1519\n",
      "Epoch 461/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1661 - val_loss: 0.1509\n",
      "Epoch 462/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1695 - val_loss: 0.1580\n",
      "Epoch 463/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1648 - val_loss: 0.1539\n",
      "Epoch 464/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1625 - val_loss: 0.1513\n",
      "Epoch 465/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1672 - val_loss: 0.1531\n",
      "Epoch 466/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1629 - val_loss: 0.1522\n",
      "Epoch 467/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1637 - val_loss: 0.1557\n",
      "Epoch 468/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1633 - val_loss: 0.1534\n",
      "Epoch 469/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1637 - val_loss: 0.1511\n",
      "Epoch 470/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1691 - val_loss: 0.1527\n",
      "Epoch 471/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1654 - val_loss: 0.1539\n",
      "Epoch 472/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1696 - val_loss: 0.1526\n",
      "Epoch 473/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1658 - val_loss: 0.1503\n",
      "Epoch 474/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1652 - val_loss: 0.1591\n",
      "Epoch 475/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1654 - val_loss: 0.1651\n",
      "Epoch 476/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1670 - val_loss: 0.1564\n",
      "Epoch 477/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1622 - val_loss: 0.1537\n",
      "Epoch 478/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1677 - val_loss: 0.1507\n",
      "Epoch 479/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1666 - val_loss: 0.1535\n",
      "Epoch 480/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1658 - val_loss: 0.1515\n",
      "Epoch 481/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1643 - val_loss: 0.1512\n",
      "Epoch 482/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1665 - val_loss: 0.1520\n",
      "Epoch 483/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1657 - val_loss: 0.1522\n",
      "Epoch 484/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1628 - val_loss: 0.1515\n",
      "Epoch 485/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1655 - val_loss: 0.1508\n",
      "Epoch 486/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1647 - val_loss: 0.1652\n",
      "Epoch 487/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1645 - val_loss: 0.1499\n",
      "Epoch 488/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1657 - val_loss: 0.1531\n",
      "Epoch 489/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1618 - val_loss: 0.1587\n",
      "Epoch 490/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1651 - val_loss: 0.1510\n",
      "Epoch 491/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1633 - val_loss: 0.1536\n",
      "Epoch 492/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1683 - val_loss: 0.1521\n",
      "Epoch 493/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1652 - val_loss: 0.1503\n",
      "Epoch 494/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1693 - val_loss: 0.1505\n",
      "Epoch 495/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1635 - val_loss: 0.1509\n",
      "Epoch 496/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1668 - val_loss: 0.1501\n",
      "Epoch 497/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1630 - val_loss: 0.1521\n",
      "Epoch 498/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1670 - val_loss: 0.1512\n",
      "Epoch 499/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1667 - val_loss: 0.1540\n",
      "Epoch 500/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1655 - val_loss: 0.1544\n",
      "Epoch 501/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1658 - val_loss: 0.1506\n",
      "Epoch 502/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1646 - val_loss: 0.1521\n",
      "Epoch 503/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1648 - val_loss: 0.1539\n",
      "Epoch 504/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1686 - val_loss: 0.1501\n",
      "Epoch 505/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1682 - val_loss: 0.1501\n",
      "Epoch 506/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1667 - val_loss: 0.1523\n",
      "Epoch 507/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1627 - val_loss: 0.1577\n",
      "Epoch 508/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1648 - val_loss: 0.1499\n",
      "Epoch 509/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1637 - val_loss: 0.1501\n",
      "Epoch 510/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1719 - val_loss: 0.1505\n",
      "Epoch 511/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1649 - val_loss: 0.1521\n",
      "Epoch 512/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1686 - val_loss: 0.1524\n",
      "Epoch 513/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1660 - val_loss: 0.1509\n",
      "Epoch 514/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1668 - val_loss: 0.1646\n",
      "Epoch 515/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1665 - val_loss: 0.1494\n",
      "Epoch 516/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1627 - val_loss: 0.1496\n",
      "Epoch 517/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1629 - val_loss: 0.1510\n",
      "Epoch 518/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1674 - val_loss: 0.1504\n",
      "Epoch 519/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1652 - val_loss: 0.1500\n",
      "Epoch 520/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1658 - val_loss: 0.1624\n",
      "Epoch 521/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1627 - val_loss: 0.1511\n",
      "Epoch 522/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1642 - val_loss: 0.1506\n",
      "Epoch 523/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1669 - val_loss: 0.1494\n",
      "Epoch 524/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1627 - val_loss: 0.1521\n",
      "Epoch 525/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1670 - val_loss: 0.1516\n",
      "Epoch 526/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1640 - val_loss: 0.1512\n",
      "Epoch 527/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1652 - val_loss: 0.1516\n",
      "Epoch 528/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1640 - val_loss: 0.1501\n",
      "Epoch 529/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1616 - val_loss: 0.1505\n",
      "Epoch 530/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1669 - val_loss: 0.1503\n",
      "Epoch 531/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1618 - val_loss: 0.1499\n",
      "Epoch 532/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1657 - val_loss: 0.1496\n",
      "Epoch 533/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1658 - val_loss: 0.1499\n",
      "Epoch 534/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1642 - val_loss: 0.1522\n",
      "Epoch 535/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1691 - val_loss: 0.1504\n",
      "Epoch 536/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1646 - val_loss: 0.1564\n",
      "Epoch 537/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1674 - val_loss: 0.1551\n",
      "Epoch 538/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1652 - val_loss: 0.1532\n",
      "Epoch 539/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1610 - val_loss: 0.1516\n",
      "Epoch 540/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1625 - val_loss: 0.1520\n",
      "Epoch 541/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1688 - val_loss: 0.1541\n",
      "Epoch 542/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1634 - val_loss: 0.1521\n",
      "Epoch 543/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1627 - val_loss: 0.1502\n",
      "Epoch 544/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1659 - val_loss: 0.1616\n",
      "Epoch 545/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1612 - val_loss: 0.1514\n",
      "\n",
      "Epoch 00545: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 546/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1600 - val_loss: 0.1471\n",
      "Epoch 547/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1599 - val_loss: 0.1472\n",
      "Epoch 548/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1483\n",
      "Epoch 549/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1608 - val_loss: 0.1465\n",
      "Epoch 550/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1636 - val_loss: 0.1473\n",
      "Epoch 551/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1478\n",
      "Epoch 552/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1638 - val_loss: 0.1477\n",
      "Epoch 553/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1590 - val_loss: 0.1467\n",
      "Epoch 554/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1610 - val_loss: 0.1462\n",
      "Epoch 555/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1466\n",
      "Epoch 556/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1609 - val_loss: 0.1462\n",
      "Epoch 557/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1597 - val_loss: 0.1483\n",
      "Epoch 558/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1606 - val_loss: 0.1467\n",
      "Epoch 559/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1638 - val_loss: 0.1484\n",
      "Epoch 560/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1485\n",
      "Epoch 561/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1579 - val_loss: 0.1493\n",
      "Epoch 562/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1598 - val_loss: 0.1481\n",
      "Epoch 563/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1596 - val_loss: 0.1484\n",
      "Epoch 564/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1632 - val_loss: 0.1466\n",
      "Epoch 565/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1624 - val_loss: 0.1500\n",
      "Epoch 566/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1618 - val_loss: 0.1468\n",
      "Epoch 567/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1593 - val_loss: 0.1484\n",
      "Epoch 568/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1613 - val_loss: 0.1468\n",
      "Epoch 569/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1588 - val_loss: 0.1493\n",
      "Epoch 570/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1592 - val_loss: 0.1465\n",
      "Epoch 571/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1626 - val_loss: 0.1472\n",
      "Epoch 572/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1562 - val_loss: 0.1496\n",
      "Epoch 573/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1641 - val_loss: 0.1477\n",
      "Epoch 574/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1604 - val_loss: 0.1467\n",
      "Epoch 575/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1628 - val_loss: 0.1477\n",
      "Epoch 576/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1573 - val_loss: 0.1505\n",
      "Epoch 577/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1619 - val_loss: 0.1485\n",
      "Epoch 578/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1587 - val_loss: 0.1478\n",
      "Epoch 579/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1601 - val_loss: 0.1480\n",
      "Epoch 580/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1601 - val_loss: 0.1462\n",
      "Epoch 581/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1614 - val_loss: 0.1565\n",
      "Epoch 582/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1618 - val_loss: 0.1491\n",
      "Epoch 583/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1638 - val_loss: 0.1466\n",
      "Epoch 584/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1578 - val_loss: 0.1473\n",
      "\n",
      "Epoch 00584: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 585/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1629 - val_loss: 0.1466\n",
      "Epoch 586/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1608 - val_loss: 0.1454\n",
      "Epoch 587/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1581 - val_loss: 0.1460\n",
      "Epoch 588/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1587 - val_loss: 0.1460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 589/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1579 - val_loss: 0.1451\n",
      "Epoch 590/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1629 - val_loss: 0.1458\n",
      "Epoch 591/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1615 - val_loss: 0.1453\n",
      "Epoch 592/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1614 - val_loss: 0.1448\n",
      "Epoch 593/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1613 - val_loss: 0.1450\n",
      "Epoch 594/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1627 - val_loss: 0.1451\n",
      "Epoch 595/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1589 - val_loss: 0.1446\n",
      "Epoch 596/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1564 - val_loss: 0.1456\n",
      "Epoch 597/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1597 - val_loss: 0.1447\n",
      "Epoch 598/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1585 - val_loss: 0.1450\n",
      "Epoch 599/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1553 - val_loss: 0.1449\n",
      "Epoch 600/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1568 - val_loss: 0.1449\n",
      "Epoch 601/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1605 - val_loss: 0.1453\n",
      "Epoch 602/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1598 - val_loss: 0.1458\n",
      "Epoch 603/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1448\n",
      "Epoch 604/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1589 - val_loss: 0.1446\n",
      "Epoch 605/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1588 - val_loss: 0.1470\n",
      "Epoch 606/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1581 - val_loss: 0.1448\n",
      "Epoch 607/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1606 - val_loss: 0.1448\n",
      "Epoch 608/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1598 - val_loss: 0.1457\n",
      "Epoch 609/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1611 - val_loss: 0.1449\n",
      "Epoch 610/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1592 - val_loss: 0.1452\n",
      "Epoch 611/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1617 - val_loss: 0.1456\n",
      "Epoch 612/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1608 - val_loss: 0.1455\n",
      "Epoch 613/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1578 - val_loss: 0.1447\n",
      "Epoch 614/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1583 - val_loss: 0.1453\n",
      "Epoch 615/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1561 - val_loss: 0.1445\n",
      "Epoch 616/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1627 - val_loss: 0.1445\n",
      "Epoch 617/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1600 - val_loss: 0.1455\n",
      "Epoch 618/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1596 - val_loss: 0.1453\n",
      "Epoch 619/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1531 - val_loss: 0.1450\n",
      "Epoch 620/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1578 - val_loss: 0.1446\n",
      "Epoch 621/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1586 - val_loss: 0.1445\n",
      "Epoch 622/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1572 - val_loss: 0.1450\n",
      "Epoch 623/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1605 - val_loss: 0.1450\n",
      "Epoch 624/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1573 - val_loss: 0.1457\n",
      "Epoch 625/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1599 - val_loss: 0.1448\n",
      "Epoch 626/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1449\n",
      "Epoch 627/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1611 - val_loss: 0.1445\n",
      "Epoch 628/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1617 - val_loss: 0.1456\n",
      "Epoch 629/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1607 - val_loss: 0.1448\n",
      "Epoch 630/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1570 - val_loss: 0.1454\n",
      "Epoch 631/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1611 - val_loss: 0.1445\n",
      "Epoch 632/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1629 - val_loss: 0.1460\n",
      "Epoch 633/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1583 - val_loss: 0.1446\n",
      "Epoch 634/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1605 - val_loss: 0.1447\n",
      "Epoch 635/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1604 - val_loss: 0.1446\n",
      "Epoch 636/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1579 - val_loss: 0.1445\n",
      "Epoch 637/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1621 - val_loss: 0.1448\n",
      "Epoch 638/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1581 - val_loss: 0.1448\n",
      "Epoch 639/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1550 - val_loss: 0.1445\n",
      "Epoch 640/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1598 - val_loss: 0.1449\n",
      "Epoch 641/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1607 - val_loss: 0.1445\n",
      "Epoch 642/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1569 - val_loss: 0.1449\n",
      "Epoch 643/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1607 - val_loss: 0.1450\n",
      "Epoch 644/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1565 - val_loss: 0.1445\n",
      "Epoch 645/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1619 - val_loss: 0.1446\n",
      "Epoch 646/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1613 - val_loss: 0.1446\n",
      "\n",
      "Epoch 00646: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 647/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1440\n",
      "Epoch 648/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1566 - val_loss: 0.1441\n",
      "Epoch 649/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1601 - val_loss: 0.1444\n",
      "Epoch 650/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1599 - val_loss: 0.1442\n",
      "Epoch 651/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1599 - val_loss: 0.1438\n",
      "Epoch 652/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1576 - val_loss: 0.1439\n",
      "Epoch 653/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1439\n",
      "Epoch 654/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1604 - val_loss: 0.1440\n",
      "Epoch 655/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1596 - val_loss: 0.1439\n",
      "Epoch 656/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1568 - val_loss: 0.1440\n",
      "Epoch 657/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1575 - val_loss: 0.1443\n",
      "Epoch 658/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1572 - val_loss: 0.1439\n",
      "Epoch 659/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1441\n",
      "Epoch 660/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1599 - val_loss: 0.1442\n",
      "Epoch 661/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1584 - val_loss: 0.1439\n",
      "Epoch 662/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1440\n",
      "Epoch 663/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1582 - val_loss: 0.1443\n",
      "Epoch 664/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1574 - val_loss: 0.1440\n",
      "Epoch 665/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1574 - val_loss: 0.1438\n",
      "Epoch 666/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1594 - val_loss: 0.1442\n",
      "Epoch 667/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1602 - val_loss: 0.1441\n",
      "Epoch 668/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1553 - val_loss: 0.1438\n",
      "Epoch 669/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1550 - val_loss: 0.1437\n",
      "Epoch 670/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1606 - val_loss: 0.1444\n",
      "Epoch 671/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1556 - val_loss: 0.1438\n",
      "Epoch 672/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1569 - val_loss: 0.1442\n",
      "Epoch 673/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1592 - val_loss: 0.1439\n",
      "Epoch 674/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1606 - val_loss: 0.1437\n",
      "Epoch 675/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1592 - val_loss: 0.1438\n",
      "Epoch 676/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1572 - val_loss: 0.1441\n",
      "Epoch 677/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1591 - val_loss: 0.1441\n",
      "Epoch 678/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1605 - val_loss: 0.1440\n",
      "Epoch 679/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1574 - val_loss: 0.1441\n",
      "Epoch 680/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1581 - val_loss: 0.1440\n",
      "Epoch 681/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1563 - val_loss: 0.1435\n",
      "Epoch 682/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1438\n",
      "Epoch 683/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1607 - val_loss: 0.1444\n",
      "Epoch 684/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1594 - val_loss: 0.1437\n",
      "Epoch 685/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1437\n",
      "Epoch 686/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1595 - val_loss: 0.1445\n",
      "Epoch 687/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1439\n",
      "Epoch 688/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1592 - val_loss: 0.1437\n",
      "Epoch 689/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1635 - val_loss: 0.1436\n",
      "Epoch 690/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1598 - val_loss: 0.1439\n",
      "Epoch 691/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1612 - val_loss: 0.1437\n",
      "Epoch 692/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1586 - val_loss: 0.1440\n",
      "Epoch 693/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1563 - val_loss: 0.1438\n",
      "Epoch 694/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1581 - val_loss: 0.1436\n",
      "Epoch 695/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1613 - val_loss: 0.1436\n",
      "Epoch 696/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1602 - val_loss: 0.1439\n",
      "Epoch 697/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1550 - val_loss: 0.1439\n",
      "Epoch 698/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1576 - val_loss: 0.1444\n",
      "Epoch 699/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1554 - val_loss: 0.1446\n",
      "Epoch 700/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1575 - val_loss: 0.1436\n",
      "Epoch 701/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1600 - val_loss: 0.1437\n",
      "Epoch 702/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1582 - val_loss: 0.1435\n",
      "Epoch 703/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1567 - val_loss: 0.1441\n",
      "Epoch 704/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1545 - val_loss: 0.1436\n",
      "Epoch 705/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1439\n",
      "Epoch 706/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1547 - val_loss: 0.1440\n",
      "Epoch 707/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1571 - val_loss: 0.1436\n",
      "Epoch 708/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1584 - val_loss: 0.1437\n",
      "Epoch 709/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1593 - val_loss: 0.1440\n",
      "Epoch 710/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1582 - val_loss: 0.1438\n",
      "Epoch 711/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1571 - val_loss: 0.1439\n",
      "\n",
      "Epoch 00711: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 712/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1558 - val_loss: 0.1436\n",
      "Epoch 713/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1556 - val_loss: 0.1434\n",
      "Epoch 714/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1578 - val_loss: 0.1436\n",
      "Epoch 715/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1548 - val_loss: 0.1433\n",
      "Epoch 716/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1595 - val_loss: 0.1436\n",
      "Epoch 717/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1559 - val_loss: 0.1436\n",
      "Epoch 718/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1569 - val_loss: 0.1434\n",
      "Epoch 719/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1562 - val_loss: 0.1436\n",
      "Epoch 720/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1438\n",
      "Epoch 721/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1552 - val_loss: 0.1434\n",
      "Epoch 722/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1620 - val_loss: 0.1437\n",
      "Epoch 723/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1571 - val_loss: 0.1435\n",
      "Epoch 724/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1576 - val_loss: 0.1434\n",
      "Epoch 725/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1433\n",
      "Epoch 726/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1435\n",
      "Epoch 727/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1434\n",
      "Epoch 728/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1550 - val_loss: 0.1436\n",
      "Epoch 729/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1593 - val_loss: 0.1434\n",
      "Epoch 730/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1588 - val_loss: 0.1433\n",
      "Epoch 731/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1557 - val_loss: 0.1435\n",
      "Epoch 732/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1432\n",
      "Epoch 733/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1551 - val_loss: 0.1433\n",
      "Epoch 734/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1596 - val_loss: 0.1433\n",
      "Epoch 735/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1555 - val_loss: 0.1432\n",
      "Epoch 736/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1545 - val_loss: 0.1435\n",
      "Epoch 737/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1596 - val_loss: 0.1438\n",
      "Epoch 738/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1566 - val_loss: 0.1435\n",
      "Epoch 739/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1433\n",
      "Epoch 740/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1437\n",
      "Epoch 741/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1589 - val_loss: 0.1432\n",
      "Epoch 742/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1621 - val_loss: 0.1433\n",
      "Epoch 743/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1574 - val_loss: 0.1433\n",
      "Epoch 744/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1603 - val_loss: 0.1433\n",
      "Epoch 745/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1527 - val_loss: 0.1436\n",
      "Epoch 746/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1574 - val_loss: 0.1437\n",
      "Epoch 747/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1610 - val_loss: 0.1435\n",
      "Epoch 748/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1575 - val_loss: 0.1434\n",
      "Epoch 749/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1594 - val_loss: 0.1433\n",
      "Epoch 750/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1552 - val_loss: 0.1433\n",
      "Epoch 751/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1531 - val_loss: 0.1435\n",
      "Epoch 752/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1554 - val_loss: 0.1434\n",
      "Epoch 753/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1616 - val_loss: 0.1432\n",
      "Epoch 754/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1618 - val_loss: 0.1432\n",
      "Epoch 755/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1569 - val_loss: 0.1434\n",
      "Epoch 756/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1583 - val_loss: 0.1433\n",
      "Epoch 757/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1552 - val_loss: 0.1432\n",
      "Epoch 758/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1558 - val_loss: 0.1434\n",
      "Epoch 759/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1589 - val_loss: 0.1432\n",
      "Epoch 760/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1572 - val_loss: 0.1435\n",
      "Epoch 761/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1598 - val_loss: 0.1436\n",
      "Epoch 762/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1573 - val_loss: 0.1432\n",
      "Epoch 763/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1623 - val_loss: 0.1432\n",
      "Epoch 764/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1585 - val_loss: 0.1438\n",
      "Epoch 765/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1560 - val_loss: 0.1432\n",
      "\n",
      "Epoch 00765: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 766/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1561 - val_loss: 0.1432\n",
      "Epoch 767/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1588 - val_loss: 0.1431\n",
      "Epoch 768/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1604 - val_loss: 0.1437\n",
      "Epoch 769/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1606 - val_loss: 0.1436\n",
      "Epoch 770/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1579 - val_loss: 0.1431\n",
      "Epoch 771/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1583 - val_loss: 0.1433\n",
      "Epoch 772/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1431\n",
      "Epoch 773/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1574 - val_loss: 0.1431\n",
      "Epoch 774/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1602 - val_loss: 0.1431\n",
      "Epoch 775/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1598 - val_loss: 0.1433\n",
      "Epoch 776/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1583 - val_loss: 0.1432\n",
      "Epoch 777/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1593 - val_loss: 0.1434\n",
      "Epoch 778/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1602 - val_loss: 0.1430\n",
      "Epoch 779/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1627 - val_loss: 0.1432\n",
      "Epoch 780/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1586 - val_loss: 0.1434\n",
      "Epoch 781/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1553 - val_loss: 0.1430\n",
      "Epoch 782/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1606 - val_loss: 0.1432\n",
      "Epoch 783/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1609 - val_loss: 0.1436\n",
      "Epoch 784/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1587 - val_loss: 0.1432\n",
      "Epoch 785/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1557 - val_loss: 0.1430\n",
      "Epoch 786/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1563 - val_loss: 0.1431\n",
      "Epoch 787/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1568 - val_loss: 0.1432\n",
      "Epoch 788/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1540 - val_loss: 0.1432\n",
      "Epoch 789/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1557 - val_loss: 0.1431\n",
      "Epoch 790/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1571 - val_loss: 0.1433\n",
      "Epoch 791/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1554 - val_loss: 0.1432\n",
      "Epoch 792/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1571 - val_loss: 0.1432\n",
      "Epoch 793/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1555 - val_loss: 0.1432\n",
      "Epoch 794/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1561 - val_loss: 0.1431\n",
      "Epoch 795/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1610 - val_loss: 0.1436\n",
      "Epoch 796/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1552 - val_loss: 0.1431\n",
      "Epoch 797/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1588 - val_loss: 0.1433\n",
      "Epoch 798/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1611 - val_loss: 0.1442\n",
      "Epoch 799/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1564 - val_loss: 0.1430\n",
      "Epoch 800/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1589 - val_loss: 0.1430\n",
      "Epoch 801/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1587 - val_loss: 0.1430\n",
      "Epoch 802/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1594 - val_loss: 0.1437\n",
      "\n",
      "Epoch 00802: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 803/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1573 - val_loss: 0.1433\n",
      "Epoch 804/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1565 - val_loss: 0.1430\n",
      "Epoch 805/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1572 - val_loss: 0.1434\n",
      "Epoch 806/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1570 - val_loss: 0.1432\n",
      "Epoch 807/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1578 - val_loss: 0.1437\n",
      "Epoch 808/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1560 - val_loss: 0.1431\n",
      "Epoch 809/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1592 - val_loss: 0.1430\n",
      "Epoch 810/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1431\n",
      "Epoch 811/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1431\n",
      "Epoch 812/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1610 - val_loss: 0.1432\n",
      "Epoch 813/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1602 - val_loss: 0.1432\n",
      "Epoch 814/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1562 - val_loss: 0.1432\n",
      "Epoch 815/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1558 - val_loss: 0.1430\n",
      "Epoch 816/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1582 - val_loss: 0.1430\n",
      "Epoch 817/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1572 - val_loss: 0.1431\n",
      "Epoch 818/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1558 - val_loss: 0.1430\n",
      "Epoch 819/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1596 - val_loss: 0.1438\n",
      "Epoch 820/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1580 - val_loss: 0.1431\n",
      "Epoch 821/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1610 - val_loss: 0.1432\n",
      "Epoch 822/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1558 - val_loss: 0.1431\n",
      "Epoch 823/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1548 - val_loss: 0.1434\n",
      "Epoch 824/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1571 - val_loss: 0.1430\n",
      "Epoch 825/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1592 - val_loss: 0.1430\n",
      "Epoch 826/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1601 - val_loss: 0.1431\n",
      "Epoch 827/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1551 - val_loss: 0.1433\n",
      "Epoch 828/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1614 - val_loss: 0.1430\n",
      "Epoch 829/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1569 - val_loss: 0.1430\n",
      "Epoch 830/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1608 - val_loss: 0.1437\n",
      "Epoch 831/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1578 - val_loss: 0.1431\n",
      "Epoch 832/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1546 - val_loss: 0.1433\n",
      "Epoch 833/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1554 - val_loss: 0.1430\n",
      "Epoch 834/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1590 - val_loss: 0.1430\n",
      "Epoch 835/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1616 - val_loss: 0.1431\n",
      "Epoch 836/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1581 - val_loss: 0.1430\n",
      "Epoch 837/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1562 - val_loss: 0.1432\n",
      "Epoch 838/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1431\n",
      "Epoch 839/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1549 - val_loss: 0.1429\n",
      "Epoch 840/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1570 - val_loss: 0.1431\n",
      "Epoch 841/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1559 - val_loss: 0.1432\n",
      "Epoch 842/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1608 - val_loss: 0.1430\n",
      "Epoch 843/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1557 - val_loss: 0.1433\n",
      "Epoch 844/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1579 - val_loss: 0.1431\n",
      "Epoch 845/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1584 - val_loss: 0.1434\n",
      "Epoch 846/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1581 - val_loss: 0.1431\n",
      "Epoch 847/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1587 - val_loss: 0.1429\n",
      "Epoch 848/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1566 - val_loss: 0.1431\n",
      "\n",
      "Epoch 00848: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 849/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1558 - val_loss: 0.1435\n",
      "Epoch 850/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1554 - val_loss: 0.1430\n",
      "Epoch 851/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1591 - val_loss: 0.1432\n",
      "Epoch 852/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1564 - val_loss: 0.1432\n",
      "Epoch 853/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1564 - val_loss: 0.1430\n",
      "Epoch 854/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1571 - val_loss: 0.1434\n",
      "Epoch 855/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1570 - val_loss: 0.1429\n",
      "Epoch 856/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1619 - val_loss: 0.1431\n",
      "Epoch 857/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1578 - val_loss: 0.1438\n",
      "Epoch 858/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1562 - val_loss: 0.1430\n",
      "Epoch 859/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1601 - val_loss: 0.1430\n",
      "Epoch 860/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1577 - val_loss: 0.1430\n",
      "Epoch 861/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1562 - val_loss: 0.1445\n",
      "Epoch 862/2000\n",
      "472508/472508 [==============================] - 5s 10us/step - loss: 0.1576 - val_loss: 0.1431\n",
      "Epoch 863/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1556 - val_loss: 0.1430\n",
      "Epoch 864/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1552 - val_loss: 0.1432\n",
      "Epoch 865/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1604 - val_loss: 0.1430\n",
      "Epoch 866/2000\n",
      "472508/472508 [==============================] - 4s 10us/step - loss: 0.1580 - val_loss: 0.1430\n",
      "Epoch 867/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1604 - val_loss: 0.1431\n",
      "Epoch 868/2000\n",
      "472508/472508 [==============================] - 4s 9us/step - loss: 0.1552 - val_loss: 0.1432\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00868: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAzH8LKNx23P+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.944956146704153\n",
      "Training 3JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 1207321 samples, validate on 303058 samples\n",
      "Epoch 1/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.9360 - val_loss: 0.7089\n",
      "Epoch 2/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.6435 - val_loss: 0.6394\n",
      "Epoch 3/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.5941 - val_loss: 0.5491\n",
      "Epoch 4/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.5610 - val_loss: 0.5597\n",
      "Epoch 5/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.5410 - val_loss: 0.5335\n",
      "Epoch 6/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.5244 - val_loss: 0.5233\n",
      "Epoch 7/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.5098 - val_loss: 0.5009\n",
      "Epoch 8/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.5013 - val_loss: 0.4966\n",
      "Epoch 9/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4910 - val_loss: 0.4741\n",
      "Epoch 10/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4810 - val_loss: 0.5177\n",
      "Epoch 11/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4735 - val_loss: 0.4729\n",
      "Epoch 12/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4687 - val_loss: 0.4605\n",
      "Epoch 13/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4611 - val_loss: 0.4515\n",
      "Epoch 14/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4565 - val_loss: 0.4385\n",
      "Epoch 15/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4519 - val_loss: 0.4440\n",
      "Epoch 16/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4453 - val_loss: 0.4363\n",
      "Epoch 17/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4419 - val_loss: 0.4292\n",
      "Epoch 18/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4396 - val_loss: 0.4362\n",
      "Epoch 19/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4354 - val_loss: 0.4357\n",
      "Epoch 20/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4309 - val_loss: 0.4146\n",
      "Epoch 21/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4278 - val_loss: 0.4228\n",
      "Epoch 22/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4245 - val_loss: 0.4052\n",
      "Epoch 23/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4210 - val_loss: 0.4183\n",
      "Epoch 24/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4205 - val_loss: 0.4130\n",
      "Epoch 25/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4167 - val_loss: 0.4139\n",
      "Epoch 26/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4135 - val_loss: 0.3973\n",
      "Epoch 27/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4121 - val_loss: 0.4234\n",
      "Epoch 28/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4106 - val_loss: 0.4043\n",
      "Epoch 29/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4076 - val_loss: 0.4222\n",
      "Epoch 30/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4073 - val_loss: 0.3980\n",
      "Epoch 31/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.4059 - val_loss: 0.4261\n",
      "Epoch 32/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4030 - val_loss: 0.4042\n",
      "Epoch 33/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.4009 - val_loss: 0.3936\n",
      "Epoch 34/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3996 - val_loss: 0.3909\n",
      "Epoch 35/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3985 - val_loss: 0.4090\n",
      "Epoch 36/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3962 - val_loss: 0.4004\n",
      "Epoch 37/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3943 - val_loss: 0.3933\n",
      "Epoch 38/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3944 - val_loss: 0.3856\n",
      "Epoch 39/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3924 - val_loss: 0.3872\n",
      "Epoch 40/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3903 - val_loss: 0.3751\n",
      "Epoch 41/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3895 - val_loss: 0.3874\n",
      "Epoch 42/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3880 - val_loss: 0.3895\n",
      "Epoch 43/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3883 - val_loss: 0.3836\n",
      "Epoch 44/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3856 - val_loss: 0.3968\n",
      "Epoch 45/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3844 - val_loss: 0.3826\n",
      "Epoch 46/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3842 - val_loss: 0.3855\n",
      "Epoch 47/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3825 - val_loss: 0.3875\n",
      "Epoch 48/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3828 - val_loss: 0.3881\n",
      "Epoch 49/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3809 - val_loss: 0.3903\n",
      "Epoch 50/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3799 - val_loss: 0.3713\n",
      "Epoch 51/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3795 - val_loss: 0.3815\n",
      "Epoch 52/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3781 - val_loss: 0.3908\n",
      "Epoch 53/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3779 - val_loss: 0.3749\n",
      "Epoch 54/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3769 - val_loss: 0.3719\n",
      "Epoch 55/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3739 - val_loss: 0.3672\n",
      "Epoch 56/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3741 - val_loss: 0.3729\n",
      "Epoch 57/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3748 - val_loss: 0.3739\n",
      "Epoch 58/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3718 - val_loss: 0.3657\n",
      "Epoch 59/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3722 - val_loss: 0.3681\n",
      "Epoch 60/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3710 - val_loss: 0.3699\n",
      "Epoch 61/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3715 - val_loss: 0.3623\n",
      "Epoch 62/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3694 - val_loss: 0.3608\n",
      "Epoch 63/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3685 - val_loss: 0.3618\n",
      "Epoch 64/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3685 - val_loss: 0.3604\n",
      "Epoch 65/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3681 - val_loss: 0.3750\n",
      "Epoch 66/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3671 - val_loss: 0.3687\n",
      "Epoch 67/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3671 - val_loss: 0.3630\n",
      "Epoch 68/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3654 - val_loss: 0.3740\n",
      "Epoch 69/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3668 - val_loss: 0.3590\n",
      "Epoch 70/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3646 - val_loss: 0.3578\n",
      "Epoch 71/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3627 - val_loss: 0.3803\n",
      "Epoch 72/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3644 - val_loss: 0.3628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3621 - val_loss: 0.3613\n",
      "Epoch 74/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3626 - val_loss: 0.3557\n",
      "Epoch 75/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3616 - val_loss: 0.3560\n",
      "Epoch 76/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3609 - val_loss: 0.3604\n",
      "Epoch 77/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3609 - val_loss: 0.3558\n",
      "Epoch 78/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3601 - val_loss: 0.3687\n",
      "Epoch 79/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3604 - val_loss: 0.3515\n",
      "Epoch 80/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3588 - val_loss: 0.3542\n",
      "Epoch 81/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3575 - val_loss: 0.3559\n",
      "Epoch 82/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3587 - val_loss: 0.3541\n",
      "Epoch 83/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3566 - val_loss: 0.3631\n",
      "Epoch 84/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3565 - val_loss: 0.3542\n",
      "Epoch 85/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3576 - val_loss: 0.3525\n",
      "Epoch 86/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3556 - val_loss: 0.3575\n",
      "Epoch 87/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3558 - val_loss: 0.3665\n",
      "Epoch 88/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3557 - val_loss: 0.3498\n",
      "Epoch 89/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3555 - val_loss: 0.3579\n",
      "Epoch 90/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3538 - val_loss: 0.3467\n",
      "Epoch 91/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3526 - val_loss: 0.3600\n",
      "Epoch 92/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3523 - val_loss: 0.3528\n",
      "Epoch 93/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3531 - val_loss: 0.3570\n",
      "Epoch 94/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3524 - val_loss: 0.3498\n",
      "Epoch 95/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3517 - val_loss: 0.3526\n",
      "Epoch 96/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3516 - val_loss: 0.3549\n",
      "Epoch 97/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3510 - val_loss: 0.3462\n",
      "Epoch 98/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3521 - val_loss: 0.3524\n",
      "Epoch 99/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3502 - val_loss: 0.3477\n",
      "Epoch 100/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3502 - val_loss: 0.3691\n",
      "Epoch 101/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3502 - val_loss: 0.3446\n",
      "Epoch 102/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3486 - val_loss: 0.3519\n",
      "Epoch 103/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3493 - val_loss: 0.3471\n",
      "Epoch 104/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3486 - val_loss: 0.3582\n",
      "Epoch 105/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3489 - val_loss: 0.3475\n",
      "Epoch 106/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3477 - val_loss: 0.3459\n",
      "Epoch 107/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3466 - val_loss: 0.3546\n",
      "Epoch 108/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3491 - val_loss: 0.3467\n",
      "Epoch 109/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3460 - val_loss: 0.3561\n",
      "Epoch 110/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3467 - val_loss: 0.3479\n",
      "Epoch 111/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3455 - val_loss: 0.3598\n",
      "Epoch 112/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3454 - val_loss: 0.3473\n",
      "Epoch 113/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3449 - val_loss: 0.3436\n",
      "Epoch 114/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3450 - val_loss: 0.3585\n",
      "Epoch 115/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3452 - val_loss: 0.3426\n",
      "Epoch 116/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3445 - val_loss: 0.3476\n",
      "Epoch 117/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3430 - val_loss: 0.3459\n",
      "Epoch 118/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3447 - val_loss: 0.3399\n",
      "Epoch 119/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3444 - val_loss: 0.3440\n",
      "Epoch 120/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3442 - val_loss: 0.3482\n",
      "Epoch 121/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3410 - val_loss: 0.3405\n",
      "Epoch 122/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3421 - val_loss: 0.3450\n",
      "Epoch 123/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3413 - val_loss: 0.3402\n",
      "Epoch 124/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3417 - val_loss: 0.3372\n",
      "Epoch 125/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3424 - val_loss: 0.3548\n",
      "Epoch 126/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3429 - val_loss: 0.3489\n",
      "Epoch 127/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3425 - val_loss: 0.3437\n",
      "Epoch 128/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3415 - val_loss: 0.3412\n",
      "Epoch 129/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3410 - val_loss: 0.3383\n",
      "Epoch 130/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3412 - val_loss: 0.3418\n",
      "Epoch 131/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3402 - val_loss: 0.3411\n",
      "Epoch 132/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3397 - val_loss: 0.3390\n",
      "Epoch 133/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3408 - val_loss: 0.3352\n",
      "Epoch 134/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3403 - val_loss: 0.3431\n",
      "Epoch 135/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3390 - val_loss: 0.3418\n",
      "Epoch 136/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3381 - val_loss: 0.3359\n",
      "Epoch 137/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3395 - val_loss: 0.3380\n",
      "Epoch 138/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3378 - val_loss: 0.3370\n",
      "Epoch 139/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3390 - val_loss: 0.3500\n",
      "Epoch 140/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3382 - val_loss: 0.3346\n",
      "Epoch 141/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3380 - val_loss: 0.3365\n",
      "Epoch 142/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3378 - val_loss: 0.3380\n",
      "Epoch 143/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3393 - val_loss: 0.3434\n",
      "Epoch 144/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3371 - val_loss: 0.3381\n",
      "Epoch 145/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3365 - val_loss: 0.3392\n",
      "Epoch 146/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3372 - val_loss: 0.3370\n",
      "Epoch 147/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3368 - val_loss: 0.3376\n",
      "Epoch 148/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3359 - val_loss: 0.3500\n",
      "Epoch 149/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3371 - val_loss: 0.3513\n",
      "Epoch 150/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3355 - val_loss: 0.3400\n",
      "Epoch 151/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3362 - val_loss: 0.3310\n",
      "Epoch 152/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3357 - val_loss: 0.3359\n",
      "Epoch 153/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3352 - val_loss: 0.3366\n",
      "Epoch 154/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3350 - val_loss: 0.3329\n",
      "Epoch 155/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3349 - val_loss: 0.3394\n",
      "Epoch 156/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3356 - val_loss: 0.3353\n",
      "Epoch 157/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3352 - val_loss: 0.3457\n",
      "Epoch 158/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3340 - val_loss: 0.3310\n",
      "Epoch 159/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3331 - val_loss: 0.3365\n",
      "Epoch 160/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3335 - val_loss: 0.3312\n",
      "Epoch 161/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3320 - val_loss: 0.3324\n",
      "Epoch 162/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3336 - val_loss: 0.3375\n",
      "Epoch 163/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3337 - val_loss: 0.3434\n",
      "Epoch 164/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3329 - val_loss: 0.3340\n",
      "Epoch 165/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3321 - val_loss: 0.3363\n",
      "Epoch 166/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3332 - val_loss: 0.3371\n",
      "Epoch 167/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3323 - val_loss: 0.3385\n",
      "Epoch 168/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3319 - val_loss: 0.3333\n",
      "Epoch 169/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3318 - val_loss: 0.3372\n",
      "Epoch 170/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3327 - val_loss: 0.3354\n",
      "Epoch 171/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3316 - val_loss: 0.3300\n",
      "Epoch 172/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3307 - val_loss: 0.3277\n",
      "Epoch 173/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3305 - val_loss: 0.3317\n",
      "Epoch 174/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3309 - val_loss: 0.3353\n",
      "Epoch 175/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3315 - val_loss: 0.3354\n",
      "Epoch 176/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3307 - val_loss: 0.3332\n",
      "Epoch 177/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3315 - val_loss: 0.3297\n",
      "Epoch 178/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3300 - val_loss: 0.3320\n",
      "Epoch 179/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3288 - val_loss: 0.3381\n",
      "Epoch 180/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3326 - val_loss: 0.3355\n",
      "Epoch 181/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3313 - val_loss: 0.3249\n",
      "Epoch 182/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3288 - val_loss: 0.3482\n",
      "Epoch 183/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3296 - val_loss: 0.3279\n",
      "Epoch 184/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3298 - val_loss: 0.3336\n",
      "Epoch 185/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3303 - val_loss: 0.3383\n",
      "Epoch 186/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3284 - val_loss: 0.3336\n",
      "Epoch 187/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3290 - val_loss: 0.3286\n",
      "Epoch 188/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3303 - val_loss: 0.3280\n",
      "Epoch 189/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3296 - val_loss: 0.3313\n",
      "Epoch 190/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3262 - val_loss: 0.3288\n",
      "Epoch 191/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3299 - val_loss: 0.3305\n",
      "Epoch 192/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3275 - val_loss: 0.3313\n",
      "Epoch 193/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3283 - val_loss: 0.3401\n",
      "Epoch 194/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3274 - val_loss: 0.3266\n",
      "Epoch 195/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3265 - val_loss: 0.3298\n",
      "Epoch 196/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3290 - val_loss: 0.3377\n",
      "Epoch 197/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3263 - val_loss: 0.3370\n",
      "Epoch 198/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3277 - val_loss: 0.3280\n",
      "Epoch 199/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3273 - val_loss: 0.3319\n",
      "Epoch 200/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3263 - val_loss: 0.3304\n",
      "Epoch 201/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3271 - val_loss: 0.3303\n",
      "Epoch 202/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3278 - val_loss: 0.3263\n",
      "Epoch 203/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3263 - val_loss: 0.3249\n",
      "Epoch 204/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3255 - val_loss: 0.3274\n",
      "Epoch 205/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3252 - val_loss: 0.3242\n",
      "Epoch 206/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3271 - val_loss: 0.3275\n",
      "Epoch 207/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3248 - val_loss: 0.3314\n",
      "Epoch 208/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3258 - val_loss: 0.3316\n",
      "Epoch 209/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3259 - val_loss: 0.3290\n",
      "Epoch 210/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3270 - val_loss: 0.3248\n",
      "Epoch 211/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3250 - val_loss: 0.3281\n",
      "Epoch 212/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3254 - val_loss: 0.3292\n",
      "Epoch 213/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3263 - val_loss: 0.3291\n",
      "Epoch 214/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3249 - val_loss: 0.3249\n",
      "Epoch 215/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3243 - val_loss: 0.3226\n",
      "Epoch 216/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3239 - val_loss: 0.3350\n",
      "Epoch 217/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3250 - val_loss: 0.3308\n",
      "Epoch 218/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3256 - val_loss: 0.3252\n",
      "Epoch 219/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3245 - val_loss: 0.3311\n",
      "Epoch 220/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3246 - val_loss: 0.3236\n",
      "Epoch 221/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3236 - val_loss: 0.3251\n",
      "Epoch 222/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3246 - val_loss: 0.3238\n",
      "Epoch 223/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3237 - val_loss: 0.3225\n",
      "Epoch 224/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3237 - val_loss: 0.3262\n",
      "Epoch 225/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3238 - val_loss: 0.3282\n",
      "Epoch 226/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3228 - val_loss: 0.3253\n",
      "Epoch 227/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3239 - val_loss: 0.3220\n",
      "Epoch 228/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3227 - val_loss: 0.3390\n",
      "Epoch 229/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3231 - val_loss: 0.3219\n",
      "Epoch 230/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3226 - val_loss: 0.3251\n",
      "Epoch 231/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3234 - val_loss: 0.3283\n",
      "Epoch 232/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3232 - val_loss: 0.3352\n",
      "Epoch 233/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3228 - val_loss: 0.3243\n",
      "Epoch 234/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3232 - val_loss: 0.3266\n",
      "Epoch 235/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3221 - val_loss: 0.3273\n",
      "Epoch 236/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3221 - val_loss: 0.3236\n",
      "Epoch 237/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3226 - val_loss: 0.3348\n",
      "Epoch 238/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3211 - val_loss: 0.3260\n",
      "Epoch 239/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3215 - val_loss: 0.3286\n",
      "Epoch 240/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3212 - val_loss: 0.3290\n",
      "Epoch 241/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3217 - val_loss: 0.3226\n",
      "Epoch 242/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3232 - val_loss: 0.3211\n",
      "Epoch 243/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3213 - val_loss: 0.3229\n",
      "Epoch 244/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3209 - val_loss: 0.3280\n",
      "Epoch 245/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3195 - val_loss: 0.3332\n",
      "Epoch 246/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3212 - val_loss: 0.3313\n",
      "Epoch 247/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3214 - val_loss: 0.3341\n",
      "Epoch 248/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3202 - val_loss: 0.3225\n",
      "Epoch 249/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3198 - val_loss: 0.3234\n",
      "Epoch 250/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3200 - val_loss: 0.3230\n",
      "Epoch 251/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3204 - val_loss: 0.3275\n",
      "Epoch 252/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3191 - val_loss: 0.3248\n",
      "Epoch 253/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3202 - val_loss: 0.3305\n",
      "Epoch 254/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3195 - val_loss: 0.3277\n",
      "Epoch 255/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3198 - val_loss: 0.3218\n",
      "Epoch 256/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3195 - val_loss: 0.3182\n",
      "Epoch 257/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3199 - val_loss: 0.3200\n",
      "Epoch 258/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3186 - val_loss: 0.3194\n",
      "Epoch 259/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3196 - val_loss: 0.3214\n",
      "Epoch 260/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3190 - val_loss: 0.3222\n",
      "Epoch 261/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3210 - val_loss: 0.3212\n",
      "Epoch 262/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3179 - val_loss: 0.3207\n",
      "Epoch 263/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3185 - val_loss: 0.3190\n",
      "Epoch 264/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3184 - val_loss: 0.3201\n",
      "Epoch 265/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3192 - val_loss: 0.3184\n",
      "Epoch 266/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3178 - val_loss: 0.3224\n",
      "Epoch 267/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3177 - val_loss: 0.3223\n",
      "Epoch 268/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3185 - val_loss: 0.3204\n",
      "Epoch 269/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3188 - val_loss: 0.3200\n",
      "Epoch 270/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3174 - val_loss: 0.3168\n",
      "Epoch 271/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3188 - val_loss: 0.3232\n",
      "Epoch 272/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3175 - val_loss: 0.3239\n",
      "Epoch 273/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3184 - val_loss: 0.3268\n",
      "Epoch 274/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3181 - val_loss: 0.3208\n",
      "Epoch 275/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3179 - val_loss: 0.3230\n",
      "Epoch 276/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3191 - val_loss: 0.3261\n",
      "Epoch 277/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3183 - val_loss: 0.3240\n",
      "Epoch 278/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3176 - val_loss: 0.3273\n",
      "Epoch 279/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3181 - val_loss: 0.3364\n",
      "Epoch 280/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3182 - val_loss: 0.3215\n",
      "Epoch 281/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3177 - val_loss: 0.3190\n",
      "Epoch 282/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3180 - val_loss: 0.3176\n",
      "Epoch 283/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3167 - val_loss: 0.3313\n",
      "Epoch 284/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3163 - val_loss: 0.3226\n",
      "Epoch 285/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3170 - val_loss: 0.3209\n",
      "Epoch 286/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3175 - val_loss: 0.3172\n",
      "Epoch 287/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3177 - val_loss: 0.3196\n",
      "Epoch 288/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3172 - val_loss: 0.3216\n",
      "Epoch 289/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3153 - val_loss: 0.3226\n",
      "Epoch 290/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3169 - val_loss: 0.3216\n",
      "Epoch 291/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3168 - val_loss: 0.3176\n",
      "Epoch 292/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3165 - val_loss: 0.3165\n",
      "Epoch 293/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3156 - val_loss: 0.3229\n",
      "Epoch 294/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3171 - val_loss: 0.3287\n",
      "Epoch 295/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3159 - val_loss: 0.3183\n",
      "Epoch 296/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3162 - val_loss: 0.3194\n",
      "Epoch 297/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3156 - val_loss: 0.3211\n",
      "Epoch 298/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3149 - val_loss: 0.3166\n",
      "Epoch 299/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3159 - val_loss: 0.3199\n",
      "Epoch 300/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3153 - val_loss: 0.3187\n",
      "Epoch 301/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3145 - val_loss: 0.3167\n",
      "Epoch 302/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3163 - val_loss: 0.3205\n",
      "Epoch 303/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3149 - val_loss: 0.3258\n",
      "Epoch 304/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3156 - val_loss: 0.3186\n",
      "Epoch 305/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3152 - val_loss: 0.3148\n",
      "Epoch 306/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3153 - val_loss: 0.3195\n",
      "Epoch 307/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3149 - val_loss: 0.3238\n",
      "Epoch 308/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3148 - val_loss: 0.3218\n",
      "Epoch 309/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3162 - val_loss: 0.3180\n",
      "Epoch 310/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3152 - val_loss: 0.3178\n",
      "Epoch 311/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3141 - val_loss: 0.3224\n",
      "Epoch 312/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3146 - val_loss: 0.3151\n",
      "Epoch 313/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3148 - val_loss: 0.3280\n",
      "Epoch 314/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3143 - val_loss: 0.3169\n",
      "Epoch 315/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3160 - val_loss: 0.3160\n",
      "Epoch 316/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3143 - val_loss: 0.3165\n",
      "Epoch 317/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3138 - val_loss: 0.3233\n",
      "Epoch 318/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3159 - val_loss: 0.3243\n",
      "Epoch 319/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3143 - val_loss: 0.3222\n",
      "Epoch 320/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3144 - val_loss: 0.3203\n",
      "Epoch 321/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3142 - val_loss: 0.3237\n",
      "Epoch 322/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3145 - val_loss: 0.3219\n",
      "Epoch 323/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3150 - val_loss: 0.3154\n",
      "Epoch 324/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3145 - val_loss: 0.3174\n",
      "Epoch 325/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3155 - val_loss: 0.3300\n",
      "Epoch 326/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3131 - val_loss: 0.3206\n",
      "Epoch 327/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3139 - val_loss: 0.3209\n",
      "Epoch 328/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3160 - val_loss: 0.3248\n",
      "Epoch 329/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3131 - val_loss: 0.3199\n",
      "Epoch 330/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3141 - val_loss: 0.3218\n",
      "Epoch 331/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3130 - val_loss: 0.3157\n",
      "Epoch 332/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3134 - val_loss: 0.3182\n",
      "Epoch 333/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3128 - val_loss: 0.3251\n",
      "Epoch 334/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3140 - val_loss: 0.3131\n",
      "Epoch 335/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3128 - val_loss: 0.3214\n",
      "Epoch 336/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3124 - val_loss: 0.3130\n",
      "Epoch 337/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3146 - val_loss: 0.3135\n",
      "Epoch 338/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3123 - val_loss: 0.3290\n",
      "Epoch 339/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3118 - val_loss: 0.3129\n",
      "Epoch 340/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3122 - val_loss: 0.3198\n",
      "Epoch 341/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3124 - val_loss: 0.3164\n",
      "Epoch 342/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3122 - val_loss: 0.3262\n",
      "Epoch 343/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3131 - val_loss: 0.3181\n",
      "Epoch 344/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3127 - val_loss: 0.3126\n",
      "Epoch 345/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3133 - val_loss: 0.3162\n",
      "Epoch 346/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3125 - val_loss: 0.3218\n",
      "Epoch 347/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3128 - val_loss: 0.3132\n",
      "Epoch 348/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3106 - val_loss: 0.3281\n",
      "Epoch 349/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3112 - val_loss: 0.3157\n",
      "Epoch 350/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3132 - val_loss: 0.3178\n",
      "Epoch 351/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3118 - val_loss: 0.3138\n",
      "Epoch 352/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3120 - val_loss: 0.3170\n",
      "Epoch 353/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3125 - val_loss: 0.3214\n",
      "Epoch 354/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3118 - val_loss: 0.3219\n",
      "Epoch 355/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3112 - val_loss: 0.3243\n",
      "Epoch 356/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3123 - val_loss: 0.3121\n",
      "Epoch 357/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3128 - val_loss: 0.3131\n",
      "Epoch 358/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3116 - val_loss: 0.3166\n",
      "Epoch 359/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3115 - val_loss: 0.3137\n",
      "Epoch 360/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3115 - val_loss: 0.3180\n",
      "Epoch 361/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3102 - val_loss: 0.3220\n",
      "Epoch 362/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3101 - val_loss: 0.3133\n",
      "Epoch 363/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3102 - val_loss: 0.3144\n",
      "Epoch 364/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3114 - val_loss: 0.3225\n",
      "Epoch 365/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3097 - val_loss: 0.3114\n",
      "Epoch 366/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3108 - val_loss: 0.3140\n",
      "Epoch 367/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3109 - val_loss: 0.3192\n",
      "Epoch 368/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3104 - val_loss: 0.3190\n",
      "Epoch 369/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3110 - val_loss: 0.3189\n",
      "Epoch 370/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3126 - val_loss: 0.3141\n",
      "Epoch 371/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3103 - val_loss: 0.3140\n",
      "Epoch 372/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3101 - val_loss: 0.3240\n",
      "Epoch 373/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3107 - val_loss: 0.3167\n",
      "Epoch 374/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3097 - val_loss: 0.3110\n",
      "Epoch 375/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3112 - val_loss: 0.3164\n",
      "Epoch 376/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3102 - val_loss: 0.3125\n",
      "Epoch 377/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3103 - val_loss: 0.3204\n",
      "Epoch 378/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3095 - val_loss: 0.3174\n",
      "Epoch 379/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3096 - val_loss: 0.3135\n",
      "Epoch 380/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3103 - val_loss: 0.3219\n",
      "Epoch 381/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3091 - val_loss: 0.3122\n",
      "Epoch 382/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3105 - val_loss: 0.3158\n",
      "Epoch 383/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3090 - val_loss: 0.3111\n",
      "Epoch 384/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3096 - val_loss: 0.3148\n",
      "Epoch 385/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3112 - val_loss: 0.3190\n",
      "Epoch 386/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3086 - val_loss: 0.3174\n",
      "Epoch 387/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3100 - val_loss: 0.3149\n",
      "Epoch 388/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3088 - val_loss: 0.3148\n",
      "Epoch 389/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3090 - val_loss: 0.3146\n",
      "Epoch 390/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3093 - val_loss: 0.3137\n",
      "Epoch 391/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3091 - val_loss: 0.3165\n",
      "Epoch 392/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3097 - val_loss: 0.3163\n",
      "Epoch 393/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3090 - val_loss: 0.3091\n",
      "Epoch 394/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3085 - val_loss: 0.3133\n",
      "Epoch 395/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3091 - val_loss: 0.3130\n",
      "Epoch 396/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3074 - val_loss: 0.3113\n",
      "Epoch 397/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3097 - val_loss: 0.3125\n",
      "Epoch 398/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3093 - val_loss: 0.3208\n",
      "Epoch 399/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3089 - val_loss: 0.3166\n",
      "Epoch 400/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3094 - val_loss: 0.3213\n",
      "Epoch 401/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3093 - val_loss: 0.3239\n",
      "Epoch 402/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3084 - val_loss: 0.3225\n",
      "Epoch 403/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3085 - val_loss: 0.3162\n",
      "Epoch 404/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3096 - val_loss: 0.3140\n",
      "Epoch 405/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3090 - val_loss: 0.3143\n",
      "Epoch 406/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3080 - val_loss: 0.3150\n",
      "Epoch 407/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3086 - val_loss: 0.3214\n",
      "Epoch 408/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3084 - val_loss: 0.3137\n",
      "Epoch 409/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3073 - val_loss: 0.3210\n",
      "Epoch 410/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3086 - val_loss: 0.3288\n",
      "Epoch 411/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3087 - val_loss: 0.3138\n",
      "Epoch 412/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3084 - val_loss: 0.3098\n",
      "Epoch 413/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3087 - val_loss: 0.3235\n",
      "Epoch 414/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3086 - val_loss: 0.3144\n",
      "Epoch 415/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3072 - val_loss: 0.3146\n",
      "Epoch 416/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3075 - val_loss: 0.3106\n",
      "Epoch 417/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3071 - val_loss: 0.3206\n",
      "Epoch 418/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3078 - val_loss: 0.3194\n",
      "Epoch 419/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3069 - val_loss: 0.3127\n",
      "Epoch 420/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3080 - val_loss: 0.3175\n",
      "Epoch 421/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3082 - val_loss: 0.3082\n",
      "Epoch 422/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3074 - val_loss: 0.3185\n",
      "Epoch 423/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3077 - val_loss: 0.3105\n",
      "Epoch 424/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3066 - val_loss: 0.3146\n",
      "Epoch 425/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3074 - val_loss: 0.3085\n",
      "Epoch 426/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3067 - val_loss: 0.3093\n",
      "Epoch 427/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3077 - val_loss: 0.3188\n",
      "Epoch 428/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3071 - val_loss: 0.3087\n",
      "Epoch 429/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3069 - val_loss: 0.3138\n",
      "Epoch 430/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3075 - val_loss: 0.3152\n",
      "Epoch 431/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3069 - val_loss: 0.3075\n",
      "Epoch 432/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3081 - val_loss: 0.3160\n",
      "Epoch 433/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3076 - val_loss: 0.3099\n",
      "Epoch 434/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3067 - val_loss: 0.3073\n",
      "Epoch 435/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3070 - val_loss: 0.3108\n",
      "Epoch 436/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3070 - val_loss: 0.3179\n",
      "Epoch 437/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3061 - val_loss: 0.3101\n",
      "Epoch 438/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3079 - val_loss: 0.3107\n",
      "Epoch 439/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3061 - val_loss: 0.3089\n",
      "Epoch 440/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3070 - val_loss: 0.3113\n",
      "Epoch 441/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3066 - val_loss: 0.3108\n",
      "Epoch 442/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3058 - val_loss: 0.3096\n",
      "Epoch 443/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3065 - val_loss: 0.3128\n",
      "Epoch 444/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3068 - val_loss: 0.3115\n",
      "Epoch 445/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3063 - val_loss: 0.3141\n",
      "Epoch 446/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3063 - val_loss: 0.3180\n",
      "Epoch 447/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3061 - val_loss: 0.3108\n",
      "Epoch 448/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3068 - val_loss: 0.3208\n",
      "Epoch 449/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3053 - val_loss: 0.3186\n",
      "Epoch 450/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3058 - val_loss: 0.3200\n",
      "Epoch 451/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3071 - val_loss: 0.3132\n",
      "Epoch 452/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3052 - val_loss: 0.3190\n",
      "Epoch 453/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3068 - val_loss: 0.3198\n",
      "Epoch 454/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3062 - val_loss: 0.3253\n",
      "Epoch 455/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3071 - val_loss: 0.3126\n",
      "Epoch 456/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3046 - val_loss: 0.3120\n",
      "Epoch 457/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3055 - val_loss: 0.3105\n",
      "Epoch 458/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3051 - val_loss: 0.3088\n",
      "Epoch 459/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3069 - val_loss: 0.3103\n",
      "Epoch 460/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3046 - val_loss: 0.3127\n",
      "Epoch 461/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3062 - val_loss: 0.3091\n",
      "Epoch 462/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3065 - val_loss: 0.3083\n",
      "Epoch 463/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.3066 - val_loss: 0.3096\n",
      "Epoch 464/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3056 - val_loss: 0.3064\n",
      "Epoch 465/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3039 - val_loss: 0.3110\n",
      "Epoch 466/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3053 - val_loss: 0.3076\n",
      "Epoch 467/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3047 - val_loss: 0.3085\n",
      "Epoch 468/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3053 - val_loss: 0.3120\n",
      "Epoch 469/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3063 - val_loss: 0.3158\n",
      "Epoch 470/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3053 - val_loss: 0.3106\n",
      "Epoch 471/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3048 - val_loss: 0.3070\n",
      "Epoch 472/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3055 - val_loss: 0.3127\n",
      "Epoch 473/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3059 - val_loss: 0.3096\n",
      "Epoch 474/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3056 - val_loss: 0.3129\n",
      "Epoch 475/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3049 - val_loss: 0.3239\n",
      "Epoch 476/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.3046 - val_loss: 0.3147\n",
      "Epoch 477/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3042 - val_loss: 0.3106\n",
      "Epoch 478/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3050 - val_loss: 0.3112\n",
      "Epoch 479/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3043 - val_loss: 0.3132\n",
      "Epoch 480/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3045 - val_loss: 0.3063\n",
      "Epoch 481/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3055 - val_loss: 0.3102\n",
      "Epoch 482/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3042 - val_loss: 0.3073\n",
      "Epoch 483/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3043 - val_loss: 0.3072\n",
      "Epoch 484/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3056 - val_loss: 0.3126\n",
      "Epoch 485/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3043 - val_loss: 0.3141\n",
      "Epoch 486/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3036 - val_loss: 0.3167\n",
      "Epoch 487/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3045 - val_loss: 0.3101\n",
      "Epoch 488/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3036 - val_loss: 0.3138\n",
      "Epoch 489/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3039 - val_loss: 0.3068\n",
      "Epoch 490/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3031 - val_loss: 0.3145\n",
      "Epoch 491/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3053 - val_loss: 0.3094\n",
      "Epoch 492/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3032 - val_loss: 0.3130\n",
      "Epoch 493/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3045 - val_loss: 0.3122\n",
      "Epoch 494/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.3066 - val_loss: 0.3136\n",
      "\n",
      "Epoch 00494: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 495/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2968 - val_loss: 0.2988\n",
      "Epoch 496/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2952 - val_loss: 0.2992\n",
      "Epoch 497/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2946 - val_loss: 0.2986\n",
      "Epoch 498/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2936 - val_loss: 0.2989\n",
      "Epoch 499/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2942 - val_loss: 0.3030\n",
      "Epoch 500/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2931 - val_loss: 0.2981\n",
      "Epoch 501/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2932 - val_loss: 0.2976\n",
      "Epoch 502/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2936 - val_loss: 0.2980\n",
      "Epoch 503/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2928 - val_loss: 0.3003\n",
      "Epoch 504/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2931 - val_loss: 0.2970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2921 - val_loss: 0.2991\n",
      "Epoch 506/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2933 - val_loss: 0.2987\n",
      "Epoch 507/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2926 - val_loss: 0.3002\n",
      "Epoch 508/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2930 - val_loss: 0.2980\n",
      "Epoch 509/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2924 - val_loss: 0.3012\n",
      "Epoch 510/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2922 - val_loss: 0.2989\n",
      "Epoch 511/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2919 - val_loss: 0.3011\n",
      "Epoch 512/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2926 - val_loss: 0.3028\n",
      "Epoch 513/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2922 - val_loss: 0.2978\n",
      "Epoch 514/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2931 - val_loss: 0.2996\n",
      "Epoch 515/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2928 - val_loss: 0.2963\n",
      "Epoch 516/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2918 - val_loss: 0.2986\n",
      "Epoch 517/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2925 - val_loss: 0.2965\n",
      "Epoch 518/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2917 - val_loss: 0.3050\n",
      "Epoch 519/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2927 - val_loss: 0.2996\n",
      "Epoch 520/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2922 - val_loss: 0.2979\n",
      "Epoch 521/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2926 - val_loss: 0.2969\n",
      "Epoch 522/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2914 - val_loss: 0.2992\n",
      "Epoch 523/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2933 - val_loss: 0.2976\n",
      "Epoch 524/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2914 - val_loss: 0.2962\n",
      "Epoch 525/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2921 - val_loss: 0.2968\n",
      "Epoch 526/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2916 - val_loss: 0.2991\n",
      "Epoch 527/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2918 - val_loss: 0.2980\n",
      "Epoch 528/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2922 - val_loss: 0.2982\n",
      "Epoch 529/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2924 - val_loss: 0.2994\n",
      "Epoch 530/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2928 - val_loss: 0.2962\n",
      "Epoch 531/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2925 - val_loss: 0.3016\n",
      "Epoch 532/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.2974\n",
      "Epoch 533/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2922 - val_loss: 0.2958\n",
      "Epoch 534/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2923 - val_loss: 0.2958\n",
      "Epoch 535/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2923 - val_loss: 0.3008\n",
      "Epoch 536/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2912 - val_loss: 0.2968\n",
      "Epoch 537/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2906 - val_loss: 0.2978\n",
      "Epoch 538/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2914 - val_loss: 0.2954\n",
      "Epoch 539/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2928 - val_loss: 0.3016\n",
      "Epoch 540/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.3048\n",
      "Epoch 541/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2914 - val_loss: 0.2971\n",
      "Epoch 542/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2914 - val_loss: 0.3017\n",
      "Epoch 543/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2925 - val_loss: 0.2973\n",
      "Epoch 544/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2923 - val_loss: 0.2996\n",
      "Epoch 545/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2909 - val_loss: 0.2975\n",
      "Epoch 546/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2917 - val_loss: 0.2966\n",
      "Epoch 547/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2910 - val_loss: 0.2966\n",
      "Epoch 548/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2917 - val_loss: 0.2967\n",
      "Epoch 549/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2919 - val_loss: 0.2975\n",
      "Epoch 550/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2921 - val_loss: 0.3018\n",
      "Epoch 551/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2906 - val_loss: 0.3014\n",
      "Epoch 552/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2966\n",
      "Epoch 553/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2914 - val_loss: 0.2977\n",
      "Epoch 554/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2911 - val_loss: 0.2963\n",
      "Epoch 555/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.2981\n",
      "Epoch 556/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2903 - val_loss: 0.2984\n",
      "Epoch 557/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2921 - val_loss: 0.2966\n",
      "Epoch 558/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2922 - val_loss: 0.3023\n",
      "Epoch 559/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2907 - val_loss: 0.2959\n",
      "Epoch 560/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2976\n",
      "Epoch 561/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2904 - val_loss: 0.2965\n",
      "Epoch 562/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2901 - val_loss: 0.2987\n",
      "Epoch 563/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2926 - val_loss: 0.2953\n",
      "Epoch 564/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.2982\n",
      "Epoch 565/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2899 - val_loss: 0.2977\n",
      "Epoch 566/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2907 - val_loss: 0.2986\n",
      "Epoch 567/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2905 - val_loss: 0.3001\n",
      "Epoch 568/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2915 - val_loss: 0.2963\n",
      "Epoch 569/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2912 - val_loss: 0.3007\n",
      "Epoch 570/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.2983\n",
      "Epoch 571/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2903 - val_loss: 0.2981\n",
      "Epoch 572/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2917 - val_loss: 0.2976\n",
      "Epoch 573/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2919 - val_loss: 0.2966\n",
      "Epoch 574/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2909 - val_loss: 0.2972\n",
      "Epoch 575/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2913 - val_loss: 0.2970\n",
      "Epoch 576/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2906 - val_loss: 0.2965\n",
      "Epoch 577/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2920 - val_loss: 0.2958\n",
      "Epoch 578/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2903 - val_loss: 0.3011\n",
      "Epoch 579/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2914 - val_loss: 0.2954\n",
      "Epoch 580/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2913 - val_loss: 0.2968\n",
      "Epoch 581/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2909 - val_loss: 0.2979\n",
      "Epoch 582/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2897 - val_loss: 0.2943\n",
      "Epoch 583/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2899 - val_loss: 0.2954\n",
      "Epoch 584/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2902 - val_loss: 0.2983\n",
      "Epoch 585/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2928 - val_loss: 0.3013\n",
      "Epoch 586/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.2952\n",
      "Epoch 587/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2895 - val_loss: 0.2970\n",
      "Epoch 588/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2900 - val_loss: 0.2979\n",
      "Epoch 589/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.3015\n",
      "Epoch 590/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2913 - val_loss: 0.2985\n",
      "Epoch 591/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2916 - val_loss: 0.3009\n",
      "Epoch 592/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2902 - val_loss: 0.2955\n",
      "Epoch 593/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2906 - val_loss: 0.2954\n",
      "Epoch 594/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2921 - val_loss: 0.2970\n",
      "Epoch 595/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2906 - val_loss: 0.2984\n",
      "Epoch 596/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2902 - val_loss: 0.2960\n",
      "Epoch 597/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2907 - val_loss: 0.2962\n",
      "Epoch 598/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2911 - val_loss: 0.2947\n",
      "Epoch 599/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2904 - val_loss: 0.2975\n",
      "Epoch 600/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2901 - val_loss: 0.2991\n",
      "Epoch 601/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2955\n",
      "Epoch 602/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2901 - val_loss: 0.3025\n",
      "Epoch 603/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2905 - val_loss: 0.2954\n",
      "Epoch 604/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2898 - val_loss: 0.2983\n",
      "Epoch 605/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2913 - val_loss: 0.2976\n",
      "Epoch 606/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2909 - val_loss: 0.2987\n",
      "Epoch 607/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2901 - val_loss: 0.2951\n",
      "Epoch 608/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2897 - val_loss: 0.2996\n",
      "Epoch 609/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2900 - val_loss: 0.2950\n",
      "Epoch 610/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2896 - val_loss: 0.2966\n",
      "Epoch 611/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2904 - val_loss: 0.2940\n",
      "Epoch 612/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2959\n",
      "Epoch 613/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2901 - val_loss: 0.2969\n",
      "Epoch 614/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2901 - val_loss: 0.2964\n",
      "Epoch 615/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2959\n",
      "Epoch 616/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2900 - val_loss: 0.2951\n",
      "Epoch 617/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2883 - val_loss: 0.2976\n",
      "Epoch 618/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2910 - val_loss: 0.2993\n",
      "Epoch 619/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2896 - val_loss: 0.2927\n",
      "Epoch 620/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2882 - val_loss: 0.2980\n",
      "Epoch 621/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2896 - val_loss: 0.2952\n",
      "Epoch 622/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2898 - val_loss: 0.2972\n",
      "Epoch 623/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2895 - val_loss: 0.2975\n",
      "Epoch 624/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2902 - val_loss: 0.3010\n",
      "Epoch 625/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2886 - val_loss: 0.2957\n",
      "Epoch 626/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2902 - val_loss: 0.2971\n",
      "Epoch 627/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2904 - val_loss: 0.2941\n",
      "Epoch 628/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2907 - val_loss: 0.2975\n",
      "Epoch 629/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2904 - val_loss: 0.2960\n",
      "Epoch 630/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2898 - val_loss: 0.2947\n",
      "Epoch 631/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2897 - val_loss: 0.2966\n",
      "Epoch 632/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2894 - val_loss: 0.2982\n",
      "Epoch 633/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2949\n",
      "Epoch 634/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2879 - val_loss: 0.2983\n",
      "Epoch 635/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2891 - val_loss: 0.2969\n",
      "Epoch 636/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2899 - val_loss: 0.2997\n",
      "Epoch 637/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2899 - val_loss: 0.2936\n",
      "Epoch 638/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2908 - val_loss: 0.2977\n",
      "Epoch 639/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2893 - val_loss: 0.2957\n",
      "Epoch 640/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2895 - val_loss: 0.2976\n",
      "Epoch 641/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2892 - val_loss: 0.3066\n",
      "Epoch 642/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2891 - val_loss: 0.2951\n",
      "Epoch 643/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2909 - val_loss: 0.2973\n",
      "Epoch 644/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2895 - val_loss: 0.2971\n",
      "Epoch 645/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2901 - val_loss: 0.2961\n",
      "Epoch 646/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2884 - val_loss: 0.2967\n",
      "Epoch 647/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2893 - val_loss: 0.2940\n",
      "Epoch 648/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2893 - val_loss: 0.2953\n",
      "Epoch 649/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2894 - val_loss: 0.2984\n",
      "\n",
      "Epoch 00649: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 650/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2845 - val_loss: 0.2901\n",
      "Epoch 651/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2857 - val_loss: 0.2895\n",
      "Epoch 652/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2841 - val_loss: 0.2909\n",
      "Epoch 653/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2847 - val_loss: 0.2904\n",
      "Epoch 654/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2834 - val_loss: 0.2904\n",
      "Epoch 655/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2842 - val_loss: 0.2926\n",
      "Epoch 656/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2838 - val_loss: 0.2891\n",
      "Epoch 657/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2840 - val_loss: 0.2902\n",
      "Epoch 658/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2836 - val_loss: 0.2898\n",
      "Epoch 659/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2840 - val_loss: 0.2916\n",
      "Epoch 660/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2841 - val_loss: 0.2912\n",
      "Epoch 661/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2841 - val_loss: 0.2904\n",
      "Epoch 662/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2838 - val_loss: 0.2913\n",
      "Epoch 663/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2831 - val_loss: 0.2907\n",
      "Epoch 664/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2839 - val_loss: 0.2911\n",
      "Epoch 665/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2842 - val_loss: 0.2901\n",
      "Epoch 666/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2855 - val_loss: 0.2899\n",
      "Epoch 667/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2841 - val_loss: 0.2889\n",
      "Epoch 668/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2839 - val_loss: 0.2913\n",
      "Epoch 669/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2831 - val_loss: 0.2934\n",
      "Epoch 670/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2839 - val_loss: 0.2889\n",
      "Epoch 671/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2844 - val_loss: 0.2901\n",
      "Epoch 672/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2833 - val_loss: 0.2896\n",
      "Epoch 673/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2840 - val_loss: 0.2891\n",
      "Epoch 674/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2831 - val_loss: 0.2888\n",
      "Epoch 675/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2838 - val_loss: 0.2897\n",
      "Epoch 676/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2824 - val_loss: 0.2902\n",
      "Epoch 677/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2837 - val_loss: 0.2899\n",
      "Epoch 678/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2838 - val_loss: 0.2907\n",
      "Epoch 679/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2833 - val_loss: 0.2895\n",
      "Epoch 680/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2832 - val_loss: 0.2895\n",
      "Epoch 681/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2824 - val_loss: 0.2897\n",
      "Epoch 682/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2838 - val_loss: 0.2896\n",
      "Epoch 683/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2832 - val_loss: 0.2903\n",
      "Epoch 684/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2847 - val_loss: 0.2908\n",
      "Epoch 685/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2839 - val_loss: 0.2923\n",
      "Epoch 686/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2851 - val_loss: 0.2948\n",
      "Epoch 687/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2830 - val_loss: 0.2925\n",
      "Epoch 688/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2843 - val_loss: 0.2896\n",
      "Epoch 689/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2840 - val_loss: 0.2902\n",
      "Epoch 690/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2833 - val_loss: 0.2883\n",
      "Epoch 691/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2834 - val_loss: 0.2905\n",
      "Epoch 692/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2845 - val_loss: 0.2898\n",
      "Epoch 693/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2837 - val_loss: 0.2895\n",
      "Epoch 694/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2828 - val_loss: 0.2895\n",
      "Epoch 695/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2831 - val_loss: 0.2908\n",
      "Epoch 696/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2842 - val_loss: 0.2894\n",
      "Epoch 697/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2835 - val_loss: 0.2887\n",
      "Epoch 698/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2841 - val_loss: 0.2900\n",
      "Epoch 699/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2830 - val_loss: 0.2897\n",
      "Epoch 700/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2833 - val_loss: 0.2904\n",
      "Epoch 701/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2839 - val_loss: 0.2905\n",
      "Epoch 702/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2827 - val_loss: 0.2907\n",
      "Epoch 703/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2831 - val_loss: 0.2892\n",
      "Epoch 704/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2829 - val_loss: 0.2908\n",
      "Epoch 705/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2835 - val_loss: 0.2898\n",
      "Epoch 706/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2841 - val_loss: 0.2917\n",
      "Epoch 707/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2829 - val_loss: 0.2903\n",
      "Epoch 708/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2836 - val_loss: 0.2910\n",
      "Epoch 709/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2825 - val_loss: 0.2889\n",
      "Epoch 710/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2832 - val_loss: 0.2905\n",
      "Epoch 711/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2823 - val_loss: 0.2914\n",
      "Epoch 712/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2819 - val_loss: 0.2886\n",
      "Epoch 713/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2832 - val_loss: 0.2894\n",
      "Epoch 714/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2836 - val_loss: 0.2911\n",
      "Epoch 715/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2824 - val_loss: 0.2907\n",
      "Epoch 716/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2823 - val_loss: 0.2893\n",
      "Epoch 717/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2835 - val_loss: 0.2894\n",
      "Epoch 718/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2823 - val_loss: 0.2884\n",
      "Epoch 719/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2829 - val_loss: 0.2914\n",
      "Epoch 720/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2835 - val_loss: 0.2910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00720: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 721/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2811 - val_loss: 0.2877\n",
      "Epoch 722/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2811 - val_loss: 0.2871\n",
      "Epoch 723/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2804 - val_loss: 0.2869\n",
      "Epoch 724/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2877\n",
      "Epoch 725/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2810 - val_loss: 0.2877\n",
      "Epoch 726/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2870\n",
      "Epoch 727/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2870\n",
      "Epoch 728/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2809 - val_loss: 0.2866\n",
      "Epoch 729/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2803 - val_loss: 0.2861\n",
      "Epoch 730/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2912\n",
      "Epoch 731/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2800 - val_loss: 0.2867\n",
      "Epoch 732/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2808 - val_loss: 0.2868\n",
      "Epoch 733/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2895\n",
      "Epoch 734/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2807 - val_loss: 0.2865\n",
      "Epoch 735/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2788 - val_loss: 0.2909\n",
      "Epoch 736/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2796 - val_loss: 0.2866\n",
      "Epoch 737/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2803 - val_loss: 0.2863\n",
      "Epoch 738/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2803 - val_loss: 0.2862\n",
      "Epoch 739/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2795 - val_loss: 0.2862\n",
      "Epoch 740/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2872\n",
      "Epoch 741/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2866\n",
      "Epoch 742/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2800 - val_loss: 0.2871\n",
      "Epoch 743/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2806 - val_loss: 0.2875\n",
      "Epoch 744/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2803 - val_loss: 0.2868\n",
      "Epoch 745/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2804 - val_loss: 0.2874\n",
      "Epoch 746/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2804 - val_loss: 0.2873\n",
      "Epoch 747/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2800 - val_loss: 0.2862\n",
      "Epoch 748/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2796 - val_loss: 0.2869\n",
      "Epoch 749/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2806 - val_loss: 0.2865\n",
      "Epoch 750/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2805 - val_loss: 0.2879\n",
      "Epoch 751/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2811 - val_loss: 0.2876\n",
      "Epoch 752/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2803 - val_loss: 0.2870\n",
      "Epoch 753/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2802 - val_loss: 0.2859\n",
      "Epoch 754/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2798 - val_loss: 0.2871\n",
      "Epoch 755/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2867\n",
      "Epoch 756/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2881\n",
      "Epoch 757/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2871\n",
      "Epoch 758/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2806 - val_loss: 0.2866\n",
      "Epoch 759/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2792 - val_loss: 0.2861\n",
      "Epoch 760/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2803 - val_loss: 0.2862\n",
      "Epoch 761/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2860\n",
      "Epoch 762/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2798 - val_loss: 0.2867\n",
      "Epoch 763/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2788 - val_loss: 0.2868\n",
      "Epoch 764/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2868\n",
      "Epoch 765/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2808 - val_loss: 0.2873\n",
      "Epoch 766/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2866\n",
      "Epoch 767/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2790 - val_loss: 0.2860\n",
      "Epoch 768/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2876\n",
      "Epoch 769/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2861\n",
      "Epoch 770/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2862\n",
      "Epoch 771/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2791 - val_loss: 0.2872\n",
      "Epoch 772/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2867\n",
      "Epoch 773/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2789 - val_loss: 0.2859\n",
      "Epoch 774/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2804 - val_loss: 0.2867\n",
      "Epoch 775/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2862\n",
      "Epoch 776/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2865\n",
      "Epoch 777/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2800 - val_loss: 0.2865\n",
      "Epoch 778/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2863\n",
      "Epoch 779/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2806 - val_loss: 0.2858\n",
      "Epoch 780/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2863\n",
      "Epoch 781/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2804 - val_loss: 0.2862\n",
      "Epoch 782/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2790 - val_loss: 0.2876\n",
      "Epoch 783/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2803 - val_loss: 0.2866\n",
      "Epoch 784/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2794 - val_loss: 0.2868\n",
      "Epoch 785/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2873\n",
      "Epoch 786/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2798 - val_loss: 0.2867\n",
      "Epoch 787/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2877\n",
      "Epoch 788/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2866\n",
      "Epoch 789/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2796 - val_loss: 0.2864\n",
      "Epoch 790/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2792 - val_loss: 0.2863\n",
      "Epoch 791/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2882\n",
      "Epoch 792/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2863\n",
      "Epoch 793/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2807 - val_loss: 0.2860\n",
      "Epoch 794/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2863\n",
      "Epoch 795/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2804 - val_loss: 0.2880\n",
      "Epoch 796/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2800 - val_loss: 0.2864\n",
      "Epoch 797/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2870\n",
      "Epoch 798/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2807 - val_loss: 0.2873\n",
      "Epoch 799/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2796 - val_loss: 0.2860\n",
      "Epoch 800/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2799 - val_loss: 0.2870\n",
      "Epoch 801/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2791 - val_loss: 0.2864\n",
      "Epoch 802/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2806 - val_loss: 0.2858\n",
      "Epoch 803/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2809 - val_loss: 0.2872\n",
      "Epoch 804/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2798 - val_loss: 0.2863\n",
      "Epoch 805/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2796 - val_loss: 0.2860\n",
      "Epoch 806/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2875\n",
      "Epoch 807/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2795 - val_loss: 0.2856\n",
      "Epoch 808/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2790 - val_loss: 0.2859\n",
      "Epoch 809/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2790 - val_loss: 0.2867\n",
      "Epoch 810/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2787 - val_loss: 0.2862\n",
      "Epoch 811/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2859\n",
      "Epoch 812/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2791 - val_loss: 0.2859\n",
      "Epoch 813/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2883\n",
      "Epoch 814/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2801 - val_loss: 0.2858\n",
      "Epoch 815/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2788 - val_loss: 0.2862\n",
      "Epoch 816/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2792 - val_loss: 0.2860\n",
      "Epoch 817/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2875\n",
      "Epoch 818/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2782 - val_loss: 0.2872\n",
      "Epoch 819/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2800 - val_loss: 0.2882\n",
      "Epoch 820/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2795 - val_loss: 0.2853\n",
      "Epoch 821/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2795 - val_loss: 0.2868\n",
      "Epoch 822/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2860\n",
      "Epoch 823/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2798 - val_loss: 0.2885\n",
      "Epoch 824/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2865\n",
      "Epoch 825/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2786 - val_loss: 0.2866\n",
      "Epoch 826/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2863\n",
      "Epoch 827/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2799 - val_loss: 0.2854\n",
      "Epoch 828/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2798 - val_loss: 0.2861\n",
      "Epoch 829/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2789 - val_loss: 0.2873\n",
      "Epoch 830/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2867\n",
      "Epoch 831/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2860\n",
      "Epoch 832/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2863\n",
      "Epoch 833/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2792 - val_loss: 0.2857\n",
      "Epoch 834/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2795 - val_loss: 0.2867\n",
      "Epoch 835/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2787 - val_loss: 0.2861\n",
      "Epoch 836/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2860\n",
      "Epoch 837/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2792 - val_loss: 0.2872\n",
      "Epoch 838/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2869\n",
      "Epoch 839/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2802 - val_loss: 0.2855\n",
      "Epoch 840/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2797 - val_loss: 0.2860\n",
      "Epoch 841/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2793 - val_loss: 0.2863\n",
      "Epoch 842/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2794 - val_loss: 0.2864\n",
      "Epoch 843/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2789 - val_loss: 0.2888\n",
      "Epoch 844/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2786 - val_loss: 0.2861\n",
      "Epoch 845/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2786 - val_loss: 0.2861\n",
      "Epoch 846/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2796 - val_loss: 0.2861\n",
      "Epoch 847/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2792 - val_loss: 0.2868\n",
      "Epoch 848/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2791 - val_loss: 0.2869\n",
      "Epoch 849/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2786 - val_loss: 0.2863\n",
      "Epoch 850/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2807 - val_loss: 0.2862\n",
      "\n",
      "Epoch 00850: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 851/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2780 - val_loss: 0.2854\n",
      "Epoch 852/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2784 - val_loss: 0.2854\n",
      "Epoch 853/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2846\n",
      "Epoch 854/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2789 - val_loss: 0.2851\n",
      "Epoch 855/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2784 - val_loss: 0.2852\n",
      "Epoch 856/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2858\n",
      "Epoch 857/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2773 - val_loss: 0.2847\n",
      "Epoch 858/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2778 - val_loss: 0.2847\n",
      "Epoch 859/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2781 - val_loss: 0.2853\n",
      "Epoch 860/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2777 - val_loss: 0.2860\n",
      "Epoch 861/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2782 - val_loss: 0.2852\n",
      "Epoch 862/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2787 - val_loss: 0.2849\n",
      "Epoch 863/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2776 - val_loss: 0.2848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 864/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2787 - val_loss: 0.2850\n",
      "Epoch 865/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2777 - val_loss: 0.2850\n",
      "Epoch 866/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2786 - val_loss: 0.2853\n",
      "Epoch 867/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2779 - val_loss: 0.2856\n",
      "Epoch 868/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2779 - val_loss: 0.2850\n",
      "Epoch 869/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2778 - val_loss: 0.2851\n",
      "Epoch 870/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2850\n",
      "Epoch 871/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2849\n",
      "Epoch 872/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2781 - val_loss: 0.2849\n",
      "Epoch 873/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2784 - val_loss: 0.2848\n",
      "Epoch 874/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2855\n",
      "Epoch 875/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2848\n",
      "Epoch 876/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2847\n",
      "Epoch 877/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2785 - val_loss: 0.2849\n",
      "Epoch 878/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2782 - val_loss: 0.2849\n",
      "Epoch 879/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2786 - val_loss: 0.2856\n",
      "Epoch 880/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2847\n",
      "Epoch 881/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2772 - val_loss: 0.2848\n",
      "Epoch 882/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2781 - val_loss: 0.2852\n",
      "Epoch 883/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2780 - val_loss: 0.2861\n",
      "\n",
      "Epoch 00883: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 884/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2775 - val_loss: 0.2844\n",
      "Epoch 885/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2840\n",
      "Epoch 886/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2842\n",
      "Epoch 887/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2843\n",
      "Epoch 888/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2844\n",
      "Epoch 889/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2841\n",
      "Epoch 890/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2845\n",
      "Epoch 891/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2842\n",
      "Epoch 892/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2769 - val_loss: 0.2840\n",
      "Epoch 893/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2777 - val_loss: 0.2843\n",
      "Epoch 894/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2781 - val_loss: 0.2838\n",
      "Epoch 895/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2841\n",
      "Epoch 896/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2775 - val_loss: 0.2842\n",
      "Epoch 897/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2766 - val_loss: 0.2843\n",
      "Epoch 898/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2766 - val_loss: 0.2846\n",
      "Epoch 899/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2841\n",
      "Epoch 900/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2841\n",
      "Epoch 901/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2772 - val_loss: 0.2841\n",
      "Epoch 902/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2843\n",
      "Epoch 903/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2773 - val_loss: 0.2843\n",
      "Epoch 904/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2843\n",
      "Epoch 905/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2842\n",
      "Epoch 906/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2772 - val_loss: 0.2839\n",
      "Epoch 907/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2840\n",
      "Epoch 908/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2842\n",
      "Epoch 909/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2775 - val_loss: 0.2847\n",
      "Epoch 910/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2844\n",
      "Epoch 911/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2844\n",
      "Epoch 912/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2848\n",
      "Epoch 913/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2841\n",
      "Epoch 914/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2841\n",
      "Epoch 915/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2849\n",
      "Epoch 916/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2770 - val_loss: 0.2840\n",
      "Epoch 917/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2846\n",
      "Epoch 918/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2843\n",
      "Epoch 919/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2841\n",
      "Epoch 920/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2842\n",
      "Epoch 921/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2765 - val_loss: 0.2847\n",
      "Epoch 922/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2776 - val_loss: 0.2844\n",
      "Epoch 923/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2838\n",
      "Epoch 924/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2769 - val_loss: 0.2842\n",
      "\n",
      "Epoch 00924: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 925/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2841\n",
      "Epoch 926/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2846\n",
      "Epoch 927/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2839\n",
      "Epoch 928/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2765 - val_loss: 0.2844\n",
      "Epoch 929/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2764 - val_loss: 0.2839\n",
      "Epoch 930/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2841\n",
      "Epoch 931/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2838\n",
      "Epoch 932/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2841\n",
      "Epoch 933/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2839\n",
      "Epoch 934/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2775 - val_loss: 0.2839\n",
      "Epoch 935/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2841\n",
      "Epoch 936/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2841\n",
      "Epoch 937/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2767 - val_loss: 0.2843\n",
      "Epoch 938/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2843\n",
      "Epoch 939/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2844\n",
      "Epoch 940/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2766 - val_loss: 0.2837\n",
      "Epoch 941/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2751 - val_loss: 0.2840\n",
      "Epoch 942/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2842\n",
      "Epoch 943/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2838\n",
      "Epoch 944/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2840\n",
      "Epoch 945/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2759 - val_loss: 0.2837\n",
      "Epoch 946/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2839\n",
      "Epoch 947/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2837\n",
      "Epoch 948/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2839\n",
      "Epoch 949/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2766 - val_loss: 0.2841\n",
      "Epoch 950/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2760 - val_loss: 0.2841\n",
      "Epoch 951/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2758 - val_loss: 0.2842\n",
      "Epoch 952/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2840\n",
      "Epoch 953/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2848\n",
      "Epoch 954/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2839\n",
      "Epoch 955/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2758 - val_loss: 0.2839\n",
      "Epoch 956/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2841\n",
      "Epoch 957/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2836\n",
      "Epoch 958/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2841\n",
      "Epoch 959/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2852\n",
      "Epoch 960/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2762 - val_loss: 0.2851\n",
      "Epoch 961/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2838\n",
      "Epoch 962/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2774 - val_loss: 0.2838\n",
      "Epoch 963/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2842\n",
      "Epoch 964/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2840\n",
      "Epoch 965/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2759 - val_loss: 0.2837\n",
      "Epoch 966/2000\n",
      "1207321/1207321 [==============================] - 12s 10us/step - loss: 0.2764 - val_loss: 0.2837\n",
      "Epoch 967/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2765 - val_loss: 0.2837\n",
      "Epoch 968/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2841\n",
      "Epoch 969/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2839\n",
      "Epoch 970/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2835\n",
      "Epoch 971/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2760 - val_loss: 0.2841\n",
      "Epoch 972/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2839\n",
      "Epoch 973/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2830\n",
      "Epoch 974/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2765 - val_loss: 0.2841\n",
      "Epoch 975/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2756 - val_loss: 0.2841\n",
      "Epoch 976/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2840\n",
      "Epoch 977/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2840\n",
      "Epoch 978/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2841\n",
      "Epoch 979/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2835\n",
      "Epoch 980/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2841\n",
      "Epoch 981/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2762 - val_loss: 0.2841\n",
      "Epoch 982/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2777 - val_loss: 0.2840\n",
      "Epoch 983/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2754 - val_loss: 0.2850\n",
      "Epoch 984/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2845\n",
      "Epoch 985/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2846\n",
      "Epoch 986/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2839\n",
      "Epoch 987/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2762 - val_loss: 0.2840\n",
      "Epoch 988/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2838\n",
      "Epoch 989/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2841\n",
      "Epoch 990/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2773 - val_loss: 0.2840\n",
      "Epoch 991/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2766 - val_loss: 0.2837\n",
      "Epoch 992/2000\n",
      "1207321/1207321 [==============================] - 11s 10us/step - loss: 0.2769 - val_loss: 0.2840\n",
      "Epoch 993/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2765 - val_loss: 0.2846\n",
      "Epoch 994/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2835\n",
      "Epoch 995/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2845\n",
      "Epoch 996/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2771 - val_loss: 0.2839\n",
      "Epoch 997/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2760 - val_loss: 0.2838\n",
      "Epoch 998/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2841\n",
      "Epoch 999/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2839\n",
      "Epoch 1000/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2837\n",
      "Epoch 1001/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2760 - val_loss: 0.2838\n",
      "Epoch 1002/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2836\n",
      "Epoch 1003/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2841\n",
      "\n",
      "Epoch 01003: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 1004/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2754 - val_loss: 0.2838\n",
      "Epoch 1005/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2844\n",
      "Epoch 1006/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2770 - val_loss: 0.2840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1007/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2836\n",
      "Epoch 1008/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2838\n",
      "Epoch 1009/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2754 - val_loss: 0.2836\n",
      "Epoch 1010/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2769 - val_loss: 0.2837\n",
      "Epoch 1011/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2762 - val_loss: 0.2840\n",
      "Epoch 1012/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2763 - val_loss: 0.2838\n",
      "Epoch 1013/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2756 - val_loss: 0.2839\n",
      "Epoch 1014/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2761 - val_loss: 0.2837\n",
      "Epoch 1015/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2766 - val_loss: 0.2845\n",
      "Epoch 1016/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2775 - val_loss: 0.2836\n",
      "Epoch 1017/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2768 - val_loss: 0.2845\n",
      "Epoch 1018/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2765 - val_loss: 0.2840\n",
      "Epoch 1019/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2840\n",
      "Epoch 1020/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2764 - val_loss: 0.2840\n",
      "Epoch 1021/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2843\n",
      "Epoch 1022/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2767 - val_loss: 0.2845\n",
      "Epoch 1023/2000\n",
      "1207321/1207321 [==============================] - 11s 9us/step - loss: 0.2759 - val_loss: 0.2837\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 01023: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkYhTIhARiTglAhGRiFMiEBGJuP8PiQoNQzAVThAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2622940918525685\n",
      "Training 3JHN out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "Train on 133228 samples, validate on 33187 samples\n",
      "Epoch 1/2000\n",
      "133228/133228 [==============================] - 2s 14us/step - loss: 0.5960 - val_loss: 0.4455\n",
      "Epoch 2/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.3711 - val_loss: 0.3818\n",
      "Epoch 3/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.3253 - val_loss: 0.3051\n",
      "Epoch 4/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.2952 - val_loss: 0.3004\n",
      "Epoch 5/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2782 - val_loss: 0.3089\n",
      "Epoch 6/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2622 - val_loss: 0.2725\n",
      "Epoch 7/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2489 - val_loss: 0.2667\n",
      "Epoch 8/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2411 - val_loss: 0.2418\n",
      "Epoch 9/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2328 - val_loss: 0.2308\n",
      "Epoch 10/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2250 - val_loss: 0.2204\n",
      "Epoch 11/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2214 - val_loss: 0.2271\n",
      "Epoch 12/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.2144 - val_loss: 0.2200\n",
      "Epoch 13/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.2084 - val_loss: 0.2129\n",
      "Epoch 14/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.2034 - val_loss: 0.2266\n",
      "Epoch 15/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1998 - val_loss: 0.2078\n",
      "Epoch 16/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1954 - val_loss: 0.2259\n",
      "Epoch 17/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1956 - val_loss: 0.2124\n",
      "Epoch 18/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1926 - val_loss: 0.1983\n",
      "Epoch 19/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1903 - val_loss: 0.2171\n",
      "Epoch 20/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1885 - val_loss: 0.2077\n",
      "Epoch 21/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1870 - val_loss: 0.1921\n",
      "Epoch 22/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1809 - val_loss: 0.2159\n",
      "Epoch 23/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1812 - val_loss: 0.1926\n",
      "Epoch 24/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1796 - val_loss: 0.1859\n",
      "Epoch 25/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1773 - val_loss: 0.1974\n",
      "Epoch 26/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1750 - val_loss: 0.1847\n",
      "Epoch 27/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1730 - val_loss: 0.1938\n",
      "Epoch 28/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1725 - val_loss: 0.1737\n",
      "Epoch 29/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1689 - val_loss: 0.1834\n",
      "Epoch 30/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1682 - val_loss: 0.1850\n",
      "Epoch 31/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1693 - val_loss: 0.1784\n",
      "Epoch 32/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1642 - val_loss: 0.1687\n",
      "Epoch 33/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1650 - val_loss: 0.1774\n",
      "Epoch 34/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1626 - val_loss: 0.1643\n",
      "Epoch 35/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1631 - val_loss: 0.1727\n",
      "Epoch 36/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1599 - val_loss: 0.1689\n",
      "Epoch 37/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1603 - val_loss: 0.1716\n",
      "Epoch 38/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1581 - val_loss: 0.1760\n",
      "Epoch 39/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1567 - val_loss: 0.1745\n",
      "Epoch 40/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1554 - val_loss: 0.1744\n",
      "Epoch 41/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1561 - val_loss: 0.1664\n",
      "Epoch 42/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1549 - val_loss: 0.1680\n",
      "Epoch 43/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1550 - val_loss: 0.1673\n",
      "Epoch 44/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1508 - val_loss: 0.1691\n",
      "Epoch 45/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1512 - val_loss: 0.1618\n",
      "Epoch 46/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1509 - val_loss: 0.1607\n",
      "Epoch 47/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1498 - val_loss: 0.1673\n",
      "Epoch 48/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1495 - val_loss: 0.1727\n",
      "Epoch 49/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1487 - val_loss: 0.1610\n",
      "Epoch 50/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1492 - val_loss: 0.1595\n",
      "Epoch 51/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1458 - val_loss: 0.1627\n",
      "Epoch 52/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1459 - val_loss: 0.1706\n",
      "Epoch 53/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1433 - val_loss: 0.1521\n",
      "Epoch 54/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1444 - val_loss: 0.1657\n",
      "Epoch 55/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1433 - val_loss: 0.1660\n",
      "Epoch 56/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1422 - val_loss: 0.1543\n",
      "Epoch 57/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1412 - val_loss: 0.1548\n",
      "Epoch 58/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1418 - val_loss: 0.1561\n",
      "Epoch 59/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1400 - val_loss: 0.1540\n",
      "Epoch 60/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1409 - val_loss: 0.1570\n",
      "Epoch 61/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1394 - val_loss: 0.1575\n",
      "Epoch 62/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1398 - val_loss: 0.1541\n",
      "Epoch 63/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1394 - val_loss: 0.1544\n",
      "Epoch 64/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1390 - val_loss: 0.1570\n",
      "Epoch 65/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1389 - val_loss: 0.1503\n",
      "Epoch 66/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1374 - val_loss: 0.1510\n",
      "Epoch 67/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1358 - val_loss: 0.1658\n",
      "Epoch 68/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1376 - val_loss: 0.1539\n",
      "Epoch 69/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1362 - val_loss: 0.1565\n",
      "Epoch 70/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1350 - val_loss: 0.1500\n",
      "Epoch 71/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1349 - val_loss: 0.1555\n",
      "Epoch 72/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1332 - val_loss: 0.1577\n",
      "Epoch 73/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1336 - val_loss: 0.1511\n",
      "Epoch 74/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1330 - val_loss: 0.1468\n",
      "Epoch 75/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1304 - val_loss: 0.1521\n",
      "Epoch 76/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1334 - val_loss: 0.1443\n",
      "Epoch 77/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1311 - val_loss: 0.1528\n",
      "Epoch 78/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1301 - val_loss: 0.1523\n",
      "Epoch 79/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1301 - val_loss: 0.1455\n",
      "Epoch 80/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1299 - val_loss: 0.1528\n",
      "Epoch 81/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1283 - val_loss: 0.1475\n",
      "Epoch 82/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1290 - val_loss: 0.1546\n",
      "Epoch 83/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1297 - val_loss: 0.1546\n",
      "Epoch 84/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1287 - val_loss: 0.1440\n",
      "Epoch 85/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1288 - val_loss: 0.1493\n",
      "Epoch 86/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1261 - val_loss: 0.1476\n",
      "Epoch 87/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1253 - val_loss: 0.1461\n",
      "Epoch 88/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1284 - val_loss: 0.1473\n",
      "Epoch 89/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1234 - val_loss: 0.1460\n",
      "Epoch 90/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1280 - val_loss: 0.1443\n",
      "Epoch 91/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1252 - val_loss: 0.1421\n",
      "Epoch 92/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1243 - val_loss: 0.1446\n",
      "Epoch 93/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1232 - val_loss: 0.1485\n",
      "Epoch 94/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1248 - val_loss: 0.1460\n",
      "Epoch 95/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1248 - val_loss: 0.1446\n",
      "Epoch 96/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1260 - val_loss: 0.1445\n",
      "Epoch 97/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1227 - val_loss: 0.1433\n",
      "Epoch 98/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1242 - val_loss: 0.1461\n",
      "Epoch 99/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1230 - val_loss: 0.1400\n",
      "Epoch 100/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1238 - val_loss: 0.1538\n",
      "Epoch 101/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1210 - val_loss: 0.1401\n",
      "Epoch 102/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1240 - val_loss: 0.1502\n",
      "Epoch 103/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1217 - val_loss: 0.1419\n",
      "Epoch 104/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1216 - val_loss: 0.1463\n",
      "Epoch 105/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1200 - val_loss: 0.1370\n",
      "Epoch 106/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1200 - val_loss: 0.1385\n",
      "Epoch 107/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1203 - val_loss: 0.1352\n",
      "Epoch 108/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1198 - val_loss: 0.1366\n",
      "Epoch 109/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1195 - val_loss: 0.1432\n",
      "Epoch 110/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1210 - val_loss: 0.1390\n",
      "Epoch 111/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1203 - val_loss: 0.1442\n",
      "Epoch 112/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1178 - val_loss: 0.1377\n",
      "Epoch 113/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1183 - val_loss: 0.1358\n",
      "Epoch 114/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1181 - val_loss: 0.1357\n",
      "Epoch 115/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1181 - val_loss: 0.1371\n",
      "Epoch 116/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1171 - val_loss: 0.1386\n",
      "Epoch 117/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1190 - val_loss: 0.1343\n",
      "Epoch 118/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1178 - val_loss: 0.1364\n",
      "Epoch 119/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1171 - val_loss: 0.1399\n",
      "Epoch 120/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1171 - val_loss: 0.1362\n",
      "Epoch 121/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1178 - val_loss: 0.1400\n",
      "Epoch 122/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1163 - val_loss: 0.1413\n",
      "Epoch 123/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1165 - val_loss: 0.1417\n",
      "Epoch 124/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1144 - val_loss: 0.1368\n",
      "Epoch 125/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1170 - val_loss: 0.1372\n",
      "Epoch 126/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1172 - val_loss: 0.1378\n",
      "Epoch 127/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1150 - val_loss: 0.1330\n",
      "Epoch 128/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1161 - val_loss: 0.1418\n",
      "Epoch 129/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1143 - val_loss: 0.1448\n",
      "Epoch 130/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1135 - val_loss: 0.1404\n",
      "Epoch 131/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1153 - val_loss: 0.1395\n",
      "Epoch 132/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1148 - val_loss: 0.1399\n",
      "Epoch 133/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1147 - val_loss: 0.1387\n",
      "Epoch 134/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1136 - val_loss: 0.1380\n",
      "Epoch 135/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1142 - val_loss: 0.1388\n",
      "Epoch 136/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1136 - val_loss: 0.1313\n",
      "Epoch 137/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1125 - val_loss: 0.1371\n",
      "Epoch 138/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1143 - val_loss: 0.1355\n",
      "Epoch 139/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1144 - val_loss: 0.1344\n",
      "Epoch 140/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1120 - val_loss: 0.1331\n",
      "Epoch 141/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1139 - val_loss: 0.1331\n",
      "Epoch 142/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1110 - val_loss: 0.1360\n",
      "Epoch 143/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1117 - val_loss: 0.1360\n",
      "Epoch 144/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1096 - val_loss: 0.1450\n",
      "Epoch 145/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1120 - val_loss: 0.1355\n",
      "Epoch 146/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1119 - val_loss: 0.1343\n",
      "Epoch 147/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1091 - val_loss: 0.1378\n",
      "Epoch 148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1109 - val_loss: 0.1357\n",
      "Epoch 149/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1099 - val_loss: 0.1294\n",
      "Epoch 150/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1099 - val_loss: 0.1344\n",
      "Epoch 151/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1096 - val_loss: 0.1310\n",
      "Epoch 152/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1090 - val_loss: 0.1314\n",
      "Epoch 153/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1101 - val_loss: 0.1325\n",
      "Epoch 154/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1111 - val_loss: 0.1309\n",
      "Epoch 155/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1094 - val_loss: 0.1336\n",
      "Epoch 156/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1095 - val_loss: 0.1350\n",
      "Epoch 157/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1087 - val_loss: 0.1457\n",
      "Epoch 158/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1086 - val_loss: 0.1316\n",
      "Epoch 159/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1071 - val_loss: 0.1280\n",
      "Epoch 160/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1081 - val_loss: 0.1321\n",
      "Epoch 161/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1083 - val_loss: 0.1362\n",
      "Epoch 162/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1080 - val_loss: 0.1294\n",
      "Epoch 163/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1070 - val_loss: 0.1288\n",
      "Epoch 164/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1085 - val_loss: 0.1343\n",
      "Epoch 165/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1073 - val_loss: 0.1300\n",
      "Epoch 166/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1090 - val_loss: 0.1348\n",
      "Epoch 167/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1097 - val_loss: 0.1317\n",
      "Epoch 168/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1075 - val_loss: 0.1381\n",
      "Epoch 169/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1083 - val_loss: 0.1296\n",
      "Epoch 170/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1073 - val_loss: 0.1325\n",
      "Epoch 171/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1051 - val_loss: 0.1357\n",
      "Epoch 172/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1060 - val_loss: 0.1286\n",
      "Epoch 173/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1062 - val_loss: 0.1329\n",
      "Epoch 174/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1073 - val_loss: 0.1273\n",
      "Epoch 175/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1048 - val_loss: 0.1324\n",
      "Epoch 176/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1067 - val_loss: 0.1305\n",
      "Epoch 177/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1064 - val_loss: 0.1305\n",
      "Epoch 178/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1065 - val_loss: 0.1324\n",
      "Epoch 179/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1058 - val_loss: 0.1300\n",
      "Epoch 180/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1077 - val_loss: 0.1333\n",
      "Epoch 181/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1042 - val_loss: 0.1302\n",
      "Epoch 182/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1069 - val_loss: 0.1367\n",
      "Epoch 183/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1051 - val_loss: 0.1307\n",
      "Epoch 184/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1052 - val_loss: 0.1360\n",
      "Epoch 185/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1043 - val_loss: 0.1300\n",
      "Epoch 186/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1063 - val_loss: 0.1282\n",
      "Epoch 187/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1046 - val_loss: 0.1314\n",
      "Epoch 188/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1066 - val_loss: 0.1331\n",
      "Epoch 189/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1040 - val_loss: 0.1250\n",
      "Epoch 190/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1030 - val_loss: 0.1299\n",
      "Epoch 191/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1037 - val_loss: 0.1292\n",
      "Epoch 192/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1071 - val_loss: 0.1319\n",
      "Epoch 193/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1061 - val_loss: 0.1322\n",
      "Epoch 194/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1044 - val_loss: 0.1291\n",
      "Epoch 195/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1031 - val_loss: 0.1331\n",
      "Epoch 196/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1043 - val_loss: 0.1267\n",
      "Epoch 197/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1054 - val_loss: 0.1269\n",
      "Epoch 198/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1023 - val_loss: 0.1259\n",
      "Epoch 199/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1051 - val_loss: 0.1347\n",
      "Epoch 200/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1027 - val_loss: 0.1263\n",
      "Epoch 201/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1022 - val_loss: 0.1293\n",
      "Epoch 202/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1029 - val_loss: 0.1281\n",
      "Epoch 203/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1011 - val_loss: 0.1286\n",
      "Epoch 204/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1013 - val_loss: 0.1276\n",
      "Epoch 205/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1022 - val_loss: 0.1321\n",
      "Epoch 206/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1038 - val_loss: 0.1331\n",
      "Epoch 207/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1032 - val_loss: 0.1255\n",
      "Epoch 208/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1023 - val_loss: 0.1385\n",
      "Epoch 209/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1025 - val_loss: 0.1289\n",
      "Epoch 210/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1040 - val_loss: 0.1259\n",
      "Epoch 211/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1027 - val_loss: 0.1308\n",
      "Epoch 212/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1017 - val_loss: 0.1306\n",
      "Epoch 213/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1023 - val_loss: 0.1304\n",
      "Epoch 214/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1027 - val_loss: 0.1256\n",
      "Epoch 215/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1011 - val_loss: 0.1291\n",
      "Epoch 216/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0995 - val_loss: 0.1250\n",
      "Epoch 217/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1005 - val_loss: 0.1242\n",
      "Epoch 218/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1009 - val_loss: 0.1247\n",
      "Epoch 219/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1000 - val_loss: 0.1272\n",
      "Epoch 220/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0996 - val_loss: 0.1236\n",
      "Epoch 221/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1007 - val_loss: 0.1285\n",
      "Epoch 222/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1007 - val_loss: 0.1278\n",
      "Epoch 223/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1015 - val_loss: 0.1285\n",
      "Epoch 224/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1008 - val_loss: 0.1282\n",
      "Epoch 225/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1009 - val_loss: 0.1257\n",
      "Epoch 226/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0995 - val_loss: 0.1271\n",
      "Epoch 227/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0996 - val_loss: 0.1270\n",
      "Epoch 228/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1006 - val_loss: 0.1247\n",
      "Epoch 229/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1004 - val_loss: 0.1240\n",
      "Epoch 230/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0996 - val_loss: 0.1269\n",
      "Epoch 231/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1000 - val_loss: 0.1330\n",
      "Epoch 232/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1004 - val_loss: 0.1288\n",
      "Epoch 233/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1003 - val_loss: 0.1305\n",
      "Epoch 234/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0993 - val_loss: 0.1309\n",
      "Epoch 235/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.1010 - val_loss: 0.1246\n",
      "Epoch 236/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1008 - val_loss: 0.1276\n",
      "Epoch 237/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0999 - val_loss: 0.1293\n",
      "Epoch 238/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1009 - val_loss: 0.1248\n",
      "Epoch 239/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0982 - val_loss: 0.1300\n",
      "Epoch 240/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0987 - val_loss: 0.1294\n",
      "Epoch 241/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0985 - val_loss: 0.1247\n",
      "Epoch 242/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0986 - val_loss: 0.1210\n",
      "Epoch 243/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0979 - val_loss: 0.1236\n",
      "Epoch 244/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0975 - val_loss: 0.1238\n",
      "Epoch 245/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0984 - val_loss: 0.1259\n",
      "Epoch 246/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0984 - val_loss: 0.1224\n",
      "Epoch 247/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0977 - val_loss: 0.1255\n",
      "Epoch 248/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0988 - val_loss: 0.1320\n",
      "Epoch 249/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0996 - val_loss: 0.1255\n",
      "Epoch 250/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0999 - val_loss: 0.1230\n",
      "Epoch 251/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0990 - val_loss: 0.1294\n",
      "Epoch 252/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0977 - val_loss: 0.1275\n",
      "Epoch 253/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0981 - val_loss: 0.1316\n",
      "Epoch 254/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0964 - val_loss: 0.1244\n",
      "Epoch 255/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0982 - val_loss: 0.1257\n",
      "Epoch 256/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.1004 - val_loss: 0.1216\n",
      "Epoch 257/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0967 - val_loss: 0.1279\n",
      "Epoch 258/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0967 - val_loss: 0.1214\n",
      "Epoch 259/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0981 - val_loss: 0.1294\n",
      "Epoch 260/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0981 - val_loss: 0.1254\n",
      "Epoch 261/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0986 - val_loss: 0.1259\n",
      "Epoch 262/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0976 - val_loss: 0.1273\n",
      "Epoch 263/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0958 - val_loss: 0.1280\n",
      "Epoch 264/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0979 - val_loss: 0.1225\n",
      "Epoch 265/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0976 - val_loss: 0.1212\n",
      "Epoch 266/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0959 - val_loss: 0.1213\n",
      "Epoch 267/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0968 - val_loss: 0.1194\n",
      "Epoch 268/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0977 - val_loss: 0.1242\n",
      "Epoch 269/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0970 - val_loss: 0.1257\n",
      "Epoch 270/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0990 - val_loss: 0.1289\n",
      "Epoch 271/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0967 - val_loss: 0.1235\n",
      "Epoch 272/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0946 - val_loss: 0.1290\n",
      "Epoch 273/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0952 - val_loss: 0.1224\n",
      "Epoch 274/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0956 - val_loss: 0.1205\n",
      "Epoch 275/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0988 - val_loss: 0.1225\n",
      "Epoch 276/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0964 - val_loss: 0.1244\n",
      "Epoch 277/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0979 - val_loss: 0.1251\n",
      "Epoch 278/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0955 - val_loss: 0.1223\n",
      "Epoch 279/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0976 - val_loss: 0.1248\n",
      "Epoch 280/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0952 - val_loss: 0.1236\n",
      "Epoch 281/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0951 - val_loss: 0.1209\n",
      "Epoch 282/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0931 - val_loss: 0.1246\n",
      "Epoch 283/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0956 - val_loss: 0.1222\n",
      "Epoch 284/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0944 - val_loss: 0.1216\n",
      "Epoch 285/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0965 - val_loss: 0.1248\n",
      "Epoch 286/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0962 - val_loss: 0.1197\n",
      "Epoch 287/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0964 - val_loss: 0.1203\n",
      "Epoch 288/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0970 - val_loss: 0.1205\n",
      "Epoch 289/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0951 - val_loss: 0.1205\n",
      "Epoch 290/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0955 - val_loss: 0.1263\n",
      "Epoch 291/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0962 - val_loss: 0.1205\n",
      "Epoch 292/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0944 - val_loss: 0.1268\n",
      "Epoch 293/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0946 - val_loss: 0.1285\n",
      "Epoch 294/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0957 - val_loss: 0.1209\n",
      "Epoch 295/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0959 - val_loss: 0.1239\n",
      "Epoch 296/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0952 - val_loss: 0.1211\n",
      "Epoch 297/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0947 - val_loss: 0.1240\n",
      "\n",
      "Epoch 00297: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 298/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0878 - val_loss: 0.1156\n",
      "Epoch 299/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0866 - val_loss: 0.1152\n",
      "Epoch 300/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0883 - val_loss: 0.1188\n",
      "Epoch 301/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0850 - val_loss: 0.1158\n",
      "Epoch 302/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0847 - val_loss: 0.1139\n",
      "Epoch 303/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0893 - val_loss: 0.1135\n",
      "Epoch 304/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0879 - val_loss: 0.1138\n",
      "Epoch 305/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0868 - val_loss: 0.1147\n",
      "Epoch 306/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0874 - val_loss: 0.1152\n",
      "Epoch 307/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0857 - val_loss: 0.1176\n",
      "Epoch 308/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0862 - val_loss: 0.1140\n",
      "Epoch 309/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0851 - val_loss: 0.1135\n",
      "Epoch 310/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0854 - val_loss: 0.1144\n",
      "Epoch 311/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0840 - val_loss: 0.1145\n",
      "Epoch 312/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0862 - val_loss: 0.1126\n",
      "Epoch 313/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0872 - val_loss: 0.1127\n",
      "Epoch 314/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0852 - val_loss: 0.1167\n",
      "Epoch 315/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0853 - val_loss: 0.1144\n",
      "Epoch 316/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0842 - val_loss: 0.1137\n",
      "Epoch 317/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0845 - val_loss: 0.1157\n",
      "Epoch 318/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0840 - val_loss: 0.1150\n",
      "Epoch 319/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0865 - val_loss: 0.1176\n",
      "Epoch 320/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0840 - val_loss: 0.1154\n",
      "Epoch 321/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0874 - val_loss: 0.1149\n",
      "Epoch 322/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0863 - val_loss: 0.1129\n",
      "Epoch 323/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0856 - val_loss: 0.1131\n",
      "Epoch 324/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0852 - val_loss: 0.1129\n",
      "Epoch 325/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0841 - val_loss: 0.1154\n",
      "Epoch 326/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0863 - val_loss: 0.1151\n",
      "Epoch 327/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0834 - val_loss: 0.1166\n",
      "Epoch 328/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0836 - val_loss: 0.1117\n",
      "Epoch 329/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0852 - val_loss: 0.1149\n",
      "Epoch 330/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0835 - val_loss: 0.1168\n",
      "Epoch 331/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0862 - val_loss: 0.1130\n",
      "Epoch 332/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0861 - val_loss: 0.1148\n",
      "Epoch 333/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0867 - val_loss: 0.1136\n",
      "Epoch 334/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0865 - val_loss: 0.1146\n",
      "Epoch 335/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0846 - val_loss: 0.1117\n",
      "Epoch 336/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0849 - val_loss: 0.1118\n",
      "Epoch 337/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0836 - val_loss: 0.1180\n",
      "Epoch 338/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0843 - val_loss: 0.1129\n",
      "Epoch 339/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0858 - val_loss: 0.1137\n",
      "Epoch 340/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0848 - val_loss: 0.1143\n",
      "Epoch 341/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0856 - val_loss: 0.1135\n",
      "Epoch 342/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0853 - val_loss: 0.1137\n",
      "Epoch 343/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0831 - val_loss: 0.1136\n",
      "Epoch 344/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0837 - val_loss: 0.1134\n",
      "Epoch 345/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0846 - val_loss: 0.1124\n",
      "Epoch 346/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0833 - val_loss: 0.1122\n",
      "Epoch 347/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0863 - val_loss: 0.1115\n",
      "Epoch 348/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0822 - val_loss: 0.1117\n",
      "Epoch 349/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0821 - val_loss: 0.1109\n",
      "Epoch 350/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0830 - val_loss: 0.1147\n",
      "Epoch 351/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0841 - val_loss: 0.1135\n",
      "Epoch 352/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0860 - val_loss: 0.1129\n",
      "Epoch 353/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0826 - val_loss: 0.1138\n",
      "Epoch 354/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0858 - val_loss: 0.1133\n",
      "Epoch 355/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0835 - val_loss: 0.1133\n",
      "Epoch 356/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0839 - val_loss: 0.1131\n",
      "Epoch 357/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0850 - val_loss: 0.1140\n",
      "Epoch 358/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0821 - val_loss: 0.1123\n",
      "Epoch 359/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0872 - val_loss: 0.1130\n",
      "Epoch 360/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0811 - val_loss: 0.1119\n",
      "Epoch 361/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0836 - val_loss: 0.1142\n",
      "Epoch 362/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0836 - val_loss: 0.1166\n",
      "Epoch 363/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0842 - val_loss: 0.1127\n",
      "Epoch 364/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0863 - val_loss: 0.1122\n",
      "Epoch 365/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0839 - val_loss: 0.1124\n",
      "Epoch 366/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0849 - val_loss: 0.1121\n",
      "Epoch 367/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0812 - val_loss: 0.1117\n",
      "Epoch 368/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0826 - val_loss: 0.1128\n",
      "Epoch 369/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0833 - val_loss: 0.1120\n",
      "Epoch 370/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0819 - val_loss: 0.1120\n",
      "Epoch 371/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0819 - val_loss: 0.1112\n",
      "Epoch 372/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0849 - val_loss: 0.1128\n",
      "Epoch 373/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0812 - val_loss: 0.1127\n",
      "Epoch 374/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0852 - val_loss: 0.1122\n",
      "Epoch 375/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0835 - val_loss: 0.1129\n",
      "Epoch 376/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0832 - val_loss: 0.1147\n",
      "Epoch 377/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0846 - val_loss: 0.1129\n",
      "Epoch 378/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0837 - val_loss: 0.1129\n",
      "Epoch 379/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0832 - val_loss: 0.1176\n",
      "\n",
      "Epoch 00379: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 380/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0793 - val_loss: 0.1086\n",
      "Epoch 381/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0796 - val_loss: 0.1098\n",
      "Epoch 382/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0771 - val_loss: 0.1083\n",
      "Epoch 383/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0818 - val_loss: 0.1081\n",
      "Epoch 384/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0788 - val_loss: 0.1085\n",
      "Epoch 385/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0775 - val_loss: 0.1125\n",
      "Epoch 386/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0793 - val_loss: 0.1100\n",
      "Epoch 387/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0787 - val_loss: 0.1077\n",
      "Epoch 388/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0766 - val_loss: 0.1086\n",
      "Epoch 389/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0786 - val_loss: 0.1077\n",
      "Epoch 390/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0794 - val_loss: 0.1085\n",
      "Epoch 391/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0768 - val_loss: 0.1096\n",
      "Epoch 392/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0784 - val_loss: 0.1081\n",
      "Epoch 393/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0777 - val_loss: 0.1086\n",
      "Epoch 394/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0782 - val_loss: 0.1085\n",
      "Epoch 395/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0788 - val_loss: 0.1079\n",
      "Epoch 396/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0799 - val_loss: 0.1094\n",
      "Epoch 397/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0787 - val_loss: 0.1088\n",
      "Epoch 398/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0767 - val_loss: 0.1080\n",
      "Epoch 399/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0787 - val_loss: 0.1094\n",
      "Epoch 400/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0770 - val_loss: 0.1086\n",
      "Epoch 401/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0778 - val_loss: 0.1088\n",
      "Epoch 402/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0781 - val_loss: 0.1098\n",
      "Epoch 403/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0777 - val_loss: 0.1103\n",
      "Epoch 404/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0775 - val_loss: 0.1099\n",
      "Epoch 405/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0774 - val_loss: 0.1081\n",
      "Epoch 406/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0779 - val_loss: 0.1104\n",
      "Epoch 407/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0780 - val_loss: 0.1096\n",
      "Epoch 408/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0784 - val_loss: 0.1098\n",
      "Epoch 409/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0792 - val_loss: 0.1076\n",
      "Epoch 410/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0771 - val_loss: 0.1082\n",
      "Epoch 411/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0776 - val_loss: 0.1076\n",
      "Epoch 412/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0765 - val_loss: 0.1083\n",
      "Epoch 413/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0770 - val_loss: 0.1096\n",
      "Epoch 414/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0794 - val_loss: 0.1078\n",
      "Epoch 415/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0773 - val_loss: 0.1081\n",
      "Epoch 416/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0758 - val_loss: 0.1086\n",
      "Epoch 417/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0781 - val_loss: 0.1082\n",
      "Epoch 418/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0810 - val_loss: 0.1077\n",
      "Epoch 419/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0802 - val_loss: 0.1092\n",
      "Epoch 420/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0779 - val_loss: 0.1082\n",
      "Epoch 421/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0760 - val_loss: 0.1074\n",
      "Epoch 422/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0774 - val_loss: 0.1081\n",
      "Epoch 423/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0768 - val_loss: 0.1087\n",
      "Epoch 424/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0771 - val_loss: 0.1088\n",
      "Epoch 425/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0783 - val_loss: 0.1087\n",
      "Epoch 426/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0799 - val_loss: 0.1114\n",
      "Epoch 427/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0776 - val_loss: 0.1082\n",
      "Epoch 428/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0777 - val_loss: 0.1075\n",
      "Epoch 429/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0772 - val_loss: 0.1079\n",
      "Epoch 430/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0793 - val_loss: 0.1071\n",
      "Epoch 431/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0742 - val_loss: 0.1090\n",
      "Epoch 432/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0770 - val_loss: 0.1076\n",
      "Epoch 433/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0770 - val_loss: 0.1075\n",
      "Epoch 434/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0774 - val_loss: 0.1084\n",
      "Epoch 435/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0782 - val_loss: 0.1088\n",
      "Epoch 436/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0778 - val_loss: 0.1067\n",
      "Epoch 437/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0766 - val_loss: 0.1077\n",
      "Epoch 438/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0777 - val_loss: 0.1095\n",
      "Epoch 439/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0769 - val_loss: 0.1084\n",
      "Epoch 440/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0760 - val_loss: 0.1090\n",
      "Epoch 441/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0764 - val_loss: 0.1078\n",
      "Epoch 442/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0746 - val_loss: 0.1094\n",
      "Epoch 443/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0766 - val_loss: 0.1076\n",
      "Epoch 444/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0762 - val_loss: 0.1104\n",
      "Epoch 445/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0788 - val_loss: 0.1095\n",
      "Epoch 446/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0771 - val_loss: 0.1090\n",
      "Epoch 447/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0783 - val_loss: 0.1076\n",
      "Epoch 448/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0767 - val_loss: 0.1105\n",
      "Epoch 449/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0766 - val_loss: 0.1074\n",
      "Epoch 450/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0780 - val_loss: 0.1072\n",
      "Epoch 451/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0767 - val_loss: 0.1086\n",
      "Epoch 452/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0784 - val_loss: 0.1078\n",
      "Epoch 453/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0766 - val_loss: 0.1076\n",
      "Epoch 454/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0776 - val_loss: 0.1090\n",
      "Epoch 455/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0756 - val_loss: 0.1095\n",
      "Epoch 456/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0779 - val_loss: 0.1077\n",
      "Epoch 457/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0750 - val_loss: 0.1085\n",
      "Epoch 458/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0762 - val_loss: 0.1071\n",
      "Epoch 459/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0764 - val_loss: 0.1095\n",
      "Epoch 460/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0781 - val_loss: 0.1077\n",
      "Epoch 461/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0755 - val_loss: 0.1080\n",
      "Epoch 462/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0784 - val_loss: 0.1092\n",
      "Epoch 463/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0758 - val_loss: 0.1076\n",
      "Epoch 464/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0765 - val_loss: 0.1087\n",
      "Epoch 465/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0768 - val_loss: 0.1071\n",
      "Epoch 466/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0755 - val_loss: 0.1082\n",
      "\n",
      "Epoch 00466: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 467/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0774 - val_loss: 0.1072\n",
      "Epoch 468/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0744 - val_loss: 0.1057\n",
      "Epoch 469/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0748 - val_loss: 0.1068\n",
      "Epoch 470/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0742 - val_loss: 0.1057\n",
      "Epoch 471/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0742 - val_loss: 0.1062\n",
      "Epoch 472/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0742 - val_loss: 0.1058\n",
      "Epoch 473/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0751 - val_loss: 0.1059\n",
      "Epoch 474/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0749 - val_loss: 0.1054\n",
      "Epoch 475/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0759 - val_loss: 0.1062\n",
      "Epoch 476/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0742 - val_loss: 0.1068\n",
      "Epoch 477/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0740 - val_loss: 0.1064\n",
      "Epoch 478/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0737 - val_loss: 0.1063\n",
      "Epoch 479/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0726 - val_loss: 0.1057\n",
      "Epoch 480/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0752 - val_loss: 0.1056\n",
      "Epoch 481/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0759 - val_loss: 0.1062\n",
      "Epoch 482/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0739 - val_loss: 0.1052\n",
      "Epoch 483/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0735 - val_loss: 0.1061\n",
      "Epoch 484/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0747 - val_loss: 0.1058\n",
      "Epoch 485/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0735 - val_loss: 0.1063\n",
      "Epoch 486/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0732 - val_loss: 0.1059\n",
      "Epoch 487/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0733 - val_loss: 0.1049\n",
      "Epoch 488/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0736 - val_loss: 0.1070\n",
      "Epoch 489/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0758 - val_loss: 0.1056\n",
      "Epoch 490/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0739 - val_loss: 0.1058\n",
      "Epoch 491/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0738 - val_loss: 0.1058\n",
      "Epoch 492/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0732 - val_loss: 0.1061\n",
      "Epoch 493/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0747 - val_loss: 0.1053\n",
      "Epoch 494/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0747 - val_loss: 0.1063\n",
      "Epoch 495/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0743 - val_loss: 0.1053\n",
      "Epoch 496/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0750 - val_loss: 0.1056\n",
      "Epoch 497/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0782 - val_loss: 0.1053\n",
      "Epoch 498/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0747 - val_loss: 0.1056\n",
      "Epoch 499/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0754 - val_loss: 0.1064\n",
      "Epoch 500/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0750 - val_loss: 0.1055\n",
      "Epoch 501/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0750 - val_loss: 0.1057\n",
      "Epoch 502/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0737 - val_loss: 0.1070\n",
      "Epoch 503/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0739 - val_loss: 0.1060\n",
      "Epoch 504/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0737 - val_loss: 0.1065\n",
      "Epoch 505/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0747 - val_loss: 0.1053\n",
      "Epoch 506/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0742 - val_loss: 0.1056\n",
      "Epoch 507/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0761 - val_loss: 0.1084\n",
      "Epoch 508/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0751 - val_loss: 0.1055\n",
      "Epoch 509/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0750 - val_loss: 0.1052\n",
      "Epoch 510/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0752 - val_loss: 0.1072\n",
      "Epoch 511/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0731 - val_loss: 0.1061\n",
      "Epoch 512/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0733 - val_loss: 0.1051\n",
      "Epoch 513/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0753 - val_loss: 0.1068\n",
      "Epoch 514/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0734 - val_loss: 0.1050\n",
      "Epoch 515/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0742 - val_loss: 0.1054\n",
      "Epoch 516/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0747 - val_loss: 0.1067\n",
      "Epoch 517/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0738 - val_loss: 0.1058\n",
      "\n",
      "Epoch 00517: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 518/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0731 - val_loss: 0.1053\n",
      "Epoch 519/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0725 - val_loss: 0.1051\n",
      "Epoch 520/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0724 - val_loss: 0.1051\n",
      "Epoch 521/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0755 - val_loss: 0.1051\n",
      "Epoch 522/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0744 - val_loss: 0.1048\n",
      "Epoch 523/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0730 - val_loss: 0.1046\n",
      "Epoch 524/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0726 - val_loss: 0.1056\n",
      "Epoch 525/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0736 - val_loss: 0.1052\n",
      "Epoch 526/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0715 - val_loss: 0.1051\n",
      "Epoch 527/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0732 - val_loss: 0.1048\n",
      "Epoch 528/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0720 - val_loss: 0.1050\n",
      "Epoch 529/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0743 - val_loss: 0.1052\n",
      "Epoch 530/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0719 - val_loss: 0.1047\n",
      "Epoch 531/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0726 - val_loss: 0.1049\n",
      "Epoch 532/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0727 - val_loss: 0.1059\n",
      "Epoch 533/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1049\n",
      "Epoch 534/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0731 - val_loss: 0.1052\n",
      "Epoch 535/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0744 - val_loss: 0.1052\n",
      "Epoch 536/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0734 - val_loss: 0.1045\n",
      "Epoch 537/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0743 - val_loss: 0.1045\n",
      "Epoch 538/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0717 - val_loss: 0.1045\n",
      "Epoch 539/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0731 - val_loss: 0.1049\n",
      "Epoch 540/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0739 - val_loss: 0.1049\n",
      "Epoch 541/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0726 - val_loss: 0.1046\n",
      "Epoch 542/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0740 - val_loss: 0.1046\n",
      "Epoch 543/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0718 - val_loss: 0.1045\n",
      "Epoch 544/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0722 - val_loss: 0.1043\n",
      "Epoch 545/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0721 - val_loss: 0.1049\n",
      "Epoch 546/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0744 - val_loss: 0.1048\n",
      "Epoch 547/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0747 - val_loss: 0.1043\n",
      "Epoch 548/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0746 - val_loss: 0.1043\n",
      "Epoch 549/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0729 - val_loss: 0.1049\n",
      "Epoch 550/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0726 - val_loss: 0.1063\n",
      "Epoch 551/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0724 - val_loss: 0.1046\n",
      "Epoch 552/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0724 - val_loss: 0.1053\n",
      "Epoch 553/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0711 - val_loss: 0.1045\n",
      "Epoch 554/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0748 - val_loss: 0.1045\n",
      "Epoch 555/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0712 - val_loss: 0.1043\n",
      "Epoch 556/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1047\n",
      "Epoch 557/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1053\n",
      "Epoch 558/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0726 - val_loss: 0.1053\n",
      "Epoch 559/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0716 - val_loss: 0.1047\n",
      "Epoch 560/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0726 - val_loss: 0.1047\n",
      "Epoch 561/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0731 - val_loss: 0.1052\n",
      "Epoch 562/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0726 - val_loss: 0.1058\n",
      "Epoch 563/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0729 - val_loss: 0.1043\n",
      "Epoch 564/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0726 - val_loss: 0.1045\n",
      "Epoch 565/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0729 - val_loss: 0.1047\n",
      "Epoch 566/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0724 - val_loss: 0.1043\n",
      "Epoch 567/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0727 - val_loss: 0.1045\n",
      "Epoch 568/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1049\n",
      "Epoch 569/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0719 - val_loss: 0.1052\n",
      "Epoch 570/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0738 - val_loss: 0.1047\n",
      "Epoch 571/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0713 - val_loss: 0.1048\n",
      "Epoch 572/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0737 - val_loss: 0.1045\n",
      "Epoch 573/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1047\n",
      "Epoch 574/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0738 - val_loss: 0.1051\n",
      "\n",
      "Epoch 00574: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 575/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0732 - val_loss: 0.1045\n",
      "Epoch 576/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0734 - val_loss: 0.1042\n",
      "Epoch 577/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0723 - val_loss: 0.1042\n",
      "Epoch 578/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1042\n",
      "Epoch 579/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0736 - val_loss: 0.1042\n",
      "Epoch 580/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0706 - val_loss: 0.1043\n",
      "Epoch 581/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0714 - val_loss: 0.1043\n",
      "Epoch 582/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0716 - val_loss: 0.1041\n",
      "Epoch 583/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0719 - val_loss: 0.1040\n",
      "Epoch 584/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0714 - val_loss: 0.1040\n",
      "Epoch 585/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0728 - val_loss: 0.1041\n",
      "Epoch 586/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0711 - val_loss: 0.1043\n",
      "Epoch 587/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0716 - val_loss: 0.1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 588/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0723 - val_loss: 0.1042\n",
      "Epoch 589/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0733 - val_loss: 0.1041\n",
      "Epoch 590/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0703 - val_loss: 0.1041\n",
      "Epoch 591/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0717 - val_loss: 0.1042\n",
      "Epoch 592/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0705 - val_loss: 0.1040\n",
      "Epoch 593/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0726 - val_loss: 0.1044\n",
      "Epoch 594/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0718 - val_loss: 0.1044\n",
      "Epoch 595/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0733 - val_loss: 0.1040\n",
      "Epoch 596/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0708 - val_loss: 0.1044\n",
      "Epoch 597/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0720 - val_loss: 0.1040\n",
      "Epoch 598/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0723 - val_loss: 0.1044\n",
      "Epoch 599/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0711 - val_loss: 0.1043\n",
      "Epoch 600/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0720 - val_loss: 0.1042\n",
      "Epoch 601/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0725 - val_loss: 0.1039\n",
      "Epoch 602/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0709 - val_loss: 0.1039\n",
      "Epoch 603/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0721 - val_loss: 0.1044\n",
      "Epoch 604/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0723 - val_loss: 0.1039\n",
      "Epoch 605/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0702 - val_loss: 0.1043\n",
      "Epoch 606/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0699 - val_loss: 0.1040\n",
      "Epoch 607/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0741 - val_loss: 0.1041\n",
      "Epoch 608/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0725 - val_loss: 0.1042\n",
      "Epoch 609/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0718 - val_loss: 0.1040\n",
      "Epoch 610/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0704 - val_loss: 0.1040\n",
      "Epoch 611/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0722 - val_loss: 0.1041\n",
      "Epoch 612/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0723 - val_loss: 0.1046\n",
      "Epoch 613/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0722 - val_loss: 0.1039\n",
      "\n",
      "Epoch 00613: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 614/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0718 - val_loss: 0.1039\n",
      "Epoch 615/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0728 - val_loss: 0.1040\n",
      "Epoch 616/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0720 - val_loss: 0.1044\n",
      "Epoch 617/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0732 - val_loss: 0.1039\n",
      "Epoch 618/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0712 - val_loss: 0.1037\n",
      "Epoch 619/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0707 - val_loss: 0.1038\n",
      "Epoch 620/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0716 - val_loss: 0.1038\n",
      "Epoch 621/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0702 - val_loss: 0.1041\n",
      "Epoch 622/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0714 - val_loss: 0.1039\n",
      "Epoch 623/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0715 - val_loss: 0.1039\n",
      "Epoch 624/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0720 - val_loss: 0.1038\n",
      "Epoch 625/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0719 - val_loss: 0.1039\n",
      "Epoch 626/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0703 - val_loss: 0.1039\n",
      "Epoch 627/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0710 - val_loss: 0.1041\n",
      "Epoch 628/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0724 - val_loss: 0.1039\n",
      "Epoch 629/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0718 - val_loss: 0.1038\n",
      "Epoch 630/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0703 - val_loss: 0.1043\n",
      "Epoch 631/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0722 - val_loss: 0.1039\n",
      "Epoch 632/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0720 - val_loss: 0.1041\n",
      "Epoch 633/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0721 - val_loss: 0.1042\n",
      "Epoch 634/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0719 - val_loss: 0.1040\n",
      "Epoch 635/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0727 - val_loss: 0.1040\n",
      "Epoch 636/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0706 - val_loss: 0.1041\n",
      "Epoch 637/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0700 - val_loss: 0.1043\n",
      "Epoch 638/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0702 - val_loss: 0.1040\n",
      "Epoch 639/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0706 - val_loss: 0.1041\n",
      "Epoch 640/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0701 - val_loss: 0.1041\n",
      "Epoch 641/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0701 - val_loss: 0.1042\n",
      "Epoch 642/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0726 - val_loss: 0.1039\n",
      "Epoch 643/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0693 - val_loss: 0.1040\n",
      "Epoch 644/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0706 - val_loss: 0.1039\n",
      "Epoch 645/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0720 - val_loss: 0.1039\n",
      "Epoch 646/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0721 - val_loss: 0.1040\n",
      "Epoch 647/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0705 - val_loss: 0.1039\n",
      "Epoch 648/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0721 - val_loss: 0.1041\n",
      "\n",
      "Epoch 00648: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 649/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0722 - val_loss: 0.1040\n",
      "Epoch 650/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0699 - val_loss: 0.1037\n",
      "Epoch 651/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0711 - val_loss: 0.1039\n",
      "Epoch 652/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0697 - val_loss: 0.1039\n",
      "Epoch 653/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0719 - val_loss: 0.1039\n",
      "Epoch 654/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0697 - val_loss: 0.1038\n",
      "Epoch 655/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0730 - val_loss: 0.1040\n",
      "Epoch 656/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0713 - val_loss: 0.1038\n",
      "Epoch 657/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0692 - val_loss: 0.1040\n",
      "Epoch 658/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0707 - val_loss: 0.1040\n",
      "Epoch 659/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0705 - val_loss: 0.1041\n",
      "Epoch 660/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0709 - val_loss: 0.1040\n",
      "Epoch 661/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0713 - val_loss: 0.1040\n",
      "Epoch 662/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0713 - val_loss: 0.1039\n",
      "Epoch 663/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0724 - val_loss: 0.1038\n",
      "Epoch 664/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0698 - val_loss: 0.1037\n",
      "Epoch 665/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0696 - val_loss: 0.1038\n",
      "Epoch 666/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0689 - val_loss: 0.1038\n",
      "Epoch 667/2000\n",
      "133228/133228 [==============================] - 1s 9us/step - loss: 0.0707 - val_loss: 0.1037\n",
      "Epoch 668/2000\n",
      "133228/133228 [==============================] - 1s 10us/step - loss: 0.0718 - val_loss: 0.1040\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00668: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3xV9f348df73tzsRUiAQNhLhsiI\nuPcEFa3SKq39Vlu12lrb2oUd1tqhrdaqra119/t1UKs/LVqtE1dbEbCAAjJkhpEFZK977/v3x+ck\nJCEhg1ySy30/H4887hmfc877XsJ95zPO54iqYowxxhyIr7cDMMYY0/dZsjDGGNMhSxbGGGM6ZMnC\nGGNMhyxZGGOM6ZAlC2OMMR2yZGFMJ4nIL0SkRER29XYsxhxqlixMVBGRzSJyZi9cdyjwHWCiqg7q\noXMuEpFiESkXkRUicmGzfaeKSEGz9bdE5KpWx7dVptaLtXHbmSKyuSfiNbHNkoUxnTMcKFXVoq4e\nKCJx7ez6JpCrqunANcDjIpJ7EDECVAE/OchzGLMfSxbmsCEiV4vIBhHZLSILRWSwt11E5HciUiQi\nZSKyUkQme/tmi8hqEakQke0i8t02znsm8BowWEQqReQxb/scEVklInu9v+onNDtms4j8QERWAlVt\nJQxVXamqwcZVIAAMbV2ui+4F5onImIM8jzEtWLIwhwUROR24DfgckAtsARZ4u88GTgbGAZnApUCp\nt+9h4KuqmgZMBt5sfW5VfR2YBexQ1VRVvUJExgFPAd8CcoCXgBdEJL7ZofOA84DMZkmhddwvikgt\nsBh4C1jarQ9gn+3Ag8AtB3keY1qwZGEOF18AHlHVD1W1DrgJOE5ERgANQBpwBCCqukZVd3rHNQAT\nRSRdVfeo6oedvN6lwD9U9TVVbQDuBJKA45uVuVdVt6lqTXsnUdXzvdhmA6+oavgA17zXq8XsFZG9\nwIvtlLsNuEBEJnXyvRjTIUsW5nAxGFebAEBVK3G1hyGq+ibwB+A+oFBEHhCRdK/oJbgv6i0i8raI\nHNfN64WBbcCQZmW2deZEqtqgqi8D54jInAMUvUFVMxt/gPPbOV8x7v3e2pnrG9MZlizM4WIHrhMa\nABFJAfrjmmVQ1XtVdQYwCdcc9T1v+xJVvRAYADwPPN3N6wmuv2F7szJdndI5DhjdxWPacwdwGjCj\nh85nYpwlCxONAiKS2OwnDngSuFJEpopIAvArYLGqbhaRo0XkGBEJ4EYL1QIhEYkXkS+ISIbXlFQO\nhDoZw9PAeSJyhnfe7wB1wL87c7CIHCEis0QkSUQCInI5rl/l7a58EO1R1b3Ab4Hv98T5jLFkYaLR\nS0BNs59bVPUN3JDRZ4GduL/QL/PKp+M6fffgmo5KcX0MAF8ENotIOXAtcHlnAlDVtV7Z3wMlwAXA\nBapa38n3ILhO6CKgGDeM9tJWfSYH+7CZe+h88jPmgMQefmRM3+P1XdyqqlN7OxZjwGoWxvQ5XrPa\nJRz8MFpjekx7d5YaY3qBiGTgRlEtA/6nl8Mxpok1QxljjOmQNUMZY4zpUNQ1Q2VnZ+uIESN6Owxj\njIkqy5YtK1HVnO4eH3XJYsSIESxdav1+xhjTFSKypeNS7bNmKGOMMR2yZGGMMaZDliyMMcZ0KOr6\nLNrS0NBAQUEBtbW1vR3KYSMxMZG8vDwCgUBvh2KM6QMimixE5Fzc/DR+4CFVvb2NMp/DzZGjwApV\n/XxXr1NQUEBaWhojRozATf5pDoaqUlpaSkFBASNHjuztcIwxfUDEkoWI+HHPDzgLKACWiMhCVV3d\nrMxY3ENqTlDVPSIyoDvXqq2ttUTRg0SE/v37U1xc3NuhGGP6iEj2WcwENqjqRm8mzgXAha3KXA3c\np6p7AFS1qLsXs0TRs+zzNMY0F8lkMYSWTworoOVTxMA9hGaciPxLRN73mq32IyLXiMhSEVna3b92\nq+qC7CqrJWzTmxhjTJdFMlm09adp62/qOGAscCru4fYPiUjmfgepPqCq+aqan5PTvRsQq+uDFFXU\nEolcUVpaytSpU5k6dSqDBg1iyJAhTev19Z17vMGVV17J2rVrez44Y4zpAZHs4C7APWayUR7uUZSt\ny7zvPaVsk4isxSWPJRGMq8f179+f5cuXA3DLLbeQmprKd7/73RZlVBVVxedrOz8/+uijEY/TGGO6\nK5I1iyXAWBEZKSLxuKeWLWxV5nncc4IRkWxcs9TGyITTWNE5dM1QGzZsYPLkyVx77bVMnz6dnTt3\ncs0115Cfn8+kSZO49dZbm8qeeOKJLF++nGAwSGZmJvPnz+eoo47iuOOOo6io2105xhjTIyJWs1DV\noIhcD7yCGzr7iKquEpFbgaWqutDbd7aIrMY9/vF7qlp6MNf92QurWL2jfL/tDaEw9cEwKQldf8sT\nB6fz0wsmdSue1atX8+ijj3L//fcDcPvtt5OVlUUwGOS0005j7ty5TJw4scUxZWVlnHLKKdx+++3c\neOONPPLII8yfP79b1zfGmJ4Q0fssVPUl3POSm2+7udmyAjd6P4eE0nZnSqSMHj2ao48+umn9qaee\n4uGHHyYYDLJjxw5Wr169X7JISkpi1qxZAMyYMYN33333EEZsjDH7Oyzu4G6uvRpASWUdO/bWMDE3\nnTj/oZvlJCUlpWl5/fr13HPPPXzwwQdkZmZy+eWXt3nXeXx8fNOy3+8nGAwekliNMaY9NjfUIVRe\nXk5aWhrp6ens3LmTV155pbdDMsaYTjnsahZ92fTp05k4cSKTJ09m1KhRnHDCCb0dkjHGdErUPYM7\nPz9fWz/8aM2aNUyYMOGAx5VW1rF9bw0TctMJHMJmqGjWmc/VGBMdRGSZquZ393j71jTGGNMhSxbG\nGGM6ZMnCGGNMh2InWXg3V0RZF40xxvQJMZMsbMJtY4zpvphJFr0xN5QxxhwuYihZRM6pp5663w12\nd999N1/72tfaPSY1NRWAHTt2MHfu3HbP23qYcGt333031dXVTeuzZ89m7969nQ3dGGM6xZJFD5g3\nbx4LFixosW3BggXMmzevw2MHDx7MM8880+1rt04WL730EpmZ+z0SxBhjDkrMJItINkLNnTuXF198\nkbq6OgA2b97Mjh07mDp1KmeccQbTp0/nyCOP5O9///t+x27evJnJkycDUFNTw2WXXcaUKVO49NJL\nqampaSp33XXXNU1t/tOf/hSAe++9lx07dnDaaadx2mmnATBixAhKSkoAuOuuu5g8eTKTJ0/m7rvv\nbrrehAkTuPrqq5k0aRJnn312i+sYY0xbDr/pPl6eD7s+2m9zajjMqIYwgXg/dPX50oOOhFm3t7u7\nf//+zJw5k3/+859ceOGFLFiwgEsvvZSkpCSee+450tPTKSkp4dhjj2XOnDntPt/6T3/6E8nJyaxc\nuZKVK1cyffr0pn2//OUvycrKIhQKccYZZ7By5UpuuOEG7rrrLhYtWkR2dnaLcy1btoxHH32UxYsX\no6occ8wxnHLKKfTr14/169fz1FNP8eCDD/K5z32OZ599lssvv7xrn4kxJqbETM0i0po3RTU2Qakq\nP/zhD5kyZQpnnnkm27dvp7CwsN1zvPPOO01f2lOmTGHKlClN+55++mmmT5/OtGnTWLVqFatXrz5g\nPO+99x6f+cxnSElJITU1lYsvvrhpqvORI0cydepUwE2Bvnnz5oN568aYGHD41SzaqQFUVdezdXc1\n4wamkRjw9/hlL7roIm688UY+/PBDampqmD59Oo899hjFxcUsW7aMQCDAiBEj2pySvLm2ah2bNm3i\nzjvvZMmSJfTr148rrriiw/McaM6vhISEpmW/32/NUMaYDlnNooekpqZy6qmn8uUvf7mpY7usrIwB\nAwYQCARYtGgRW7ZsOeA5Tj75ZJ544gkAPv74Y1auXAm4qc1TUlLIyMigsLCQl19+uemYtLQ0Kioq\n2jzX888/T3V1NVVVVTz33HOcdNJJPfV2jTEx5vCrWfSiefPmcfHFFzc1R33hC1/gggsuID8/n6lT\np3LEEUcc8PjrrruOK6+8kilTpjB16lRmzpwJwFFHHcW0adOYNGnSflObX3PNNcyaNYvc3FwWLVrU\ntH369OlcccUVTee46qqrmDZtmjU5GWO6JWamKN8b4Waow5FNUW7M4cOmKO8km+7DGGO6L2aSRaMo\nq0gZY0yfcNgkiw6b07p6b0WMi7bmSWNMZB0WySIxMZHS0tJOfsHZl2BHVJXS0lISExN7OxRjTB9x\nWIyGysvLo6CggOLi4nbL1DSEKK2sR/ckEB93WOTIiEpMTCQvL6+3wzDG9BGHRbIIBAKMHDnygGVe\nX13I1QuX8sL1JzIhL+MQRWaMMYeHmPkTu7HLImxt8cYY02Uxkyx8XrawVGGMMV0XM8kCq1kYY0y3\nRTRZiMi5IrJWRDaIyPw29l8hIsUistz7uSpisXivliuMMabrItbBLSJ+4D7gLKAAWCIiC1W19dza\nf1XV6yMVRyOf2DO4jTGmuyJZs5gJbFDVjapaDywALozg9Q5oXwd3b0VgjDHRK5LJYgiwrdl6gbet\ntUtEZKWIPCMiQ9s6kYhcIyJLRWTpge6lOBDxGqKsGcoYY7ouksmirfk1Wn9VvwCMUNUpwOvAX9o6\nkao+oKr5qpqfk5PTrWB81sFtjDHdFslkUQA0rynkATuaF1DVUlWt81YfBGZELBppvGbErmCMMYet\nSCaLJcBYERkpIvHAZcDC5gVEJLfZ6hxgTaSC2XefhWULY4zpqoiNhlLVoIhcD7wC+IFHVHWViNwK\nLFXVhcANIjIHCAK7gSsiFY8NnTXGmO6L6NxQqvoS8FKrbTc3W74JuCmSMTQSsQ5uY4zprpi5g7ux\ng9uaoYwxputiJlnYfRbGGNN9MZQsGpuhLFsYY0xXxU6y8F4tVxhjTNfFTrKwobPGGNNtMZMsfHZT\nnjHGdFvMJIvGuaGsg9sYY7oudpJFU83CsoUxxnRVzCULq1kYY0zXxU6ywB5+ZIwx3RUzycLnvVNr\nhTLGmK6LmWRhHdzGGNN9sZMsbG4oY4zptphJFv5QLZlUoFa1MMaYLouZZJHx0aMsT/wqEqzp7VCM\nMSbqxEyyEJ/fvWqwlyMxxpjoEzPJAl/AvYYtWRhjTFfFULLwHgoYaujdOIwxJgrFTrLwu5qFhEO9\nHIgxxkSf2EkWXs1CrRnKGGO6LGaShfi9Dm5LFsYY02Uxkyysg9sYY7ovhpKFa4aSsHVwG2NMV8VM\nspDGDm61Dm5jjOmqmEkWTUNnrWZhjDFdFjvJwobOGmNMt8VMspDGmoU1QxljTJfFTLLA75KFz5qh\njDGmy2ImWTR2cGPNUMYY02URTRYicq6IrBWRDSIy/wDl5oqIikh+xGKxobPGGNNtEUsWIuIH7gNm\nAROBeSIysY1yacANwOJIxQI0NUNZB7cxxnRdJGsWM4ENqrpRVeuBBcCFbZT7OfAboDaCsSC+xvss\nrGZhjDFdFclkMQTY1my9wNvWRESmAUNV9cUDnUhErhGRpSKytLi4uFvBSJzVLIwxprsimSykjW1N\nD8AWER/wO+A7HZ1IVR9Q1XxVzc/JyeleMI01C5sbyhhjuiySyaIAGNpsPQ/Y0Ww9DZgMvCUim4Fj\ngYWR6uTeNxrKkoUxxnRVJJPFEmCsiIwUkXjgMmBh405VLVPVbFUdoaojgPeBOaq6NCLR+O0Z3MYY\n010RSxaqGgSuB14B1gBPq+oqEblVROZE6rrt8XnNUD6rWRhjTJfFRfLkqvoS8FKrbTe3U/bUSMbS\nODeUTfdhjDFdF0N3cLtmKL81QxljTJfFTLLw+ePdgjVDGWNMl8VMsmia7sOaoYwxpstiJln4/HGE\nVayD2xhjuiFmkoUIBPGB9VkYY0yXxVCyEEL4kZDNDWWMMV0VM8kCIIgfDVnNwhhjuiqmkkVI/Kj1\nWRhjTJfFVrIgzobOGmNMN8RYsvBbsjDGmG6IrWQhfnusqjHGdEOnkoWIjBaRBG/5VBG5QUQyIxta\nzwuLH+zhR8YY02WdrVk8C4REZAzwMDASeDJiUUVISOKsZmGMMd3Q2WQR9qYc/wxwt6p+G8iNXFiR\noVazMMaYbulssmgQkXnAl4DG52UHIhNS5IQlzh6raowx3dDZZHElcBzwS1XdJCIjgccjF1ZkhCUO\nn033YYwxXdaphx+p6mrgBgAR6QekqertkQwsElT8iDVDGWNMl3V2NNRbIpIuIlnACuBREbkrsqH1\nvLDPahbGGNMdnW2GylDVcuBi4FFVnQGcGbmwIsSaoYwxpls6myziRCQX+Bz7Orijjvri8NnDj4wx\npss6myxuBV4BPlXVJSIyClgfubAiQ60ZyhhjuqWzHdx/A/7WbH0jcEmkgooYn99qFsYY0w2d7eDO\nE5HnRKRIRApF5FkRyYt0cD1N/AH8GkJVezsUY4yJKp1thnoUWAgMBoYAL3jboor44/AToj4U7u1Q\njDEmqnQ2WeSo6qOqGvR+HgNyIhhXRIg/QIAQtfWWLIwxpis6myxKRORyEfF7P5cDpZEMLBJ8/gB+\nCVHTYP0WxhjTFZ1NFl/GDZvdBewE5uKmAIkqvjhXs6iutxFRxhjTFZ1KFqq6VVXnqGqOqg5Q1Ytw\nN+hFFZ8/QI6Ukfqf3/R2KMYYE1UO5kl5N3ZUQETOFZG1IrJBROa3sf9aEflIRJaLyHsiMvEg4umQ\nP85NlDvgw3sieRljjDnsHEyykAPuFPED9wGzgInAvDaSwZOqeqSqTgV+A0R0vilJTI/k6Y0x5rB1\nMMmio5sVZgIbVHWjqtYDC4ALW5zAzTfVKKUT5zwoDYOmRfL0xhhz2DrgHdwiUkHbX+ACJHVw7iHA\ntmbrBcAxbVzj67gmrXjg9HbiuAa4BmDYsGEdXLZ94REnd/tYY4yJZQesWahqmqqmt/GTpqodTRXS\nVjPVfolHVe9T1dHAD4AftxPHA6qar6r5OTndv70jMSmFOxs+61ZC9ixuY4zprINphupIATC02Xoe\nsOMA5RcAF0UwHlIT46gl3q001ETyUsYYc1iJZLJYAowVkZEiEg9chpsypImIjG22eh4Rnsk2LSGO\nBvGSRbAukpcyxpjDSqdmne0OVQ2KyPW4qc39wCOqukpEbgWWqupC4HoRORNoAPYAX4pUPAAigi+Q\n6BrDglazMMaYzopYsgBQ1ZeAl1ptu7nZ8jcjef22+BOSoRarWRhjTBdEshmqT4pP8AZxWZ+FMcZ0\nWswli0BiiluwmoUxxnRazCWLxKRkt2B9FsYY02kxlyySkq1mYYwxXRVzySIxOQ2A+qo9vRyJMcZE\nj5hLFmSNoV791Bcs7+1IjDEmasRcsshIS2GNDkd2rujtUIwxJmrEXLLITI6nUPtBdUlvh2KMMVEj\n5pJFVko8lSQhdZW9HYoxxkSNmEsW/VPjqdYEpKGqt0MxxpioEXPJol9yPNWSRFywurdDMcaYqBFz\nycLvEzSQQkDrIBTs7XCMMSYqxFyyACDB3WtBvfVbGGNMZ8RksggkNSYL67cwxpjOiMlkkZ6eCYDW\nVfRyJMYYEx1iMllk9esHQOkem/LDGGM6IyaTRfaAXACKdhX0ciTGGBMdYjJZDBw6GoDyws29G4gx\nxkSJmEwWA3KHE1Qf9bu39XYoxhgTFWIyWYg/jt3+/vgqdvR2KMYYExViMlkAVMYPJL1mG4RDvR2K\nMcb0eTGbLBpSc5kS/gRuzYJbMmDxn3s7JGOM6bNiNlkkp2a03PDy93snEGOMiQIxmywGxdfuvzFY\nf+gDMcaYKBCzySJw5o/337jiSQiHD30wxhjTx8VssmDABB4ee1/LbS98E27tB2/+sndiMsaYPip2\nkwWQOeKotne885tDG4gxxvRxMZ0sxg4fyl+Dp/Z2GMYY0+fFdLIYNzCNP4Qv7u0wjDGmz4toshCR\nc0VkrYhsEJH5bey/UURWi8hKEXlDRIZHMp7WEgN+sgaP5sH0bxzKyxpjTNSJWLIQET9wHzALmAjM\nE5GJrYr9F8hX1SnAM8Ah7yyYOTKLO0pPONSXNcaYqBLJmsVMYIOqblTVemABcGHzAqq6SFWrvdX3\ngbwIxtOm08YPoD5kw2WNMeZAIpkshgDNp3Ut8La15yvAy23tEJFrRGSpiCwtLi7uwRDh6JFZpCfG\nsU0Gt9xxSwao9ui1jDEmWkUyWUgb29r89hWRy4F84I629qvqA6qar6r5OTk5PRgiBPw+zpw4kNNr\nbmdbv2Nb7nz3zh69ljHGRKtIJosCYGiz9TxgvznBReRM4EfAHFWti2A87frFRZMZ0j+dd8oGtNzx\n5i96IxxjjOlzIpkslgBjRWSkiMQDlwELmxcQkWnAn3GJoiiCsRxQcnwcP7twMj+rvoSVA1sNpbXp\nP4wxJnLJQlWDwPXAK8Aa4GlVXSUit4rIHK/YHUAq8DcRWS4iC9s5XcSdMi6H86eN4MqtZ7fcsfYl\ne+aFMSbmiUZZJ25+fr4uXbo0Iuf+ZFc55979Lu+OeYqhBS/s23HZk3DEeRG5pjHGHAoiskxV87t7\nfEzfwd3amJxUAE7aMI+yk366b8euj3spImOM6RssWTQT5/fxo9kTAHh4ffK+He//EYo+6aWojDGm\n91myaOXqk0fx/XPH868t1fs21u6FPx4D798PfzweCpb1XoDGGNMLLFm04csnjCQxNXP/Hf/8ARSt\ngme/4tZV4eX5ljyMMYc9SxZtSAz4+epnL+CSup/ydsJp+xfYsxm2vg+b3oHFf4KHTnd3fG+3pGGM\nOTxZsmjHyeNyyJl4Ci9Wjm2549zbAYVHzoH/ndNy36LboLZ8/5O1tc0YY6KIJYsDuOOzU/ggYxZF\n6pqkQp/9Xxh7dvsHbHgNnr+u5bYVC+D2oVC8LoKRGmNMZFmyOIC0xAD3fzGfa+u/xarwcH69fgia\nNQp+uBNO+1HbB218u+X6J/9wr4UfRTZYY4yJIEsWHZiQm86ff/g1zqu/jQfeL+T+tzdCfDKMObPt\nA7TV9CD+ePcarIP6KpvJ1hgTlewO7k7aVFLFOb97h4ZwmLnT8/j2WeMY7C8DXxy8cwcsvn9f4bFn\nu87v7LGwq1WN4oJ7YMYVhzR2Y4yxO7gPkZHZKfznptMZmZ3C35YVcPztb7KuOgVSsmHWr1sWXv8q\nBGv3TxQAqxdCRSHs3gSLfgW/zIU1L+xfDlzH+Ks/gYbann9DxhjTBZYsuqB/agLPf/0EfnPJFADO\n/t07vL+xtGWhY78GQ4+FY65t+ySfvgG/HQf3ToW3fw0N1W4Yblve+x38+1547Sc9+C6MMabr4no7\ngGiTnhjgc0cP5e31xby+upDLHnifjKQAfzn/JaYOSYPcKfsKHzUPHjgFjr/Bfem35z9/gKyRcOTn\nYO9WWPWc60Cvr3L7P3gA8r8MAyZE9s0ZY0w7rM/iIJTVNPD4+1u445W1xMf5ePEbJzJuYFrLQnUV\nkJDmksCuj2HBvPZPmDHMPV9w71Y47cewqNXDl27eDT5/x4EVr4N+wyEuYf99W/4DQ48BX7NKZX0V\n/OseOOk7bR9jjIl61mfRizKSAnz9tDH8/KLJ1AfDnP27d/jda+vYU1W/r1CClzwyh8H4WTBu1r59\nJ3wTEr1pRfqNhLKtLlHA/okCoGQ93H+Su1t858qW+xpqIBR0fSH3HQ2v3bz/8etfg0fPhaUPt9z+\n3u9ck9jyJ7r2ARhjYoY1Q/WALx47HFXlloWruOeN9dzzxnpGZafw7bPGccKYbLJSvOGzIjDvKdjw\nOow+w/11P/1L7gt+7Jnw+s/gvbvgyn+60VRv/arlhZY+Aru8JLH+VdfktWcz/PcJePe3MOkiGDDR\n7V98PxxxPow8ad/xpRvca+Gqluet9vpdQg09+rkYYw4f1gzVg1SV+xZt4M5X992tHfALsybn8qXj\nRzBjeL8DnyAchrJtrgkJ4LHzYfO7kDkc9m5pWXbcLLetaHUHUQlceB9M/byrPbx1mxvue+kTMP5c\nV+T5r8Pyx+Gc2+C4r7ltVaXw0ndg9m8hpX/LU1aVQFwiJKR2cG3cfSWPnAvHfBUmX9xxeWNMRBxs\nM5QliwhQVT4trqK0so4fP/8x64sqAZg/6wiuPmkUfp907YShIPy82Rf2wMlQ2OqBTM0TSnwaXP2m\nmx23sSZy9FWw5KGWx1z+rLu58Kl57vGxCelQVw5X/AP+7zMQ8prTLvqTSzaNbslwr7PvhJlXt37z\nboqT8bMgKRNq9sCvR3jHlXXtfRtjeowliz5uV1ktD767kTc/KWJTSVXT9mevO77jmkZzVaXwz/kw\ndZ5LFu/dDduXwrbFMOsOmP5F2LnCTXCYMgC+tx7qq+FXuS3P02+Ea7pq1FatpS1Dj4FJF7tp2pvL\nnQrn3uZm3P3PfXDkZ93Ir0mfgfPugofOgN0bXdmb97TsWG9tw+uuSe2Sh9ruyN+xHFDIOQJKP4VB\nkzuO2xgDWLKIGuGw8sC7G7n9ZffEvfg4HyeNyWZIvySm5GUy+8hBxPl8xMd1YcxBKAgNVZCYsW/b\n4j/D8ONh0JFu/ZYMSM+D/CsgkOKamfZshnuOanmu1IFQWXhQ73E/Uy93zVuNxA8jTnB3sWeNgurd\nbhqUlBzwx8GvhkC9q4Vx6k1Qss71wZz0Hdffc0tGy/PP3+o6/YN17ryNKna5Y7NGQ8aQA8dYs9fV\ngIw5zFmyiDJvryvmnXXFrNpRxvsbd++3/+qTRvKj8yays6yGQemJiHSxyaq1ikIIJLZMKAAPng5J\nWXDsde7LcvGfYeVf9+0/8dtQVwlLHty3LW8mlO+AjDxIznJNV911zLUtp0g56btuvTFZNHfVm5A6\nAO5uVZMYfTp8+qZbHjAJpv8PHHtty6TSXtNXVamrAf3rbvjyKzDs2O6/F2OigCWLKLZk827SEwN8\n+bElbN9b07Q9MznA3uoGBqQl8NVTRpOZFOAz04bg62pfR1fU7IWPn3E3EjbUuGlMwI2Q+u//uSau\n8+92f+E3+uQfsMDry/hJKax4EhZ+Y9/+o69umWxSBkBVUcvrJvVz/RqNWicRcM1OxZ14BnruUXDG\nT+HxZh3px13vbnqccIHrkwEYdaqbz6vEG4iQNRpO+QEcdWnH1zAmSlmyOAzsqarnK39Zwodb97Zb\n5uLpQ7hlziR2V9ZT0xBiQkPRUUsAABYmSURBVG76IYzwADa+7ZqvpnzOrdeWQWUxpOdCfIpLBAXL\nIG0Q9B8N/30cPnoGtnlTnPy4GAqWwEdPw8a3XC1i/SuuDyQccuXDrYb0puVCxc6W2yZfAh8/e3Dv\n5fqlbvJHYw5DliwOE6rK9r01ZCQF2FJaTWLAx1tri/nFP9a0Wf6qE0cyITed4so6vnDMMOJ8PpLi\nO3F3d18RDrk7xxM7SHpv/drdb3LhH8EfgJzxrkktKcvNr1Vd6hJMYgb88ViXWCZf4mooD5/lOvAn\nfQb+/XvQ0IGvNeECOOdX7gbKUIPrY2ndIR8OH7iTfs8Wd/zBNh+CG1nWE+cxBksWh73ahhA+ERau\n2MHmkio+2VXB62v274hOjvcTDCuhsHLz+RM5bfwABmYkkBAXRQmkLeGQ68do3efSlroKeHm+64cZ\nNNl15CdluYRUXw3hoKv5fPBnN4oruT8MmQHVJXDvNHeO/mPg6x/A72fA0Jlw8QNupuBdK10taemj\ncPUbrunq37/3alK7YM7vobII/pAPZ93q7s7vyH8fdzG1NWV9OAS3Zrk5wk75flc+MWPaZMkiBm0s\nrqSksp4Pt+4hrMrijbt5e11xm2XHD0xj6+5qahpCXHvKaObPOuIQRxslbhsGdV5neO5Rro8G3Eit\nqlafrfhcraN181hzR5zvRnxNvMj1/zx+sRvZNWCCq30EEmHZY67stz5ytaz0IW4Y8+b33DDpxmv9\ndI9LRKkDevQtm9hiycIAUFRRy+Pvb+XYUVn86a1PWVdYwdgBaby3oaTN8vNmDuPKE0awoaiSXWW1\nXHnCiBYjr4KhMGGla0N5o1lFIZSshUW3ubvox892fSgla/eVyTvaDeN99qq2R2015493tYOOmr46\nktzf3W3/6Llw8UMw5bNuyLTfm6lH1f1s/Tf4E9yU93lHu6c5hkP736+y8W03MGHWr1s2cTWWDYfc\nrAEDJrrnsYw5Y991RDpuhjN9liULc0A19SHWFVawcnsZP3n+43bLDclMYki/JI4YlMbijbtZW1gB\nwH2fn87ZkwZSVtNAdmoC6wsr8PuEUTktp/qobQiRGIjyJq+2BOugYKlrkvIH3LaKXa4mkJID794J\nx1znvtTfvt2N+Jo6zzWbVZXCh3+Bdf90tYzTf+KGKVcWupsPB02G+FQ3V9eula620lDt+k5GnOiS\n0tb/tIyncUTZgEluNFe4AQLJ7rjmxp3rJo4cc6a7KTIjz/WlfPgXt3/8ea6Zbsg0KFrjflqfA9zN\nmOJrGUcgBUJ1cNRlcObP9o2cg7b7WRpqXU1qv8+23k09s30pbPvAnS8pyx1vfTU9rk8nCxE5F7gH\n8AMPqertrfafDNwNTAEuU9VnOjqnJYvuC4WVlz7aSViVshrXhFJYXktWSgJPvL+Fjc3uMO/I5CHp\n7Nhby+6qeq47dTR/eutTbpp1BLmZSYTCYc6dlBtdHe590YY3XPNVXKL7sm+cCBJaDkPOGuWehbLi\nSXdvTE2z+3cSMvY1r8UlQc64fU1sPSWpHwzJd8mrdq/rD9qzyd3oWbDElRkyw8WWkAaDp7oEvGeT\n6/NpS+ONpMnZLjGHGyBjqKvR7VwBaYMhL981zakC6poLk/vD279x5z35ey6eil0uKRV/AlMudYMX\n9myG9MHus+o/1i37/C5hi8/1JYUa3HXDQffZhYPuPPEp+5K0z+/6ysJBV7MTX9uJsSNt1QJ7WJ9N\nFiLiB9YBZwEFwBJgnqqublZmBJAOfBdYaMmi99QFQ6wsKGN4VjKrdpQzblAatQ0hrv7fpWwtrSYY\n1qb7PzojNyORH583kQVLthLv9/FpcSVzjhrMwIxELpme11QLCYW163Nlxaq6Cvel0njHeeP/3dZ/\nhZfvcAkmkOyaw+rK3F/3/oCrBdRXuy/cuAT3Rb/rI8ge50aaNdR4zVFB9yWrYVdLSkh3TXKJGa5G\nVfopbPk3vPkLN4DAF+euNWiySwRl21yyqK+G+go3zUz1bjf3mPjcecXvrhWqJ6o0jx91y26H+0wT\nM13NKxz29usBXnGfXUO1l4iS3fZwyJ239c95v4Wjv9K9sPtwsjgOuEVVz/HWbwJQ1dvaKPsY8KIl\ni76nIRQmzieUVNaTnRpPfSjMG2uKOGJQGptLq9hT1cDD723iS8cP5401Rby6unNThpw8LgeAxRtL\nmTg4naH9kgn4faQm+Hlh5U6S4/0MSEvgz1/MJyctoSmWV1btYsyAVEbnpPL7N9bz2fyhDM1KbnHu\ncFgJqxLnt7b1XtPYr9JQ6/4KT0jb10TVVlNV45djOOg18WW7Z9DX7oXyne4enYqdrukqbaArV7jK\nG+Isbj2Q7Jr4Bk52ibVxFF1arluuq3ADBfwB9/yYqmL3xV++wyUxDe+Lw+d3yc8X585fWexqFBp2\nxwW8mob4XJlgLfgC7ku/rsI71g80Nqk1a1prvR4OueSuIZdcxefVcnxek1zjss/NFD1kRrf+Sfpy\nspgLnKuqV3nrXwSOUdXr2yj7GAdIFiJyDXANwLBhw2Zs2dKJie9Mr2gIhamuD7FtdzV/X76dE8Zk\nM3VoJvFxPhZv3M2/NpTw0HubiPMJIjAhN51PdlVQH3R/nfl9Qijc8ndycEYiiQF/u81k4wamEgwr\nkwZnMDI7hXvfWE/+8H588bjhfHPBcqYNy+QrJ47k/CmDqahtoLC8jvLaBqbmZUb2rnhj+pC+nCw+\nC5zTKlnMVNVvtFH2MaxmERNUlcLyOvqlBKisDdI/NYHKuiB7qurJSA4Q7/exsbiKcQNT+ct/trCh\nqIK1uyqorg8RH+dj7ow8nv/v9hZ3u2enJlBSWdfhtZPj/VTXtxydlJ0aT8Dv4wvHDOPrp405+Lm4\njOmjDjZZRPJJeQXA0GbrecCOCF7PRAERYVCG6wBMSHX9FqkJcaQm7PtVnDjY3dX9lRNHtnmO/zlu\nBCu27WVAegI7y2qZmpfJ31dsxydCeU0D508ZzNef/JDte2u49OihrNpRzj9W7mxKFJdMz2NFwV42\nFFWyp7qBMTmp3PnqOs6ZNIixrZ+hbowBIpsslgBjRWQksB24DPj8gQ8xpnOOGuo6eXMzkgD4zLS8\nFvufvLrlLLK3X9zAE4u3csn0vKY+kPpgmJr6ENv31jD73ndZtaOcOL+PXWW1HDe61dMBjYlxEUsW\nqhoUkeuBV3BDZx9R1VUiciuwVFUXisjRwHNAP+ACEfmZqk6KVEwmdqUlBrj2lNEttsXHueeHJMa7\njvBv/XV5074xA1L53jnjmTkii9+/uYHPHzOUIZnJ/PGtDWzdXc38WUewpbSaGcP7sW13Nf/6tJS8\nfkmcNn4A5bUNJMb5iY/zsbe6nqKKOsYNTKNgTzUp8XGkJwX47atrmTdzGHXBMEnxfoZkJh3Sz8OY\nrrKb8owBbnt5DX9+e+NBnyc9MY7y2iAAA9MTKCx3fSnfOnMsd7++vt3j7pg7hSl5mVz/5IesL6rk\npLHZnD1xIPNmDrNRXaZH9NkO7kixZGEiZfm2vfzhzQ1875zx1AfD/HXpVl5YsZMTx2RTXtvAsi17\nqK4PcfG0IZTXBltM6Pjziybz4oodLN7kbogbmpXEtt017V2q0xprQ1ccP6Kpr6crVJXFm3YzJS+D\n5PhItjqbvs6ShTERpKrtjpAqq2lgd1U9qQlxTf0gpZV1ZKW4+1GWbdlDVko897/1KYMykpiSl0Gc\nNzT4uic+BGDFzWfz5tpCvv3XFeT1S6JgTw0js1P45UWTufPVtS1GfV1+7DAqa4PkZiaRnZrA4+9v\n4ZRxOZw5YSCpiXGMzknhw617WfRJEV87dTT3v72R5Hg/f1i0gQm56fzjGyfy0Hsb+efHu5h9ZC4J\ncT4umZHHmp3ljMlJIz0pDhHh7XXF7Cqr4bTxAyiprCc+TshOdTMY762pZ0NRJfnDs7p9h/7H28s4\nYlCa1ZgOMUsWxkShUFhpCIWb7mQPhxWfT2j8/ygivLe+hMsfXhzROJo3mx0xKI1QWFlf1MEkic1c\nfuwwfnHRkRRV1PLvDaWcNXEg5bUNrNpezhkTBiAibCqpIs4nDM1KZmXBXub84V+cPXEgd8w9iozk\nAJ8WV7Jsyx6OGZlFZlI8CQFf0+eyvrCCkdkpllh6gCULYw5jy7bsZvXOCrJT4nl/YymKm8/rsqOH\nMax/Mj94ZiXDspI5ZXwOBXtqGD8wjR1lNbz80S7qgiHWFVbyw9kTePnjnSzbsofPzxzGkXkZ3PbS\nJ+wqryU+zseXjhvOK6sK2bp730SCgzMSiY/zsbm0jckFWxmSmdT0WOBhWcktzvPZGXn8bVlBu8e2\nLt9oeP9kjh/dn6c+2EZ2ajznTh5EZW2Q8tog15w8iqeXbuOddcVMyE1nU0kV503JpSGoPPthAbOP\nzGXujCFc9sD7zJ81gVPGZbOrrI6q+iAJcT7SEuNYWVDGvz8tZU9VPSeNzSEtMY68fkmcPWkQm0qq\n6J8az1trizlrwkA2llRSsKeGU8blEFb3zJiA38ddr63jqLxMzpo4kGA4zIptZVTUNnDmhIEtbvYs\nr20gyUt+O/bWMCwrGRGhuKKO9KQ44v0+gmH1blQVPty6h3ED05qGk9fUh0iK91NVFyQloftNiZYs\njDHtCobCxPl9+zWnldU0cMcrn3DjWePJSokHoLo+yNNLtnHJjDzSEgMtzrO3up5VO8qZMbwf5TUN\n9E9NYH1RBf/3ny3sKqslOzWBsCpvryumqKKO+Dhf0135AD5xo89mT85l9IBUAn5h8cbdbCqpIjM5\nwPGjs1mwZFvTzZVHDErjk10VXX6/zWtK3ZGWEEdFXfePB+iXHKCyLkhaYoDdVfvPezVv5lBGZqfw\nq5cO/Fz5b54xlrfWFbN2Vzm/uOhI7njlE344ewIXTh3SrbgsWRhj+oyGUJhPiys5YpC7sbIuGCLe\n70NEaAiFCRygOak+GKYhFCYlIY5QWHl3fTHZqQlkJAV46N2NlFTWk9cviSPzMhiVnUpaYhx/W1bA\nyWOzeW11IZnJ8Xzp+OGs3VXByx/v4oF3NjJ3Rh7ZqQlkp8YzITedmvoQtcEQr68uZNueGm48axyD\nM5N4dlkBD7+3iRPG9GfR2mLGDkhlU0kVdc0S3uQh7vjy2iDFFS6pjcpOoaSyjsuPHc7uqnoWrS2i\nsLwOEcgf3o8lm/cAbhqbeTOH8vj7W7v0eWYkBZpmiM5JS+CJq45hXDdvHLVkYYwxPaB17as+GEZR\n4ny+/WZGLq2sY+vuaqYOzaQuGG7xLJdgKEx5bZCslHj2VNWTmRygoi5IemKAUFhZs7McgKq6IFt2\nVzN3eh5hVVYUlFFWU09CnJ81O8s5dfwAxgxI5ZNd5fx1yTYunDqEqd7NqN1hycIYY0yHDjZZ2BAD\nY4wxHbJkYYwxpkOWLIwxxnTIkoUxxpgOWbIwxhjTIUsWxhhjOmTJwhhjTIcsWRhjjOlQ1N2UJyLF\nwJZuHp4NlPRgOIeKxX3oRGPMEJ1xR2PMEL1xj1fVbj9kPuqehqKqOd09VkSWHswdjL3F4j50ojFm\niM64ozFmiO64D+Z4a4YyxhjTIUsWxhhjOhRryeKB3g6gmyzuQycaY4bojDsaY4YYjTvqOriNMcYc\nerFWszDGGNMNliyMMcZ0KGaShYicKyJrRWSDiMzv7XiaE5FHRKRIRD5uti1LRF4TkfXeaz9vu4jI\nvd77WCki03sp5qEiskhE1ojIKhH5ZpTEnSgiH4jICi/un3nbR4rIYi/uv4pIvLc9wVvf4O0f0Rtx\ne7H4ReS/IvJiFMW8WUQ+EpHljUM3o+B3JFNEnhGRT7zf7+OiIObx3mfc+FMuIt/q0bhV9bD/AfzA\np8AoIB5YAUzs7biaxXcyMB34uNm23wDzveX5wK+95dnAy4AAxwKLeynmXGC6t5wGrAMmRkHcAqR6\nywFgsRfP08Bl3vb7geu85a8B93vLlwF/7cXfkxuBJ4EXvfVoiHkzkN1qW1//HfkLcJW3HA9k9vWY\nW8XvB3YBw3sy7l59U4fwwzsOeKXZ+k3ATb0dV6sYR7RKFmuBXG85F1jrLf8ZmNdWuV6O/+/AWdEU\nN5AMfAgcg7sjN6717wvwCnCctxznlZNeiDUPeAM4HXjR+0/ep2P2rt9WsuizvyNAOrCp9efVl2Nu\n4z2cDfyrp+OOlWaoIcC2ZusF3ra+bKCq7gTwXgd42/vce/GaOabh/krv83F7zTnLgSLgNVytc6+q\nBtuIrSlub38Z0P/QRgzA3cD3gbC33p++HzOAAq+KyDIRucbb1pd/R0YBxcCjXpPfQyKSQt+OubXL\ngKe85R6LO1aShbSxLVrHDPep9yIiqcCzwLdUtfxARdvY1itxq2pIVafi/lqfCUxoq5j32utxi8j5\nQJGqLmu+uY2ifSbmZk5Q1enALODrInLyAcr2hbjjcE3Cf1LVaUAVrvmmPX0h5iZev9Uc4G8dFW1j\n2wHjjpVkUQAMbbaeB+zopVg6q1BEcgG81yJve595LyISwCWKJ1T1/3mb+3zcjVR1L/AWrs02U0Qa\n50prHltT3N7+DGD3oY2UE4A5IrIZWIBrirqbvh0zAKq6w3stAp7DJee+/DtSABSo6mJv/Rlc8ujL\nMTc3C/hQVQu99R6LO1aSxRJgrDd6JB5XTVvYyzF1ZCHwJW/5S7g+gcbt/+ONZjgWKGusZh5KIiLA\nw8AaVb2r2a6+HneOiGR6y0nAmcAaYBEw1yvWOu7G9zMXeFO9Rt5DRVVvUtU8VR2B+919U1W/QB+O\nGUBEUkQkrXEZ15b+MX34d0RVdwHbRGS8t+kMYHVfjrmVeexrgoKejLs3O2IOcafPbNyInU+BH/V2\nPK1iewrYCTTgMv5XcG3MbwDrvdcsr6wA93nv4yMgv5diPhFXbV0JLPd+ZkdB3FOA/3pxfwzc7G0f\nBXwAbMBV4RO87Yne+gZv/6he/l05lX2jofp0zF58K7yfVY3/76Lgd2QqsNT7HXke6NfXY/ZiSQZK\ngYxm23osbpvuwxhjTIdipRnKGGPMQbBkYYwxpkOWLIwxxnTIkoUxxpgOWbIwxhjTIUsWxrQiIqFW\nM3j22CzFIjJCms0ubEy0iOu4iDExp0bddCDGGI/VLIzpJO/ZDL8W9zyMD0RkjLd9uIi84T0X4A0R\nGeZtHygiz4l7dsYKETneO5VfRB4U9zyNV707yY3p0yxZGLO/pFbNUJc221euqjOBP+DmZ8Jb/l9V\nnQI8Adzrbb8XeFtVj8LNL7TK2z4WuE9VJwF7gUsi/H6MOWh2B7cxrYhIpaqmtrF9M3C6qm70JlHc\npar9RaQE9yyABm/7TlXNFpFiIE9V65qdYwTwmqqO9dZ/AARU9ReRf2fGdJ/VLIzpGm1nub0ybalr\nthzC+g5NFLBkYUzXXNrs9T/e8r9xs8ECfAF4z1t+A7gOmh64lH6ogjSmp9lfNMbsL8l7kl6jf6pq\n4/DZBBFZjPtDa5637QbgERH5Hu4pa1d6278JPCAiX8HVIK7DzS5sTNSxPgtjOsnrs8hX1ZLejsWY\nQ82aoYwxxnTIahbGGGM6ZDULY4wxHbJkYYwxpkOWLIwxxnTIkoUxxpgOWbIwxhjTof8PMwtDsQra\nfS4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2660278769624007\n"
     ]
    }
   ],
   "source": [
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "\n",
    "    #model_name_wrt = ('/kaggle/working/molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    #X_train = X_train.fillna(0)\n",
    "    #X_test = X_test.fillna(0)\n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data = FeatureTransformer().transform(\n",
    "        dataset=pd.concat([X_train, X_test]),\n",
    "        ohe_features=oh_cols,\n",
    "        continuous_features=continuous_cols\n",
    "    )\n",
    "    \n",
    "    #target_data = target\n",
    "    \n",
    "    train = input_data[:len(X_train)]\n",
    "    test_input = input_data[len(X_train):]\n",
    "    \n",
    "    cur_type_idx_train = (train_type == mol_type)\n",
    "    cur_type_idx_test = (test_type == mol_type)\n",
    "\n",
    "    cur_type_train = train[cur_type_idx_train]\n",
    "    cur_type_target = target[cur_type_idx_train]\n",
    "    cur_type_mols = molecule_name[cur_type_idx_train]\n",
    "\n",
    "    train_idx, val_idx = next(GroupShuffleSplit(random_state=42, n_splits=1, test_size=0.2).split(cur_type_train, cur_type_target, cur_type_mols))\n",
    "\n",
    "    x_train, y_train = cur_type_train[train_idx], cur_type_target[train_idx]\n",
    "    x_val, y_val = cur_type_train[val_idx], cur_type_target[val_idx]\n",
    "\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(x_train.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam())\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=30, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    #sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "    history = nn_model.fit(x_train, [y_train], \n",
    "            validation_data=(x_val, [y_val]), \n",
    "            callbacks=[es, rlr], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(x_val)\n",
    "    plot_history(history, mol_type)\n",
    "    accuracy=np.mean(np.abs(y_val-cv_predict[:,0]))\n",
    "    print(np.log(accuracy))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[test[\"type\"]==mol_type]=test_predict[:,0][test[\"type\"]==mol_type]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "sub = feather.read_dataframe('../data/input/sample_submission.feather')\n",
    "sub['scalar_coupling_constant'] = test_prediction\n",
    "sub.to_csv('../data/nn_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
